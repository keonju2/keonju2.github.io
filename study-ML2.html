<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>머신러닝 정리 (2) <br> 지도학습 (2)</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
    
    <!-- custom.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />
    
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- 웹폰트 추가-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">

    <!-- syntax.css 추가-->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="데이터 사이언티스트를 꿈꾸는 블로그입니다." />
    <link rel="shortcut icon" href="https://keonju2.github.io/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="https://keonju2.github.io/study-ML2" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="주건나's Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="머신러닝 정리 (2) <br> 지도학습 (2)" />
    <meta property="og:description" content="머신러닝 공부 관련 글 머신러닝 정리 (1)-지도학습 (1) 머신러닝 정리 (2)-지도학습 (2) 머신러닝 정리 (3)-비지도학습 (1) 머신러닝 정리 (4)-비지도학습 (2) 머신러닝 정리 (5)-데이터 표현과 특성 공학 머신러닝 정리 (6)-모델 평가와 성능 향상 머신러닝 정리 (2) - 지도학습 (2) 본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다. 또 데이터" />
    <meta property="og:url" content="https://keonju2.github.io/study-ML2" />
    <meta property="og:image" content="https://keonju2.github.io/assets/built/images/ML.png" />
    <meta property="article:publisher" content="https://www.facebook.com/monkeykeonju" />
    <meta property="article:author" content="https://www.facebook.com/monkeykeonju" />
    <meta property="article:published_time" content="2021-10-04T10:00:00+09:00" />
    <meta property="article:modified_time" content="2021-10-04T10:00:00+09:00" />
    <meta property="article:tag" content="Study" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="머신러닝 정리 (2) <br> 지도학습 (2)" />
    <meta name="twitter:description" content="머신러닝 공부 관련 글 머신러닝 정리 (1)-지도학습 (1) 머신러닝 정리 (2)-지도학습 (2) 머신러닝 정리 (3)-비지도학습 (1) 머신러닝 정리 (4)-비지도학습 (2) 머신러닝 정리 (5)-데이터 표현과 특성 공학 머신러닝 정리 (6)-모델 평가와 성능 향상 머신러닝 정리 (2) - 지도학습 (2) 본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다. 또 데이터" />
    <meta name="twitter:url" content="https://keonju2.github.io/" />
    <meta name="twitter:image" content="https://keonju2.github.io/assets/built/images/ML.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="주건나's Blog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Study" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "주건나's Blog",
        "logo": "https://keonju2.github.io/"
    },
    "url": "https://keonju2.github.io/study-ML2",
    "image": {
        "@type": "ImageObject",
        "url": "https://keonju2.github.io/assets/built/images/ML.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://keonju2.github.io/study-ML2"
    },
    "description": "머신러닝 공부 관련 글 머신러닝 정리 (1)-지도학습 (1) 머신러닝 정리 (2)-지도학습 (2) 머신러닝 정리 (3)-비지도학습 (1) 머신러닝 정리 (4)-비지도학습 (2) 머신러닝 정리 (5)-데이터 표현과 특성 공학 머신러닝 정리 (6)-모델 평가와 성능 향상 머신러닝 정리 (2) - 지도학습 (2) 본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다. 또 데이터"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="머신러닝 정리 (2) <br> 지도학습 (2)" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="https://keonju2.github.io/">주건나's Blog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-R" role="menuitem"><a href="/tag/programming/">프로그래밍 언어</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/activity/">대외활동</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/license/">자격증</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/study/">이론 공부</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Tag별 Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
                <a class="social-link social-link-fb" href="https://facebook.com/monkeykeonju" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>
</a>
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
            </script>
            <script type="text/javascript" src="http://cdn.mathjax.org/math...">
            </script>
        
        <article class="post-full  tag-study post tag-ML ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime=" 4 October 2021"> 4 October 2021</time>
                    
                        <span class="date-divider">/</span>
                        
                            
                               <a href='/tag/study/'>STUDY</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">머신러닝 정리 (2) <br> 지도학습 (2)</h1>
            </header>
            <!--
            
            <figure class="post-full-image" style="background-image: url(/assets/built/images/ML.png)">
            </figure>
            
             -->
            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <p><span class="table-of-contents-list">머신러닝 공부 관련 글</span></p>
<ul class="table-of-contents-list">
    <li><a href="./study-ML1">머신러닝 정리 (1)-지도학습 (1)</a></li>
    <li><a href="./study-ML2">머신러닝 정리 (2)-지도학습 (2)</a></li>
    <li><a href="./study-ML3">머신러닝 정리 (3)-비지도학습 (1)</a></li>
    <li><a href="./study-ML4">머신러닝 정리 (4)-비지도학습 (2)</a></li>
    <li><a href="./study-ML5">머신러닝 정리 (5)-데이터 표현과 특성 공학</a></li>
    <li><a href="./study-ML6">머신러닝 정리 (6)-모델 평가와 성능 향상</a></li>
</ul>
<h1 id="머신러닝-정리-2---지도학습-2">머신러닝 정리 (2) - 지도학습 (2)</h1>
<p>본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.<br />
또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.<br />
글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.<br />
코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.<br /></p>

<p>소스 코드: <a href="https://github.com/rickiepark/introduction_to_ml_with_python">https://github.com/rickiepark/introduction_to_ml_with_python</a></p>

<h2 id="지도학습-2">지도학습 (2)</h2>

<ol>
  <li>결정 트리</li>
  <li>결정 트리의 앙상블</li>
  <li>배깅, 엑스트라 트리, 에이다부스트
    <ol>
      <li>배깅</li>
      <li>엑스트라 트리</li>
      <li>선형 모델</li>
      <li>에이다부스트</li>
    </ol>
  </li>
  <li>커널 서포트 벡터 머신</li>
  <li>신경망 (딥러닝)</li>
  <li>분류 예측의 불확실성 추정
    <ol>
      <li>결정 함수</li>
      <li>예측 확률</li>
      <li>다중 분류에서의 불확실성</li>
    </ol>
  </li>
</ol>

<h3 id="결정-트리">결정 트리</h3>
<p><span style="color:orange">Decision Tree </span></p>

<p>결정 트리는 분류와 회귀 문제에서 사용되는 모델입니다.<br />
스무고개처럼 예/아니오로 나눌 수 있는 조건을 통해서 결정에 다다르게 됩니다.<br />
질문과 정답은 노드가 되고 특히 마지막 노드는 리프라고 합니다. 
<br /><br />
<img src="https://user-images.githubusercontent.com/54880474/136059620-b1adf458-14a4-474b-ae07-1fb89a48f29e.png" alt="tree1" /></p>

<p>결정트리의 구조는 왼쪽 하단에 사진처럼 가장 위에는 Root node, 질문과 답을 연결하는 Edge, 내부의 Internal node, 마지막 노드는 Leaf node, 그리고 Depth로 구성됩니다.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h6 id="결정-트리-만들기-"><span style="color:green">결정 트리 만들기 </span></h6>
<p><br />
결정 트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예/아니오 질문 (TEST) 목록을 학습한다는 뜻입니다.<br />
보통 데이터들은 예/아니오 특성으로 구분되지 않고  연속적인 특성을 가진 2차원 데이터 셋에서 보통 ‘특성 i는 값 a보다 큰가?’의 형태와 같은 테스트를 가집니다.<br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136060832-f157681f-43fa-4b0d-a159-b7554269124b.png" alt="tree2" /></p>

<p>이 데이터들을 X[1]&lt;=0.6인 테스트로 나누어 봅니다.</p>

<p><img src="https://user-images.githubusercontent.com/54880474/136061884-6c694665-0037-4ba9-9e82-25dd6d9d5c6a.png" alt="tree3" /></p>

<p>알고리즘은 가능한 모든 테스트에서 타깃 값에 대해 가장 많은 정보를 가진 것을 고르게 됩니다.</p>

<p>따라서 X[1]&lt;=0.6인 테스트를 선택하게 됩니다.<br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136062820-b9733fdf-be5b-4aaf-a6c4-02114c7cb748.png" alt="tree5" /></p>

<p>결정 트리에서 각 테스트는 하나의 축을 따라 데이터를 나눕니다.<br />
하나의 질문당 하나의 축을 만들어서 영역이 한 개의 타깃값을 가질 때까지 반복됩니다. <br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136062365-e168f4b1-9828-487a-bcd1-de32998eafb2.png" alt="tree4" /></p>

<p>결정 트리의 멈춤 조건입니다.<br />
즉, 미리 정의한 조건들이 없다면 가지를 만들 수 있을 때까지 만드는 것을 알 수 있습니다.<br />
결정트리의 예측은 그 포인트가 어느 리프에 들어갈지 확인하는 것인데 분류는 타깃 값 중 다수인 것이 예측 결과가 되고 회귀의 경우 리프 노드의 훈련 데이터 평균값이 결과로 출력됩니다.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h6 id="결정-트리-복잡도-제어하기-"><span style="color:skyblue">결정 트리 복잡도 제어하기 </span></h6>
<p><br />
결정 경계가 클래스 포인트에 멀리 떨어진 이상치에 민감하게 되어 모든 리프 노드가 순수 노드가 될 때까지 진행하면 모델이 복잡해지고 과대적합이 발생합니다.<br />
과대적합을 막기 위한 방법은 크게 사전가지치기, 사후 가지치기 두 가지입니다.<br />
사전 가지치기는 이름에서 알 수 있듯이 모델을 만들 때 깊이나 리프의 개수 또는 테스트의 최소 개수를 미리 제한하는 것입니다.<br />
미리 제한하기 때문에 정말로 중요한 포인트를 분류하지않을 수 있습니다.<br />
사후 가지치기 역시 이름에서 알 수 있듯이 트리가 만들어진 뒤 포인트가 적은 노드를 삭제 혹은 병합하게 되는데 에러감소 프루닝, 룰 포스트 프루닝 같은 방법들이 있습니다.<br />
<br />
<br /></p>

<p><span class="evidence">참고</span><br />
에러감소 프루닝</p>
<blockquote>
  <p>모든 노드를 프루닝 대상으로 고려<br />
노드 제거 후 검증을 통해 제거 전, 후 정확도 비교<br />
제거 전보다 정확도가 낮아지기 전까지 반복</p>
</blockquote>

<p>룰 포스트 프루닝</p>
<blockquote>
  <p>의사결정 트리를 룰셋으로 변환 (룰은 루트부터 리프까지의 경로)<br />
이 룰셋 속성들에 정확도를 떨어뜨리는 속성을 제거<br />
프루닝 완료 후 정확도 순으로 정렬해 이 순서대로 적용</p>
</blockquote>

<p><br />
<br /></p>

<p>결정 트리는 다음과 같이 만들 수 있고 정확도를 확인할 수 있습니다.<br />
Default값은 모든 리프가 순수 노드가 되는 모델을 만들기 때문에 훈련 세트의 정확도가 100%가 됩니다.<br />
하지만 트리가 무한정 깊어지고 복잡해지고 일반화가 잘 되지 않습니다.
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cancer</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">1.000</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.937</span>
</code></pre></div></div>
<p><br />
과대적합 때문에 반드시 훈련 세트의 정확도가 테스트 정확도와 비례하지 않아서 max_depth와 같은 파라미터를 통해 과대적합을 줄이고 테스트 세트 정확도를 높일 수 있습니다.</p>

<p><br />
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.988</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.951</span>
</code></pre></div></div>

<p><br /></p>

<hr />
<p><br /></p>

<h6 id="결정-트리-분석-"><span style="color:purple">결정 트리 분석 </span></h6>
<p><br /></p>

<p>결정 트리를 생성하고 시각화하기 위해서는 다음과 같은 모듈이 필요합니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 트리 모델 생성
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span> 

<span class="c1"># 트리의 시각화_1
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export</span> <span class="n">graphviz</span> 

<span class="c1"># 트리의 시각화_2 (.dot 파일을 만들지 않아도 가능)
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span> 
</code></pre></div></div>

<p><br />
그래프를 시각화하는 코드는 다음과 같이 쓸 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># graphviz 이용
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span>

<span class="n">export_graphviz</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="s">"tree.dot"</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s">"악성"</span><span class="p">,</span> <span class="s">"양성"</span><span class="p">],</span>
                <span class="n">feature_names</span><span class="o">=</span><span class="n">cancer</span><span class="p">.</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">impurity</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1">#plot_tree
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span>

<span class="n">plot_tree</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s">"악성"</span><span class="p">,</span> <span class="s">"양성"</span><span class="p">],</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">cancer</span><span class="p">.</span><span class="n">feature_names</span><span class="p">,</span>
         <span class="n">impurity</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div></div>

<p>filled=True를 넣어주면 다음과 같이 색상이 들어가는 트리 모델을 얻을 수 있습니다.</p>

<p><img src="https://user-images.githubusercontent.com/54880474/136067667-33e3ca00-4752-4125-9d5e-ff12703b4d94.png" alt="tree8" /></p>

<p><br /></p>

<hr />

<p><br /></p>

<h6 id="트리의-특성-중요도"><span style="color:yellow">트리의 특성 중요도</span></h6>
<p><br /></p>

<p>tree.feature_importane를 통해 특성 중요도를 알 수 있습니다.<br />
특성 중요도는 0부터 1 사이에 존재하는데 0은 전혀 사용되지 않은 특성, 1은 완벽하게 타깃 클래스를 예측한 특성을 의미합니다. <br />
특성 중요도가 낮다는 유용하지 않다가 아닌 모델이 만들어질 때 특성을 선택하지 않았거나 특성과 중복되는 정보가 있다는 것을 의미합니다.<br />
전체 합은 1이 되고 따라서 특성중요도는 ‘이 모델이 만들어지는데 어떤 특성의 비율이 높은가?’ 정도의 해석이라고 생각하면 될 것 같습니다.<br />
Worst_radius만 보고 ‘반지름이 크면 양성이다?’ 를 알 수 없는 것처럼 특성 중요도는 어떤 클래스를 지지하는지 알려주지 않습니다.<br />
<br />
결정 트리의 회귀도 분류와 비슷하게 적용됩니다.
단, 결정 트리를 회귀 모델로 사용하게 되면 훈련 데이터 범위 밖의 정보가 없어서 그 부분에 대한 예측이 불가능하게 됩니다.</p>

<p><img src="https://user-images.githubusercontent.com/54880474/136069236-7fb89212-fcc1-4919-b9ae-430f2a18c359.png" alt="tree9" /></p>

<p>다음 모델은 트리 복잡도에 제한을 두지않아서 훈련 데이터는 완벽하게 예측하지만 데이터 범위 밖으로 나가면 마지막 포인트로 예측값을 출력합니다.
따라서 트리 모델은 가격의 등락과 같은 예측을 할 때는 좋은 예측 모델을 만들 수 있지만 시계열 데이터에서는 데이터가 가진 시간 범위 밖의 예측은 안되기 때문에 잘 맞지 않습니다.<br />
<br /></p>

<hr />
<p><br /></p>

<h6 id="장단점과-매개변수"><span style="color:pink">장단점과 매개변수</span></h6>
<p><br /></p>

<p>장점</p>
<blockquote>
  <p>해석력이 높습니다.<br />
데이터의 스케일에 구애받지 않습니다. <br />
정규화나 표준화 같은 전처리 불필요합니다.
특성의 스케일이 다르거나 이진특성, 연속적인 특성이 혼합되어도 잘 작동합니다.</p>
</blockquote>

<p>단점</p>
<blockquote>
  <p>과대적합되는 경향이 있어 일반화 성능이 좋지 않습니다.  <br />
축 평행을 구분하여 일부 관계에서 모델링이 어려움이 있습니다. <br />
훈련 데이터에 대한 약간의 변경은 전체 결정논리에 큰 변화를 야기하여 샘플에 민감합니다.</p>
</blockquote>

<p><br /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">매개변수</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>min_samples_split</strong></td>
      <td>- 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용 <br /> - Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>min_samples_leaf</strong></td>
      <td>- 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수<br />- min_samples_split과 함께 과적합 제어 용도<br />- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_features</strong></td>
      <td>- 최적의 분할을 위해 고려할 최대 feature 개수<br />- Default = None → 데이터 세트의 모든 피처를 사용<br />- int형으로 지정 →피처 갯수 / float형으로 지정 →비중<br />- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정<br />- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_depth</strong></td>
      <td>- 트리의 최대 깊이<br />- default = None<br />→ 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할<br />- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_leaf_nodes</strong></td>
      <td>리프노드의 최대 개수</td>
    </tr>
  </tbody>
</table>

<p>여기서 max_depth, max_leaf_nodes,min_samples_leaf 중 하나만 지정해도 과대적합을 막는데 충분한 역할을 합니다.</p>

<p><br />
<br />
<br /></p>

<h3 id="결정-트리의-앙상블">결정 트리의 앙상블</h3>
<p><span style="color:orange">Ensemble</span></p>

<p>앙상블은 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법입니다.<br />
책에서는 결정 트리의 앙상블로 한정하고 가장 많이 쓰이는 랜덤포레스트나 부스팅 모델은 트리 기반 모델이지만 앙상블은 다른 분류 모델을 결합하여 사용할 수도 있습니다.</p>

<ul>
  <li>Voting – 서로 다른 알고리즘을 가진 분류기를 결합</li>
  <li>Bagging – 각각의 분류기는 모두 같은 유형의 알고리즘 기반, 모델을 다양하게 만들기 위해 데이터를 재구성 (랜덤포레스트)</li>
  <li>Boosting – 맞추기 어려운 데이터에 대해 좀 더 가중치를 두어 학습 (Adaboost, Gradient Boosting)</li>
  <li>Stacking – 모델의 output 값을 새로운 독립변수로 사용<br />
<br /></li>
</ul>

<p><img src="https://user-images.githubusercontent.com/54880474/136071927-2b58ace4-f191-497f-b214-4077ff7aad64.png" alt="ensemble1" /></p>

<p>앙상블의 조건입니다.<br />
<br /></p>

<hr />

<p><br /></p>

<h6 id="랜덤-포레스트"><span style="color:lightgreen">랜덤 포레스트</span></h6>

<p><br /></p>

<p>랜덤 포레스트는 조금씩 다른 결정 트리의 묶음입니다.      <br />
데이터의 일부에 과대적합되는 경향을 이용하여 서로 다른 방향으로 과대적합된 트리를 많이 만들어 그 결과를 평균냄으로써 예측 성능은 유지되면서 결과적으론 과대적합이 줄어드는 아이디어에 기초합니다.<br />
결정 트리를 많이 만들면서 각 트리는 타깃 예측을 잘 해야 하고 다른 트리와 구별되어야 합니다.<br />
따라서 무작위성을 주입하는데 트리를 만들 때 사용하는 데이터 포인트를 무작위로 선택하거나 분할 테스트에서 특성을 무작위로 선택하는 방법을 이용합니다.<br />
<br /></p>

<hr />

<p><br /></p>

<h6 id="랜덤-포레스트-구축"><span style="color:olive">랜덤 포레스트 구축</span></h6>

<p><br /></p>

<p>from sklearn.ensemble import RandomForestClassifier (or RandomForestRegressor)<br />
n_estimators로 생성할 트리의 개수를 정합니다.<br />
부트스트랩 샘플은 n_samples개의 데이터 포인트 중에서 n_samples 횟수만큼 무작위로 중복 가능하게 반복 추출하는 것을 의미합니다.<br />
따라서 데이터 셋이 원래 크기와 같지만 누락되거나 중복되는 데이터가 만들어집니다.<br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136072938-20fe1a7b-c4e1-4a72-b7ac-08c4dbc16744.png" alt="ensemble2" /></p>

<p><br /></p>

<p>각 노드에서 전체 특성을 대상으로 최선의 테스트를 찾는 것이 아닌 알고리즘이 각 노드에서 후보 특성을 무작위로 선택한 후 이 후보들 중에서 최선의 테스트를 찾습니다. (max_features) <br />
부트스트랩 샘플링을 통해 트리가 조금씩 다른 데이터셋을 이용해 만들어지도록 합니다.<br />
각 노드에서 특성의 일부만 사용하기 때문에 트리의 각 분기는 각기 다른 특성 부분 집합을 사용됩니다.</p>

<p>max_features=n_features는 특성 선택에 무작위성이 들어가지 않습니다. (부트스트랩 샘플링에는 무작위성 그대로 입니다.)<br />
max_feature=1 트리의 분기는 테스트할 특성을 고를 필요가 없게 되고 무작위로 선택한 특성의 임계값 찾기만 하면 됩니다.<br />
max_feature이 커지면 랜덤 포레스트 트리들은 매우 비슷하고 가장 두드러진 특성으로 데이터에 잘 맞춰질 것이고 작으면 트리들은 서로 많이 달라지고 각 트리는 데이터에 맞추기 위해 깊이가 깊어지게 됩니다.</p>

<p>랜덤 포레스트 예측의 경우 알고리즘이 모델에 있는 모든 트리의 예측을 만듭니다.<br />
회귀의 경우 이 예측들을 평균하여 최종 예측을 만듭니다.<br />
분류의 경우 약한 투표 전략을 사용합니다.<br />
약한 투표 전략은 각 알고리즘이 가능성 있는 출력 레이블의 확률을 제공하고 예측한 확률을 평균으로 가장 높은 확률을 가진 클래스가 예측값이 됩니다.<br />
참고로 강한 투표 전략은 다수의 분류기가 결정한 예측값을 최대로 하는 것을 말합니다.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136073413-36133d42-fdab-4036-a310-3ed03cac37be.png" alt="ensemble3" /></p>

<p><br /></p>

<hr />

<p><br /></p>

<h6 id="랜덤-포레스트-분석"><span style="color:darkblue">랜덤 포레스트 분석</span></h6>

<p><br /></p>

<p>랜덤 포레스트 훈련 모델</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">forest</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/54880474/136073874-0e16312c-47e8-4567-b4a4-545001f1d605.png" alt="ensemble4" /></p>

<p>부트스트랩 샘플링 때문에 한쪽 트리에 나타나는 훈련 포인트가 다른 트리에는 포함되지 않을 수 있어 각 트리는 불완전하지만 랜덤포레스트의 결과는 좋은 결정경계를 보여줍니다.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136074021-efc81063-ca11-437d-83f1-0c8b171a01e6.png" alt="ensemble5" /></p>

<p>단일 트리와 다르게 랜덤 포레스트에서 가장 특성 중요도가 높은 특성은 worst perimeter입니다.<br />
랜덤 포레스트에서 더 많은 특성이 0 이상의 중요도를 갖고 따라서 더 넓은 시각으로 데이터를 바라볼 수 있습니다.</p>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cancer</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">forest</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">forest</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">forest</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">1.000</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.972</span>
</code></pre></div></div>

<p><br /></p>

<p>랜덤 포레스트에선 훈련 데이터 정확도가 100% 이지만 단일 트리에 비해서 테스트 정확도가 상승한 것을 확인 할 수 있습니다.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h6 id="장단점과-매개변수-1"><span style="color:chocolate">장단점과 매개변수</span></h6>

<p><br /></p>

<p>장점</p>
<blockquote>
  <p>매개변수 튜닝을 많이 하지 않습니다. <br />
데이터의 스케일에 구애받지 않습니다. <br />
단일 트리의 단점을 보완하고 장점을 그대로 가지고 있습니다.</p>
</blockquote>

<p>단점</p>
<blockquote>
  <p>랜덤 포레스트의 트리는 특성의 일부만 사용하므로 결정 트리보다 더 깊어지는 경향이 있습니다.<br />
다른 random_state를 지정하면 전혀 다른 모델이 만들어집니다.<br />
텍스트 데이터와 같은 차원이 높고 희소한 데이터에 잘 작동하지 않습니다.<br />
선형 모델에 비해 많은 메모리를 사용하며 훈련과 예측이 느림</p>
</blockquote>

<p><br /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">매개변수</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>n_estimators</strong></td>
      <td>- 결정트리의 갯수를 지정<br />- Default = 10 (0.22버전부터 100)<br />- 무작정 트리 갯수를 늘리면 성능 좋아지는 것 대비 시간이 걸릴 수 있음</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>min_samples_split</strong></td>
      <td>- 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용<br />- Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>min_samples_leaf</strong></td>
      <td>- 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수<br />- min_samples_split과 함께 과적합 제어 용도<br />- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_features</strong></td>
      <td>- 최적의 분할을 위해 고려할 최대 feature 개수<br />- Default = ‘auto’ (결정트리에서는 default가 none이었음)<br />- int형으로 지정 →피처 갯수 / float형으로 지정 →비중<br />- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정 (RandomForestClassifier-sqrt(n_feature), RandomForestRegressor-n_feature)<br />- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_depth</strong></td>
      <td>- 트리의 최대 깊이<br />- default = None<br />→ 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할<br />- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_leaf_nodes</strong></td>
      <td>리프노드의 최대 개수</td>
    </tr>
  </tbody>
</table>

<p>N_estimatiors는 클수록 좋고 max_features와 max_depth와 같은 사전 가지치기 옵션은 단일 트리와 같이 주어집니다.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h6 id="그레이디언트-부스팅-회귀-트리"><span style="color:fuchsia">그레이디언트 부스팅 회귀 트리</span></h6>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136076700-214be147-9549-47c5-9681-f20564458f85.png" alt="gradient" /></p>

<p><br /></p>

<p>이름은 회귀이지만 회귀와 분류 모두 사용됩니다. (GradientBoostingClassifier, GradientBoostingRegressor)<br />
그레이디언트 부스팅은 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듭니다.<br />
따라서 기본적으로 무작위성이 없습니다.<br />
대신 강력한 사전 가지치기가 사용되고 깊지 않은 트리를 사용합니다.<br />
각 트리는 데이터의 일부에 대해서만 예측을 잘 수행하여 트리가 많이 추가될수록 성능이 향상됩니다.<br />
이때 손실 함수를 정의하고 경사 하강법을 사용해서 다음 값을 보정합니다.<br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136076846-03cd6618-ee17-4afd-869f-09ed4543156f.png" alt="gradient1" /></p>

<p><br />
<br /></p>

<p>random_state=0 만 입력했을 때</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="err">​</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cancer</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="err">​</span>
<span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="err">​</span>
<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">1.000</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.965</span>
</code></pre></div></div>
<p><br /></p>

<p>random_state=0, max_depth=1 을 입력했을 때</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="err">​</span>
<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.991</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.972</span>
</code></pre></div></div>
<p><br /></p>

<p>random_state=0, learning_rate=0.01을 입력했을 때</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.988</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.965</span>
</code></pre></div></div>
<p><br /></p>

<p>훈련 세트의 정확도가 100%로 과대적합이 된 모델은 max_depth나 learning_rate로 보완할 수 있습니다.<br />
Random_state는 고정시켜야 같은 모델이 나오는 것을 볼 수 있습니다.<br />
Learning_rate는 오차에 곱을 해서 예측값을 업데이트 해주는 값입니다.<br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136077751-6aba3074-c68d-4789-a9af-3d657f671ab2.png" alt="gradient2" /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136077761-e5e43c6c-e62a-4c10-a7e6-96e584cec03e.png" alt="gradient3" /></p>

<p>랜덤 포레스트에 비해 그레이디언트 부스팅은 특성들이 더 적습니다.<br />
안정성에서는 랜덤 포레스트가 더 좋지만 그레이디언트가 성능적으로 더 좋은 모습을 보여줄수 있습니다.<br />
<br />
<br /></p>

<p><span class="evidence">참고</span><br />
XGBoost</p>
<blockquote>
  <p>XGBoost는 데이터 별 오류를 다음 round 학습에 반영 시킨다는 측면에서 기존 Gradient Boosting과 큰 차이는 없음<br />
Gradient Boosting과 달리 학습을 위한 목적식(loss function)에 Regularization term이 축가되어 모델이 과적합 되는 것을 방지해줌<br />
Regularization term을 통해 XGBoost는 복잡한 모델에 패널티를 부여함</p>
</blockquote>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136078326-964b2d5e-2e10-47a7-9300-e2cd383c810c.png" alt="gradient4" /></p>

<p><br /></p>

<p>LighGBM</p>
<blockquote>
  <p>XGBoost와 다르게 lear-wise loss 사용 (loss를 더 줄일 수 있음)<br />
XGBoost 대비 2배 이상 빠른 속도 (동일 파라미터 기준)<br />
과대적합에 민감하여, 대량의 학습데이터를 필요로 함</p>
</blockquote>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136078359-4d3bf782-bcbe-4e5d-b101-b5ccae3808f4.png" alt="gradient5" /></p>

<p><br /></p>

<hr />

<p><br /></p>

<h5 id="장단점과-매개변수-2"><span style="color:teal">장단점과 매개변수</span></h5>

<p><br /></p>

<p>장점</p>
<blockquote>
  <p>이진 특성이나 연속적인 특성에도 잘 작동합니다. <br />
데이터의 스케일에 구애받지 않습니다.</p>
</blockquote>

<p>단점</p>
<blockquote>
  <p>매개변수의 조정이 필수입니다.<br />
휸련시간이 깁니다.  <br />
차원이 높고 희소한 데이터에 잘 작동하지 않습니다.</p>
</blockquote>

<p>N_estimators가 클수록 랜덤 포레스트는 좋았지만 그래이디언트 부스팅에서는 과대적합될 가능성이 높아집니다.<br />
N_estimator을 정하고 난 뒤에 learning_rate를 정하게 되는데 learning_rate를 낮추면 비슷한 복잡도의 모델을 만들기 위해 더 많은 트리를 추가해야합니다.</p>

<p><br /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">매개변수</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>n_estimators</strong></td>
      <td>- 트리의 개수를 지정<br />- 커지면 모델이 복잡해지고 과대적합 가능성 높아짐</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>learning_rate</strong></td>
      <td>- 관례상 n_estimators를 맞추고 learning_rate를 찾음<br /> - 이전 트리의 오차를 보정하는 정도</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>n_iter_no_change / validation_fraction</strong></td>
      <td>-조기 종료를 위한 매개변수 (default값: n_iter_no_change =None (조기 종료 x), validation_fraction=0.1) <br />- validation_fraction 비율만큼 검증 데이터로 사용하여 n_iter_no_change 만큼 반복하여 향상되지 않으면 훈련 종료</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>max_depth / max_leaf_nodes</strong></td>
      <td>-각 트리의 복잡도를 낮춤 <br />- max_depth는 보통 매우 작게 설정하며 트리의 깊이가 5보다 깊어지지 않게 함</td>
    </tr>
  </tbody>
</table>

<p><br />
<br />
<br /></p>

<h3 id="배깅-엑스트라-트리-에이다부스트">배깅, 엑스트라 트리, 에이다부스트</h3>
<p><span style="color:maroon">Bagging</span></p>

<p>from sklearn.ensemble import BaggingClassifier<br />
배깅은 중복을 허용한 랜덤샘플링으로 만든 훈련 세트를 사용해 분류기를 각기 다르게 학습합니다.<br />
랜덤포레스트는 배깅의 일종이지만 설명변수도 무작위로 선택하는 것이 차이가 있습니다.<br />
predict_proba() 지원하면 메서드를 통해 확률값을 평균하여 예측을 수행합니다. (지원하지 않는다면 가장 빈도가 높은 클래스 레이블)<br />
oob_score=True로 지정하면 매개변수는 부트스트래핑에 포함되지 않은 샘플로 훈련된 모델을 평가할 수 있습니다. (OOB 오차, default=False)<br />
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">bagging</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">bagging</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">bagging</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_test</span><span class="p">,</span> <span class="n">yc_test</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"OOB 샘플의 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">bagging</span><span class="p">.</span><span class="n">oob_score_</span><span class="p">))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.953</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.951</span>
<span class="n">OOB</span> <span class="n">샘플의</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.946</span>
</code></pre></div></div>
<p><br /></p>

<p>배깅은 랜덤포레스트와 달리 max_samples에 부트스트랩 샘플의 크기를 정할 수 있습니다. <br />
또한 로지스틱 회귀가 들어갈 수도 있고 결정 트리가 들어갈 수도 있습니다.</p>

<p><br /></p>

<hr />
<p><br /></p>

<p><span style="color:lime">Extra Tree</span></p>

<p>후보 특성을 무작위로 분할한 다음 최적의 분할을 찾습니다.<br />
엑스트라 트리도 랜덤 포레스트와 비슷하지만 splitter=‘random’을 사용합니다. 랜덤 포레스트는 splitter=‘best’가 고정입니다.<br />
Splitter=‘best’의 의미는 모든 변수의 정보 이득을 계산하고 그중 가장 설명력이 높은 변수를 선택하는 것입니다.<br />
또한 부트스트랩 샘플링을 적용하지 않습니다. <br />
무작위성을 증가시키면 모델 편향은 늘어나지만 분산이 감소하는 모습을 보입니다.<br />
개별 트리는 매우 복잡하지만 결정 경계는 안정적입니다.<br />
계산 비용은 위 splitter에서의 feature의 차이 때문에 랜덤 포레스트보다 적지만  일반화 성능을 높이려면 많은 트리를 만들어야합니다.<br />
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="n">xtree</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">xtree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">xtree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">xtree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_test</span><span class="p">,</span> <span class="n">yc_test</span><span class="p">))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">1.000</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.972</span>
</code></pre></div></div>

<p><br /></p>

<hr />
<p><br /></p>

<p><span style="color:gray">Adaptive Boosting</span></p>

<p>이전의 모델이 잘못 분류한 샘플에 가중치를 높여서 다음 모델을 훈련합니다.<br />
훈련된 각 모델은 성능에 따라 가중치 부여합니다.<br />
예측을 만들 때는 모델이 예측한 레이블을 기준으로 모델의 가중치를 합산하여 가장 높은 값을 가진 레이블을 선택합니다.<br />
AdaBoostClassifier은 기본값으로 DecisionTreeClassifier(max_depth=1)를 갖습니다.<br />
AdaBoostRegressor은 기본값으로 DecisionTreeRegressor(max_depth=3)을 갖습니다. (base_estimator을 이용하여 다른 모델 지정 가능)</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136081550-a82cd75f-c6fd-49d2-83e9-8fedd01c4d5e.png" alt="ada1" /></p>

<p>에이다 부스팅의 원리와 수식
<br /></p>

<p><img src="https://user-images.githubusercontent.com/54880474/136081678-0d0819ea-b048-43f2-a786-c470918bd8d1.png" alt="ada2" /></p>

<p><br /></p>

<p>예측정확도와 가중치의 곱의 합이 되어 높은 정확도를 만들게 됩니다.<br />
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="n">ada</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">ada</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">ada</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">ada</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_test</span><span class="p">,</span> <span class="n">yc_test</span><span class="p">)))</span>

<span class="n">훈련</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">1.000</span>
<span class="n">테스트</span> <span class="n">세트</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.986</span>
</code></pre></div></div>
<p><br />
<br /></p>

<h3 id="커널-서포트-벡터-머신">커널 서포트 벡터 머신</h3>

<p>커널 서포트 벡터 머신은 보통 SVM이라고 한다.<br />
입력 데이터에서 단순한 초평면으로 정의되지 않는 더 복잡한 모델을 만들 수 있도록 확장한 것이다.<br />
분류와 회귀 모두 사용 가능하다. (SVC는 분류, SVR은 회귀)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span> <span class="c1">#선형 모델
</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">''</span><span class="p">,</span><span class="n">C</span><span class="o">=</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="p">)</span> <span class="c1"># kernel, C, gamma 파라미터 존재
</span></code></pre></div></div>

<h6 id="선형-모델과-비선형-특성">선형 모델과 비선형 특성</h6>
<p>선형 모델은 직선으로만 데이터 포인트를 나눌 수 있어 밑에 같은 데이터는 잘 들어맞지 않는다.<br />
SVM 모델은 3차원에서 2차원으로 투영해본다면 더이상 선형 모델이 아니다.</p>

<h6 id="커널-기법">커널 기법</h6>
<p>커널 기법은 실제로 데이터를 확장하지 않고 확장된 특성에 대한 데이터 포인트들의 거리를 계산한다.<br />
$ (특성1)^2 * (특성2)^5 $하는 다항식 커널이 있고 가우시안 커널로 불리우는 RBF 커널이 있다.<br />
가우시안 커널은 차원이 무한한 특성 공간에 매핑하는 것이다.<br />
모든 차수의 모든 다항식을 고려하지만 특성의 중요도는 고차항이 될수록 줄어든다.</p>

<h6 id="svm-이해하기">SVM 이해하기</h6>
<p>두 클래스 사이에 경계한 데이터 포인트들을 서포트 벡터라고 한다.<br />
새로운 데이터 포인트에 대해 예측하려면 각 서포트 벡터와의 거리를 측정한다.<br />
서포트 벡터의 중요도는 훈련 과정에서 학습하는데 dual_coef_ 속성에 저장된다.</p>
<h1 id="가우시안-커널-공식-사진">가우시안 커널 공식 사진</h1>
<p>가우시안 커널에 의해 계산되며 $ X_1, X_2 $는 데이터 포인트이며 $ ||X_1 - X_2|| $는 유클리디안 거리이고 $Γ$ 은 가우시안 커널의 폭을 제어하는 매개변수이다.</p>

<h6 id="svm-매개변수-튜닝">SVM 매개변수 튜닝</h6>
<p>$Γ$는 가우시안 커널 폭의 역수에 해당하는데 하나의 훈련 샘플이 미치는 영향의 범위를 결정한다.(1~0 사이의 범위이다.)<br />
작은 값은 넓은 영역을 뜻하고, 큰 값은 영향이 미치는 범위가 제한적이다.<br />
즉 커널의 반경이 클수록 훈련 샘플의 영향 범위도 커진다.<br />
작은 $Γ$ 값은 모델의 복잡도를 낮출 수 있다.<br />
C 매개 변수는 규제 매개변수이다. dual_coef_값을 제한합니다.<br />
작은 C는 매우 제약이 큰 모델을 만들고 각 데이터 포인트의 영향력이 작다.<br />
C를 증가시키면 이 포인트들이 영향을 크게 줘서 결정 경계를 휘게 만든다.</p>

<h6 id="svm을-위한-데이터-전처리">SVM을 위한 데이터 전처리</h6>
<p>커널 SVM에서는 데이터셋의 특성 자릿수가 완전히 다르면 영향을 크게 미친다.<br />
따라서 특성 값을 평균이 0이고 단위 분산이 되도록 하거나, 0과 1 사이로 맞추는 방법을 많이 사용한다.(StandardScaler와 MinMaxScalar)</p>

<h6 id="장단점과-매개변수-3">장단점과 매개변수</h6>
<p>SVM은 저차원과 고차원의 데이터에 모두 잘 작동하지만 샘플이 많으면 잘 맞지 않는다.<br />
또한 전처리와 매개변수 설정에 신경을 많이 써야하는데 그래서 랜덤 포레스트나 그레이디언트 부스팅과 같은 전처리가 거의 필요 없는 트리 기반 모델이 선호된다.<br />
SVM은 분석도 어려워서 예측이 어떻게 결정되었는지 설명하기가 난해하다.<br />
하지만 모든 특성이 비슷한 단위이고 스케일이 비슷하다면 시도해볼 만하다.<br />
중요한 매개변수는 C이고 어떤 커널을 사용할지와 각 커널에 따른 매개변수이다.<br />
RBF는 $Γ$ 매개변수를 갖지만 다른 커널 종류도 많다.</p>

<h3 id="신경망-딥러닝">신경망 (딥러닝)</h3>
<p>다층 퍼셉트론은(MLP)는 간단하게 분류와 회귀에서 쓰일 수 있다.</p>

<h6 id="신경망-모델">신경망 모델</h6>
<p>MLP는 여러 단계를 거처 결정을 만들어내는 선형 모델의 일반화된 모습이다.</p>
<h1 id="선형-회귀-모델의-예측-공식-사진">선형 회귀 모델의 예측 공식 사진</h1>
<p>$\hat Y $는 x[0]에서 x[p]까지의 입력특성과 학습된 계수의 가중치의 합이다.</p>

<h1 id="퍼셉트론-사진">퍼셉트론 사진</h1>
<p>왼쪽 노드는 입력 특성을 나타내며 연결선은 학습된 계수를 표현하고 오른쪽 노드는 입력의 가중치 합, 즉 출력을 나타낸다.<br />
MLP는 가중치 합을 만드는 과정이 여러 번 반복되며 먼저 중간 단계를 구성하는 은닉 유닛을 계산하고 이를 이용하여 최종 결과를 산출하기 위해 다시 가중치 합을 계산한다.</p>
<h1 id="다중-퍼셉트론-사진">다중 퍼셉트론 사진</h1>
<p>각 은닉 유닛의 가중치 합을 계산한 후 결과에 비선형 함수인 렐루나 하이퍼볼릭 탄젠트, 시그모이드 함수를 적용합니다.</p>
<h1 id="회귀-분석-사진">회귀 분석 사진</h1>
<p>w는 입력 x와 은닉층 h 사이의 가중치이고, v는 은닉층 h와 출력 $\hat Y$ 사이의 가중치입니다.<br />
w와 v는 훈련 데이터에서 학습하고 x는 입력 특성이며 $ \hat Y $는 계산된 출력, h는 중간 계산값 입니다.</p>

<h6 id="신경망-튜닝">신경망 튜닝</h6>
<p>더 복잡도가 낮은 모델을 만들고 싶다면 hidden_layer_size를 통해 은닉 유닛의 개수를 줄인다.<br />
은닉 유닛을 추가하거나, 은닉층을 추가하거나 활성화함수를 바꾸면 더 매끄러운 결정 경계를 얻을 수도 있다.<br />
선형 분류와 리지 회귀 처럼 L2 페널티를 사용해서 가중치를 0에 가깝게 감소시킬 수도 있다.(default는 매우 낮다)<br />
신경망에서는 학습을 시작하기 전에 가중치를 무작위로 설정하며 이 무작위한 초기화가 모델의 학습에 영향을 준다.<br />
따라서 같은 매개변수를 사용하더라도 초깃값이 다르면 모델이 많이 달라질 수 있다.<br />
신경망도 입력 특성이 평균은 0 분산이 1이 되도록 변형하는 것이 좋다.<br />
은닉 유닛에서 작은 가중치를 가진 특성은 모델에 덜 중요하다고 추론할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span> <span class="c1"># MLP분류
</span><span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s">''</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">''</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="p">,</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">[,],</span><span class="n">max_itter</span><span class="o">=</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="p">)</span> 
<span class="c1">#solver에 최적화 알고리즘,activation에 활성화 함수 ,hidden_layer_size로 은닉 유닛의 개수 설정(default=100),max_itter은 반복 횟수,alpha는 L2 페널티
</span></code></pre></div></div>

<h6 id="장단점과-매개변수-4">장단점과 매개변수</h6>
<p>머신러닝 알고리즘을 뛰어넘는 성능을 보일 수 있지만 학습이 오래걸리고 데이터 전처리를 주의해서 해야한다.<br />
모든 특성이 같은 의미를 가지면 SVM, 다른 종류의 특성이라면 트리 기반 메딜이 더 잘 작동할 수 있다.</p>

<h6 id="신경망의-복잡도-추정">신경망의 복잡도 추정</h6>
<p>가장 중요한 매개변수는 은닉층의 개수와 각 은닉층의 유닛 수이다.<br />
복잡도에 관해 연관된 측정치는 학습된 가중치 또는 계수의 수이다.<br />
특성이 100개 은닉 유닛 100개인 이진 분류라면 입력층과 첫 번째 은닉층 사이에는 편향을 포함하여 $ 100 * 100 + 100 = 10100 $개의 가중치가 있습니다.<br />
은닉층과 출력층 사이에 $ 100 * 1 + 1 = 101 $개의 가중치가 더 있어 가중치는 10201개 이다.<br />
이렇게 가중치는 은닉층을 추가할수록 훨씬 커지게 된다.<br />
매개변수를 조정하는 일반적인 방법은 충분히 과대적합되어 문제를 해결할만한 큰 모델을 만든 뒤 훈련 데이터가 충분히 학습될 수 있다고 생각되면 신경망 구조를 줄이거나 규제 강화를 위해 alpha 값을 증가시켜 일반화 성능을 향상시킨다.<br />
층의 개수, 층당 유닛 개수, 규제, 비선형성으로 모델 구성을 할 수 있으며, solver 매개변수를 통해서 학습시키는 방법을 지정할 수 있다.<br />
solver의 경우 기본값은 adam이고 데이터 스케일에 민감하다.<br />
lbfgs는 안정적이지만 규모가 크면 시간이 오래 걸린다<br />
sgd는 momentum과 nesterovs_momentom의 영향을 받는데 다른 여러 매개변수와 함께 튜닝하여 최선의 결과를 만들 수 있다.</p>

<h3 id="분류-예측의-불확실성-추정">분류 예측의 불확실성 추정</h3>
<p>decision_function과 predict_proba로 추정 할 수 있다.</p>

<h6 id="결정-함수">결정 함수</h6>
<p>decision_function의 반환값의 크기는 (n_samples,)이며각 샘플이 하나의 실수 값을 반환한다.<br />
모델이 데이터 포인트가 양성 클래스인 클래스 1에 속한다고 믿는 정도이다.<br />
즉, 음수값은 다른 클래스에 속함을 의미한다.<br />
값의 범위는 데이터와 모델 파라미터에 따라 달라지게 된다.</p>

<h6 id="예측-확률">예측 확률</h6>
<p>predict_proba의 출력은 각 클래스에 대한 확률이고 이진 분류에서 이 값의 크기는 항상 (n_samples,2)이다.<br />
두 클래스의 확률 합은 1이므로 두 클래스 중 하나는 50% 이상의 확신을 가질 것이고 그 클래스가 예측값이 된다.<br />
데이터에 있는 불확실성이 얼마나 이 값에 잘 반영되는지는 모델과 매개변수 설정에 달렸다.<br />
그래서 과대적합된 모델 혹은 잘못된 예측도 예측의 확신이 강한 편이다.<br />
복잡도가 낮을 수록 예측에 불확실성이 더 많다.<br />
불확실성과 모델의 정확도가 동등하면 이 모델이 보정되었다고 한다.</p>

<h6 id="다중-분류에서의-불확실성">다중 분류에서의 불확실성</h6>
<p>다중 분류에서도 decision_funcion과 predict_proba를 사용할 수 있다.<br />
decision_function에서는 (n_samples, n_classes)가 결과값이 된다.<br />
글 클래스에 대한 확신 점수를 담고 그 수치가 크면 그 클래스일 가능성이 크다.<br />
데이터 포인트마다 점수들에서 가장 큰 값을 찾아 예측 결과를 재현할 수 있다.<br />
predict_proba는 (n_samples,n_classes)가 출력값이 된다.<br />
마찬가지로 각 데이터 포인트에서 클래스 확률의 합은 1이다.<br />
argmax 함수를 적용해서 예측 결과를 재현할 수 있지만 클래스가 문자열이거나 정수형을 사용하지만 연속적이지 않고 0부터 시작하지 않을 수 있다.<br />
따라서 predict 결과와 decision_function, predict_proba의 결과를 비교하기 위해서는 분류기의 classes_ 속성을 사용해 클래스의 실제 이름을 얻어야 한다.</p>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            <!--
            
                <section class="subscribe-form">
                    <h3 class="subscribe-form-title">Subscribe to 주건나's Blog</h3>
                    <p>Get the latest posts delivered right to your inbox</p>
                    <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                </section>
            
            -->
            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                        <section class="author-card">
                            
                                <img class="author-profile-image" src="/assets/built/images/economy-author-logo.jpg" alt="keonju" />
                            
                            <section class="author-card-content">
                                <h4 class="author-card-name"><a href="/author/keonju">주건나</a></h4>
                                
                                    <p>데이터 / ML / DL / AI 관련 공부중입니다.</p>
                                
                            </section>
                        </section>
                        <div class="post-full-footer-right">
                            <a class="author-card-button" href="/author/keonju">Read More</a>
                        </div>
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            var this_page_url = 'https://keonju2.github.io/study-ML2';
                            var this_page_identifier = '/study-ML2';
                            var this_page_title = '머신러닝 정리 (2) <br> 지도학습 (2)';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://xxxxxxxx.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/built/images/blog-cover1.jpg)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; 주건나's Blog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/study/">Study</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/study-book1">책 서평 (1) <br> 텐초의 파이토치 딥러닝 특강</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/study-ML6">머신러닝 정리 (6) <br> 모델 평가와 성능 향상</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/study-ML5">머신러닝 정리 (5) <br> 데이터 표현과 특성 공학</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/study/">
                                
                                    See all 6 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/programming-baekjoon5">
                <div class="post-card-image" style="background-image: url(/assets/built/images/baekjoon-logo.png)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/programming-baekjoon5">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Programming</span>
                            
                        
                    

                    <h2 class="post-card-title">백준 (5) 정렬 알고리즘 <br> (2750,11399,2751,1427, <br> 10989,1181,11650,2309)</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>백준 관련 글 백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101) 백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920) 백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157,</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/built/images/economy-author-logo.jpg" alt="주건나" />
                        
                        <span class="post-card-author">
                            <a href="/author/keonju/">주건나</a>
                        </span>
                    
                
                <span class="reading-time">
                    
                    
                      3 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/programming-baekjoon4">
                <div class="post-card-image" style="background-image: url(/assets/built/images/baekjoon-logo.png)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/programming-baekjoon4">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Programming</span>
                            
                        
                    

                    <h2 class="post-card-title">백준 (4) <br> (1157, 1546, 2577, 2675, 2908, <br> 1018, 1436, 1259, 7568, 10250)</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>백준 관련 글 백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101) 백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920) 백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157,</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/built/images/economy-author-logo.jpg" alt="주건나" />
                        
                        <span class="post-card-author">
                            <a href="/author/keonju/">주건나</a>
                        </span>
                    
                
                <span class="reading-time">
                    
                    
                      5 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="https://keonju2.github.io/">
            
            <span>주건나's Blog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">머신러닝 정리 (2) <br> 지도학습 (2)</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D+%EC%A0%95%EB%A6%AC+%282%29+%3Cbr%3E+%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5+%282%29&amp;url=https://keonju2.github.io/study-ML2"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://keonju2.github.io/study-ML2"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://keonju2.github.io/">주건나's Blog</a> &copy; 2023</section>
                <!--
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                -->
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    <a href="https://facebook.com/monkeykeonju" target="_blank" rel="noopener">Facebook</a>
                    
                    <!--
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                     -->
                </nav>

            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
    <div id="subscribe" class="subscribe-overlay">
        <a class="subscribe-overlay-close" href="#"></a>
        <div class="subscribe-overlay-content">
            
            <h1 class="subscribe-overlay-title">Search 주건나's Blog</h1>
            <p class="subscribe-overlay-description">
            검색어를 입력해주세요 </p>
            <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
        </div>
    </div>
    


    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-6FJ2289869', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
