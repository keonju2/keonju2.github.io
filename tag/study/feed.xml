<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://keonju2.github.io/tag/study/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://keonju2.github.io/" rel="alternate" type="text/html" />
  <updated>2021-10-14T19:29:43+09:00</updated>
  <id>https://keonju2.github.io/tag/study/feed.xml</id>

  
  
  

  
    <title type="html">주건나’s Blog | </title>
  

  
    <subtitle>데이터 사이언티스트를 꿈꾸는 블로그입니다.</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">머신러닝 정리 (3) 비지도학습 (1)</title>
      <link href="https://keonju2.github.io/study-ML3" rel="alternate" type="text/html" title="머신러닝 정리 (3) &lt;br&gt; 비지도학습 (1)" />
      <published>2021-10-12T10:00:00+09:00</published>
      <updated>2021-10-12T10:00:00+09:00</updated>
      <id>https://keonju2.github.io/study-ML3</id>
      <content type="html" xml:base="https://keonju2.github.io/study-ML3">&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;머신러닝 공부 관련 글&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML1&quot;&gt;머신러닝 정리 (1)-지도학습 (1)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML2&quot;&gt;머신러닝 정리 (2)-지도학습 (2)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML3&quot;&gt;머신러닝 정리 (3)-비지도학습 (1)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;머신러닝-정리-3---비지도학습-1&quot;&gt;머신러닝 정리 (3) - 비지도학습 (1)&lt;/h1&gt;
&lt;p&gt;본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.&lt;br /&gt;
또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.&lt;br /&gt;
글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.&lt;br /&gt;
코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;소스 코드: &lt;a href=&quot;https://github.com/rickiepark/introduction_to_ml_with_python&quot;&gt;https://github.com/rickiepark/introduction_to_ml_with_python&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;비지도학습-1&quot;&gt;비지도학습 (1)&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;비지도 학습의 종류&lt;/li&gt;
  &lt;li&gt;비지도 학습의 도전과제&lt;/li&gt;
  &lt;li&gt;데이터 전처리와 스케일 조정&lt;/li&gt;
  &lt;li&gt;여러 가지 전처리 방법
    &lt;ol&gt;
      &lt;li&gt;StandardScaler&lt;/li&gt;
      &lt;li&gt;RobustScaler&lt;/li&gt;
      &lt;li&gt;MinMaxScaler&lt;/li&gt;
      &lt;li&gt;Nomarlizer&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터 변환 적용하기&lt;/li&gt;
  &lt;li&gt;Quantile Transformer 와 Power Transformer
    &lt;ol&gt;
      &lt;li&gt;Quantile Transformer&lt;/li&gt;
      &lt;li&gt;Power Transformer&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;훈련 데이터와 테스트 데이터의 스케일을 같은 방법으로 조정하기&lt;/li&gt;
  &lt;li&gt;지도 학습에서 데이터 전처리 효과&lt;/li&gt;
  &lt;li&gt;차원 축소, 특성 추출, 매니폴드 학습
    &lt;ol&gt;
      &lt;li&gt;주성분 분석(PCA)
        &lt;ol&gt;
          &lt;li&gt;유방암 데이터 셋 시각화&lt;/li&gt;
          &lt;li&gt;고유얼굴 특성 추출&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;비지도-학습의-종류&quot;&gt;비지도 학습의 종류&lt;/h3&gt;

&lt;p&gt;책에서는 두 가지 비지도 학습을 공부합니다.&lt;br /&gt;
비지도 변환과 군집입니다.&lt;br /&gt;
비지도 변환은 데이터를 새롭게 표현하여 사람이나 다른 머신러닝 알고리즘이 원래 데이터보다 쉽게 해석할 수 있도록 만드는 알고리즘입니다.&lt;br /&gt;
특히 고차원 데이터를 특성의 수를 줄이면서 꼭 필요한 특징을 포함한 데이터로 표현하는 방법인 차원축소가 대표적인 예입니다.&lt;/p&gt;

&lt;p&gt;비지도 변환으로 데이터를 구성하는 단위나 성분을 찾기도 합니다.&lt;br /&gt;
텍스트 문서에서 주제를 추출하는 것이 예입니다.&lt;br /&gt;
소셜 미디어에서 선거, 총기 규제, 팝스타 같은 주제로 일어나는 토론을 추적할 때 사용한다고 합니다.&lt;/p&gt;

&lt;p&gt;군집 알고리즘은 데이터를 서로 비슷하게 그룹으로 묶는 것 입니다.&lt;br /&gt;
같은 사람이 찍힌 사진을 같은 그룹으로 묶게 추천해주는 것을 생각하면 이해하기 쉽습니다.&lt;/p&gt;

&lt;h3 id=&quot;비지도-학습의-도전과제&quot;&gt;비지도 학습의 도전과제&lt;/h3&gt;

&lt;p&gt;비지도 학습에서 가장 어려운 일은 알고리즘이 유용한 가를 평가하는 것 입니다.&lt;br /&gt;
비지도 학습은 보통 레이블이 없어서 어떤 것이 올바른 것인지 모릅니다.&lt;br /&gt;
그래서 비지도 학습의 결과를 평가하기 위해서는 직접 확인하는 것이 유일한 방법일 때도 있다고 합니다.&lt;/p&gt;

&lt;p&gt;비지도 학습 알고리즘은 데이터를 잘 이해하고 싶을 때 탐색적 분석 단계에서도 많이 사용합니다.&lt;br /&gt;
전처리 단계에서도 사용되는데 비지도 학습 결과를 사용하여 학습하면 지도 학습의 정확도가 좋아지기도 하고 메모리나 시간 절약도 할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;데이터-전처리와-스케일-조정&quot;&gt;데이터 전처리와 스케일 조정&lt;/h3&gt;

&lt;p&gt;신경망이나 SVM과 같은 스케일에 민감한 알고리즘은 데이터 특성 값을 조정해야합니다.&lt;/p&gt;

&lt;h3 id=&quot;여러-가지-전처리-방법&quot;&gt;여러 가지 전처리 방법&lt;/h3&gt;

&lt;h6 id=&quot;standardsclaer&quot;&gt;StandardSclaer&lt;/h6&gt;

&lt;p&gt;각 특성의 평균을 0, 분산을 1로 변경하여 모든 특성이 같은 크기를 가지게 합니다.&lt;br /&gt;
특성의 최솟값과 최댓값의 크기를 제한하지는 않습니다.&lt;/p&gt;

\[z= \frac {x−μ} σ ( z점수= \frac{자료값-평균} {표준편차})\]

&lt;h6 id=&quot;robustscaler&quot;&gt;RobustScaler&lt;/h6&gt;

&lt;p&gt;특성이 같은 스케일을 갖게되지만 평균과 분산 대신 중간 값과 사분위 값을 사용합니다.&lt;br /&gt;
따라서 이상치에 영향을 받지 않습니다.&lt;/p&gt;

&lt;h6 id=&quot;minmaxscaler&quot;&gt;MinMaxScaler&lt;/h6&gt;

&lt;p&gt;모든 특성이 정확하게 0과 1 사이에 위치하도록 변경합니다.&lt;br /&gt;
2차원 데이터셋일 경우에는 모든 데이터가 x 축의 0과 1, y축의 0과 1 사이의 사각 영역에 담기게 됩니다.&lt;br /&gt;
(q는 각 사분위값을 뜻합니다.)&lt;/p&gt;

\[\frac {x-q~2} {q~3-q~1}\]

&lt;h6 id=&quot;nomarlizer&quot;&gt;Nomarlizer&lt;/h6&gt;

&lt;p&gt;특성 벡터의 유클리디안 길이가 1이 되도록 데이터 포인트를 조정합니다.&lt;br /&gt;
다른 말로 하면 지름이 1인 원(3차원에서는 구)에 들어옵니다.&lt;br /&gt;
각 데이터 포인트가 다른 비율로 조정됩니다.  특성 벡터의 길이는 상관 없고 데이터의 방향이 중요할 때 많이 사용합니다.&lt;/p&gt;

&lt;h3 id=&quot;데이터-변환-적용하기&quot;&gt;데이터 변환 적용하기&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X_train_scaled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;변환된 후 크기:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;스케일 조정 전 특성별 최소값:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;스케일 조정 전 특성별 최대값:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;스케일 조정 후 특성별 최소값:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;스케일 조정 후 특성별 최대값:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;변환된 후 크기: (426, 30)
스케일 조정 전 특성별 최소값:
[  6.981   9.71   43.79  143.5     0.053   0.019   0.      0.      0.106
0.05    0.115   0.36    0.757   6.802   0.002   0.002   0.      0.
0.01    0.001   7.93   12.02   50.41  185.2     0.071   0.027   0.
0.      0.157   0.055]
스케일 조정 전 특성별 최대값:
[  28.11    39.28   188.5   2501.       0.163    0.287    0.427    0.201
    0.304    0.096    2.873    4.885   21.98   542.2      0.031    0.135
    0.396    0.053    0.061    0.03    36.04    49.54   251.2   4254.
    0.223    0.938    1.17     0.291    0.577    0.149]
스케일 조정 후 특성별 최소값:
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0. 0. 0. 0. 0. 0.]
스케일 조정 후 특성별 최대값:
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
1. 1. 1. 1. 1. 1.]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;변환 된 값들이 모두 0과 1 사이가 된 것을 알 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;X_test_scaled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;스케일 조정 후 특성별 최소값:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;스케일 조정 후 특성별 최대값:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;스케일 조정 후 특성별 최소값:
[ 0.034  0.023  0.031  0.011  0.141  0.044  0.     0.     0.154 -0.006
-0.001  0.006  0.004  0.001  0.039  0.011  0.     0.    -0.032  0.007
0.027  0.058  0.02   0.009  0.109  0.026  0.     0.    -0.    -0.002]
스케일 조정 후 특성별 최대값:
[0.958 0.815 0.956 0.894 0.811 1.22  0.88  0.933 0.932 1.037 0.427 0.498
0.441 0.284 0.487 0.739 0.767 0.629 1.337 0.391 0.896 0.793 0.849 0.745
0.915 1.132 1.07  0.924 1.205 1.631]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;테스트 세트의 최솟값과 최댓값은 0과 1이 아닐 수 있다.&lt;br /&gt;
테스트 세트의 최솟값과 범위를 사용하지 않고 훈련 세트의 최솟값을 빼고 훈련 세트의 범위로 나누기 때문이다.&lt;br /&gt;
MinMaxScaler을 사용하려면 항상 테스트와 훈련 세트 모두 같은 변환을 해야한다.&lt;/p&gt;

&lt;h3 id=&quot;quantile-transformer-와-power-transformer&quot;&gt;Quantile Transformer 와 Power Transformer&lt;/h3&gt;

&lt;h6 id=&quot;quantile-transformer&quot;&gt;Quantile Transformer&lt;/h6&gt;
&lt;p&gt;Quantile Transformer은 1000개의 분위를 사용하여 데이터를 균등하게 분포시킵니다.&lt;br /&gt;
RobustScaler과 비슷하게 이상치에 민감하지 않으며 전체 데이터를 0과 1 사이로 압축합니다.&lt;/p&gt;

&lt;p&gt;분위 수는 n_quantiles 매개 변수로 설정할 수 있으며 속성의 크기는 (n_quantiles,n_features) 입니다.&lt;/p&gt;

&lt;p&gt;output_distribution 매개변수를 통해 균등 분포에서 정규 분포로 출력을 바꿀 수도 있습니다.&lt;/p&gt;

&lt;h6 id=&quot;power-transformer&quot;&gt;Power Transformer&lt;/h6&gt;

&lt;p&gt;Power Transformer는 method 매개변수에 ‘yeo-johnson’과 ‘box-cox’ 알고리즘을 지정할 수 있습니다.&lt;br /&gt;
어떤 변환이 정규 분포에 가깝게 변환할지 사전에 알기 힘들기 때문에 히스토그램으로 확인해보는 것이 좋습니다.&lt;/p&gt;

&lt;h3 id=&quot;훈련-데이터와-테스트-데이터의-스케일을-같은-방법으로-조정하기&quot;&gt;훈련 데이터와 테스트 데이터의 스케일을 같은 방법으로 조정하기&lt;/h3&gt;

&lt;p&gt;지도 학습에서는 훈련 세트와 테스트 세트에 같은 변환을 적용하는 것이 중요합니다.&lt;br /&gt;
잘 조정된 데이터는 같은 비율로 데이터를 바꿔 원본 데이터와 비율만 다른 그래프를 보여주지만 잘못 조정된 데이터는 배열이 엉망이 되어서 결과에 영향을 미칩니다.&lt;/p&gt;

&lt;h3 id=&quot;지도-학습에서-데이터-전처리-효과&quot;&gt;지도 학습에서 데이터 전처리 효과&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.svm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;svm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;auto&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;svm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;테스트 세트 정확도: {:.2f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;테스트 세트 정확도: 0.63
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 0~1 사이로 스케일 조정
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train_scaled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_test_scaled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;svm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;스케일 조정된 테스트 세트의 정확도: {:.2f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;스케일 조정된 테스트 세트의 정확도: 0.95
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 평균 0, 분산 1을 갖도록 스케일 조정
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train_scaled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_test_scaled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;svm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SVM 테스트 정확도: {:.2f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SVM 테스트 정확도: 0.97
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;차원-축소-특성-추출-매니폴드-학습&quot;&gt;차원 축소, 특성 추출, 매니폴드 학습&lt;/h2&gt;

&lt;p&gt;주성분 분석은 가장 많이 사용되는 데이터 변환 방법입니다.&lt;br /&gt;
특성 추출에서는 비음수 행렬 분해가 많이 사용됩니다.&lt;br /&gt;
2차원 산점도를 이용한 시각화 용도로 사용되는 t-SNE 알고리즘도 있습니다.&lt;/p&gt;

&lt;h6 id=&quot;차원-축소&quot;&gt;차원 축소&lt;/h6&gt;

&lt;blockquote&gt;
  &lt;p&gt;차원 축소의 필요성&lt;br /&gt;
관측 치의 수는 한정되어 있습니다.&lt;br /&gt;
차원이 커질 수록 한정된 자료는 커진 차원의 패턴을 잘 설명하지 못하고 복잡도가 기하급수적으로 늘어나게 됩니다.&lt;br /&gt;
상관계수가 높은 변수 중 일부만 분석하게 된다면 정보의 손실이 발생하게 됩니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;차원 축소의 여러 방법&lt;br /&gt;
    &amp;gt; principal component&lt;br /&gt;
    &amp;gt; 변수 선택법&lt;br /&gt;
    &amp;gt; penalty 기반 regression&lt;br /&gt;
    &amp;gt; convolutional neural network&lt;br /&gt;
    &amp;gt; drop out &amp;amp; bagging&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;차원 축소의 활용&lt;br /&gt;
    &amp;gt; 차원 축소를 통해 데이터를 잘 설명할 수 있는 잠재적 요소 추출&lt;br /&gt;
    &amp;gt; 이미지 분류, 시맨틱, 토픽 분류 등&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h6 id=&quot;주성분-분석pca&quot;&gt;주성분 분석(PCA)&lt;/h6&gt;

&lt;p&gt;공분산 행렬 개념은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/137047363-a46303f4-dd06-40d9-9dda-08f6cae06282.png&quot; alt=&quot;pca_1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Principal Components의 개념은 차원을 줄이면서 정보 손실을 최소화하는 방법입니다.&lt;br /&gt;
더 적은 개수로 데이터를 충분히 잘 설명할 수 있는 새로운 축을 찾아냅니다.&lt;br /&gt;
공분산이 데이터의 형태를 변형시키는 방향의 축과 그것에 직교하는 축을 찾습니다.&lt;br /&gt;
2차원의 경우 공분산이 나타내는 타원의 장축과 단축입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/137047763-0be3a41d-eaca-4593-9e85-45aae3e5711f.png&quot; alt=&quot;pca_2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PC score은 새로 찾아낸 축에서의 좌표값을 의미하는데 기존 값을 새로운 축에 내린 정사영입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/137047800-70ed98ac-678b-4669-80d5-06c743257f28.png&quot; alt=&quot;pca_3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;즉, PCA는 데이터의 분산을 최대한 보존하면서 서로 직교하는 새 기저(축)을 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법입니다.&lt;br /&gt;
데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있고, 이렇게 분해된 고유벡터를 이용해 입력데이터를 선형변환하는 것이 PCA입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/137047828-2661edd3-059e-44b9-95a5-828c20077920.png&quot; alt=&quot;pca_4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/137047856-4a349a57-2de4-4bb5-8152-32f4873237b5.png&quot; alt=&quot;pca_5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PCA의 수학적개념_Singular Value Decomposition(SVD)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/137048016-ec470e89-f796-423d-b924-f510b8be5fe3.png&quot; alt=&quot;pca_6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SVD와 eigen value, eigen vector의 연관성&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/137048085-3cbe387a-2d02-47ff-b29e-a882ed70787e.png&quot; alt=&quot;pca_7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;주성분 분석은 특성들이 통계적으로 상관관계가 없도록 데이터셋을 회전시키는 기술입니다.&lt;br /&gt;
회전한 뒤에 데이터를 설명하는 데 중요한 새로운 특성 중 일부만 선택합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136796988-27ef69fd-7e90-4787-8fda-40e0cda1779b.png&quot; alt=&quot;pca&quot; /&gt;&lt;/p&gt;

&lt;p&gt;주성분 찾기&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;분산이 가장 큰 가장 많은 정보를 가지고 있는 방향을 찾는다.&lt;/li&gt;
  &lt;li&gt;첫 번째 방향과 직각인 방향 중에서 가장 많은 정보를 가지고 있는 방향을 찾는다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;두 번째 그래프는 주성분 1과 2를 x 축과 y 축에 나란히 회전한 것으로 변환된 데이터의 상관관계 행렬이 대각선 방향을 제외하고 0이 됩니다.&lt;/p&gt;

&lt;p&gt;세 번째 그래프는 차원 축소 용도로 사용될 수 있습니다.  첫 번째 주성분만 유지하므로 2차원 데이터 셋이 1차원 데이터 셋으로 차원 감소 합니다.&lt;br /&gt;
단순히 원본 특성 중 하나만 남기는 것이 아닌 가장 유용한 방향을 찾아 그 성분을 유지하는 것입니다.&lt;/p&gt;

&lt;p&gt;마지막 그래프는 데이터에 다시 평균을 더하고 반대로 회전시킨 그래프 입니다.&lt;br /&gt;
원래 특성 공간에 있지만 첫 번째 주성분의 정보만 가지고 있습니다.&lt;br /&gt;
보통 노이즈 제거나 주성분에서 유지되는 정보의 시각화를 위해 사용됩니다.&lt;/p&gt;

&lt;h6 id=&quot;유방암-데이터-셋-시각화&quot;&gt;유방암 데이터 셋 시각화&lt;/h6&gt;

&lt;p&gt;유방암 데이터는 특성이 30개를 가지고 있기 때문에 너무 많은 산점도를 요구합니다.&lt;br /&gt;
따라서 히스토그램을 그리는 방법이 있지만 히스토그램은 특성 간의 상호작용이나 클래스와의 곤계를 알려주지 못합니다.&lt;br /&gt;
따라서 PCA를 통해 데이터를 회전시키고 차원을 축소합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136798175-2b4dfcfe-1462-4712-9df0-513074f7ad3a.png&quot; alt=&quot;pca2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러면 다음과 같이 2차원 공간에서도 잘 구분됩니다.&lt;br /&gt;
분류를 할 때 좋은 결과를 얻을 수 있을 것 같은 그래프입니다.&lt;br /&gt;
하지만 두 축을 해석하기 어렵다는 단점을 가지고 있습니다.&lt;br /&gt;
따라서 pca.components_를 통해서 중요도를 확인할 수 있습니다.&lt;/p&gt;

&lt;h6 id=&quot;고유얼굴-특성-추출&quot;&gt;고유얼굴 특성 추출&lt;/h6&gt;

&lt;p&gt;PCA는 특성 추출에도 이용됩니다.&lt;br /&gt;
RGB 강도로 기록된 픽셀들을 분류하는데 유용합니다.&lt;br /&gt;
픽셀을 사용해서 두 이미지를 비교할 때, 각 픽셀의 회색톤 값을 다른 이미지에서 동일한 위치에 있는 픽셀 값과 비교합니다.&lt;/p&gt;

&lt;p&gt;하지만 이는 사람이 얼굴을 인식하는 것과 많이 다르고 특징을 잡기 어렵습니다.&lt;br /&gt;
따라서 주성분으로 변환하여 거리를 계산하면 정확도가 높아집니다.&lt;br /&gt;
PCA의 화이트닝 옵션을 사용하면 주성분의 스케일이 같습니다.&lt;br /&gt;
화이트닝 옵션은 StandardScaler과 동일한 방식입니다.&lt;/p&gt;

&lt;p&gt;PCA 모델은 픽셀을 기반으로 하므로 얼굴의 배치와 조명이 비슷한 이미지를 판단하는 데 큰 영향을 줍니다.&lt;br /&gt;
따라서 테스트 포인트를 주성분의 가중치 합으로 나타내는 것에 PCA 변환을 사용하는 방법도 해석 중 한 가지 방법입니다.&lt;/p&gt;

&lt;p&gt;원본 데이터를 재구성하는 방법도 PCA 모델의 해석 방법 중 한가지입니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>주건나</name>
        
        
      </author>

      

      
        <category term="study" />
      

      
        <summary type="html">머신러닝 공부 관련 글 머신러닝 정리 (1)-지도학습 (1) 머신러닝 정리 (2)-지도학습 (2) 머신러닝 정리 (3)-비지도학습 (1) 머신러닝 정리 (3) - 비지도학습 (1) 본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다. 또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다. 글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다. 코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다. 소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">머신러닝 정리 (2) 지도학습 (2)</title>
      <link href="https://keonju2.github.io/study-ML2" rel="alternate" type="text/html" title="머신러닝 정리 (2) &lt;br&gt; 지도학습 (2)" />
      <published>2021-10-04T10:00:00+09:00</published>
      <updated>2021-10-04T10:00:00+09:00</updated>
      <id>https://keonju2.github.io/study-ML2</id>
      <content type="html" xml:base="https://keonju2.github.io/study-ML2">&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;머신러닝 공부 관련 글&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML1&quot;&gt;머신러닝 정리 (1)-지도학습 (1)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML2&quot;&gt;머신러닝 정리 (2)-지도학습 (2)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML3&quot;&gt;머신러닝 정리 (3)-비지도학습 (1)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;머신러닝-정리-2---지도학습-2&quot;&gt;머신러닝 정리 (2) - 지도학습 (2)&lt;/h1&gt;
&lt;p&gt;본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.&lt;br /&gt;
또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.&lt;br /&gt;
글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.&lt;br /&gt;
코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;소스 코드: &lt;a href=&quot;https://github.com/rickiepark/introduction_to_ml_with_python&quot;&gt;https://github.com/rickiepark/introduction_to_ml_with_python&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;지도학습-2&quot;&gt;지도학습 (2)&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;결정 트리&lt;/li&gt;
  &lt;li&gt;결정 트리의 앙상블&lt;/li&gt;
  &lt;li&gt;배깅, 엑스트라 트리, 에이다부스트
    &lt;ol&gt;
      &lt;li&gt;배깅&lt;/li&gt;
      &lt;li&gt;엑스트라 트리&lt;/li&gt;
      &lt;li&gt;선형 모델&lt;/li&gt;
      &lt;li&gt;에이다부스트&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;커널 서포트 벡터 머신&lt;/li&gt;
  &lt;li&gt;신경망 (딥러닝)&lt;/li&gt;
  &lt;li&gt;분류 예측의 불확실성 추정
    &lt;ol&gt;
      &lt;li&gt;결정 함수&lt;/li&gt;
      &lt;li&gt;예측 확률&lt;/li&gt;
      &lt;li&gt;다중 분류에서의 불확실성&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;결정-트리&quot;&gt;결정 트리&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;color:orange&quot;&gt;Decision Tree &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;결정 트리는 분류와 회귀 문제에서 사용되는 모델입니다.&lt;br /&gt;
스무고개처럼 예/아니오로 나눌 수 있는 조건을 통해서 결정에 다다르게 됩니다.&lt;br /&gt;
질문과 정답은 노드가 되고 특히 마지막 노드는 리프라고 합니다. 
&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136059620-b1adf458-14a4-474b-ae07-1fb89a48f29e.png&quot; alt=&quot;tree1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결정트리의 구조는 왼쪽 하단에 사진처럼 가장 위에는 Root node, 질문과 답을 연결하는 Edge, 내부의 Internal node, 마지막 노드는 Leaf node, 그리고 Depth로 구성됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;결정-트리-만들기-&quot;&gt;&lt;span style=&quot;color:green&quot;&gt;결정 트리 만들기 &lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;&lt;br /&gt;
결정 트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예/아니오 질문 (TEST) 목록을 학습한다는 뜻입니다.&lt;br /&gt;
보통 데이터들은 예/아니오 특성으로 구분되지 않고  연속적인 특성을 가진 2차원 데이터 셋에서 보통 ‘특성 i는 값 a보다 큰가?’의 형태와 같은 테스트를 가집니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136060832-f157681f-43fa-4b0d-a159-b7554269124b.png&quot; alt=&quot;tree2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 데이터들을 X[1]&amp;lt;=0.6인 테스트로 나누어 봅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136061884-6c694665-0037-4ba9-9e82-25dd6d9d5c6a.png&quot; alt=&quot;tree3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;알고리즘은 가능한 모든 테스트에서 타깃 값에 대해 가장 많은 정보를 가진 것을 고르게 됩니다.&lt;/p&gt;

&lt;p&gt;따라서 X[1]&amp;lt;=0.6인 테스트를 선택하게 됩니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136062820-b9733fdf-be5b-4aaf-a6c4-02114c7cb748.png&quot; alt=&quot;tree5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결정 트리에서 각 테스트는 하나의 축을 따라 데이터를 나눕니다.&lt;br /&gt;
하나의 질문당 하나의 축을 만들어서 영역이 한 개의 타깃값을 가질 때까지 반복됩니다. &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136062365-e168f4b1-9828-487a-bcd1-de32998eafb2.png&quot; alt=&quot;tree4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결정 트리의 멈춤 조건입니다.&lt;br /&gt;
즉, 미리 정의한 조건들이 없다면 가지를 만들 수 있을 때까지 만드는 것을 알 수 있습니다.&lt;br /&gt;
결정트리의 예측은 그 포인트가 어느 리프에 들어갈지 확인하는 것인데 분류는 타깃 값 중 다수인 것이 예측 결과가 되고 회귀의 경우 리프 노드의 훈련 데이터 평균값이 결과로 출력됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;결정-트리-복잡도-제어하기-&quot;&gt;&lt;span style=&quot;color:skyblue&quot;&gt;결정 트리 복잡도 제어하기 &lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;&lt;br /&gt;
결정 경계가 클래스 포인트에 멀리 떨어진 이상치에 민감하게 되어 모든 리프 노드가 순수 노드가 될 때까지 진행하면 모델이 복잡해지고 과대적합이 발생합니다.&lt;br /&gt;
과대적합을 막기 위한 방법은 크게 사전가지치기, 사후 가지치기 두 가지입니다.&lt;br /&gt;
사전 가지치기는 이름에서 알 수 있듯이 모델을 만들 때 깊이나 리프의 개수 또는 테스트의 최소 개수를 미리 제한하는 것입니다.&lt;br /&gt;
미리 제한하기 때문에 정말로 중요한 포인트를 분류하지않을 수 있습니다.&lt;br /&gt;
사후 가지치기 역시 이름에서 알 수 있듯이 트리가 만들어진 뒤 포인트가 적은 노드를 삭제 혹은 병합하게 되는데 에러감소 프루닝, 룰 포스트 프루닝 같은 방법들이 있습니다.&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;evidence&quot;&gt;참고&lt;/span&gt;&lt;br /&gt;
에러감소 프루닝&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;모든 노드를 프루닝 대상으로 고려&lt;br /&gt;
노드 제거 후 검증을 통해 제거 전, 후 정확도 비교&lt;br /&gt;
제거 전보다 정확도가 낮아지기 전까지 반복&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;룰 포스트 프루닝&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;의사결정 트리를 룰셋으로 변환 (룰은 루트부터 리프까지의 경로)&lt;br /&gt;
이 룰셋 속성들에 정확도를 떨어뜨리는 속성을 제거&lt;br /&gt;
프루닝 완료 후 정확도 순으로 정렬해 이 순서대로 적용&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;결정 트리는 다음과 같이 만들 수 있고 정확도를 확인할 수 있습니다.&lt;br /&gt;
Default값은 모든 리프가 순수 노드가 되는 모델을 만들기 때문에 훈련 세트의 정확도가 100%가 됩니다.&lt;br /&gt;
하지만 트리가 무한정 깊어지고 복잡해지고 일반화가 잘 되지 않습니다.
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.tree&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DecisionTreeClassifier&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_breast_cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stratify&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;훈련 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;테스트 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;훈련&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;테스트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.937&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
과대적합 때문에 반드시 훈련 세트의 정확도가 테스트 정확도와 비례하지 않아서 max_depth와 같은 파라미터를 통해 과대적합을 줄이고 테스트 세트 정확도를 높일 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;훈련 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;테스트 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;훈련&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.988&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;테스트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.951&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;결정-트리-분석-&quot;&gt;&lt;span style=&quot;color:purple&quot;&gt;결정 트리 분석 &lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;결정 트리를 생성하고 시각화하기 위해서는 다음과 같은 모듈이 필요합니다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 트리 모델 생성
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.tree&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DecisionTreeClassifier&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# 트리의 시각화_1
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.tree&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graphviz&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# 트리의 시각화_2 (.dot 파일을 만들지 않아도 가능)
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.tree&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_tree&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
그래프를 시각화하는 코드는 다음과 같이 쓸 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# graphviz 이용
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.tree&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;export_graphviz&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;export_graphviz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tree.dot&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;악성&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;양성&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;impurity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#plot_tree
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.tree&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_tree&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plot_tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;악성&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;양성&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
         &lt;span class=&quot;n&quot;&gt;impurity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rounded&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fontsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;filled=True를 넣어주면 다음과 같이 색상이 들어가는 트리 모델을 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136067667-33e3ca00-4752-4125-9d5e-ff12703b4d94.png&quot; alt=&quot;tree8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;트리의-특성-중요도&quot;&gt;&lt;span style=&quot;color:yellow&quot;&gt;트리의 특성 중요도&lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;tree.feature_importane를 통해 특성 중요도를 알 수 있습니다.&lt;br /&gt;
특성 중요도는 0부터 1 사이에 존재하는데 0은 전혀 사용되지 않은 특성, 1은 완벽하게 타깃 클래스를 예측한 특성을 의미합니다. &lt;br /&gt;
특성 중요도가 낮다는 유용하지 않다가 아닌 모델이 만들어질 때 특성을 선택하지 않았거나 특성과 중복되는 정보가 있다는 것을 의미합니다.&lt;br /&gt;
전체 합은 1이 되고 따라서 특성중요도는 ‘이 모델이 만들어지는데 어떤 특성의 비율이 높은가?’ 정도의 해석이라고 생각하면 될 것 같습니다.&lt;br /&gt;
Worst_radius만 보고 ‘반지름이 크면 양성이다?’ 를 알 수 없는 것처럼 특성 중요도는 어떤 클래스를 지지하는지 알려주지 않습니다.&lt;br /&gt;
&lt;br /&gt;
결정 트리의 회귀도 분류와 비슷하게 적용됩니다.
단, 결정 트리를 회귀 모델로 사용하게 되면 훈련 데이터 범위 밖의 정보가 없어서 그 부분에 대한 예측이 불가능하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136069236-7fb89212-fcc1-4919-b9ae-430f2a18c359.png&quot; alt=&quot;tree9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음 모델은 트리 복잡도에 제한을 두지않아서 훈련 데이터는 완벽하게 예측하지만 데이터 범위 밖으로 나가면 마지막 포인트로 예측값을 출력합니다.
따라서 트리 모델은 가격의 등락과 같은 예측을 할 때는 좋은 예측 모델을 만들 수 있지만 시계열 데이터에서는 데이터가 가진 시간 범위 밖의 예측은 안되기 때문에 잘 맞지 않습니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;장단점과-매개변수&quot;&gt;&lt;span style=&quot;color:pink&quot;&gt;장단점과 매개변수&lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;장점&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;해석력이 높습니다.&lt;br /&gt;
데이터의 스케일에 구애받지 않습니다. &lt;br /&gt;
정규화나 표준화 같은 전처리 불필요합니다.
특성의 스케일이 다르거나 이진특성, 연속적인 특성이 혼합되어도 잘 작동합니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;단점&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;과대적합되는 경향이 있어 일반화 성능이 좋지 않습니다.  &lt;br /&gt;
축 평행을 구분하여 일부 관계에서 모델링이 어려움이 있습니다. &lt;br /&gt;
훈련 데이터에 대한 약간의 변경은 전체 결정논리에 큰 변화를 야기하여 샘플에 민감합니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;매개변수&lt;/th&gt;
      &lt;th&gt;설명&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;min_samples_split&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용 &lt;br /&gt; - Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;min_samples_leaf&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수&lt;br /&gt;- min_samples_split과 함께 과적합 제어 용도&lt;br /&gt;- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;max_features&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 최적의 분할을 위해 고려할 최대 feature 개수&lt;br /&gt;- Default = None → 데이터 세트의 모든 피처를 사용&lt;br /&gt;- int형으로 지정 →피처 갯수 / float형으로 지정 →비중&lt;br /&gt;- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정&lt;br /&gt;- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;max_depth&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 트리의 최대 깊이&lt;br /&gt;- default = None&lt;br /&gt;→ 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할&lt;br /&gt;- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;max_leaf_nodes&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;리프노드의 최대 개수&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;여기서 max_depth, max_leaf_nodes,min_samples_leaf 중 하나만 지정해도 과대적합을 막는데 충분한 역할을 합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;결정-트리의-앙상블&quot;&gt;결정 트리의 앙상블&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;color:orange&quot;&gt;Ensemble&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;앙상블은 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법입니다.&lt;br /&gt;
책에서는 결정 트리의 앙상블로 한정하고 가장 많이 쓰이는 랜덤포레스트나 부스팅 모델은 트리 기반 모델이지만 앙상블은 다른 분류 모델을 결합하여 사용할 수도 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Voting – 서로 다른 알고리즘을 가진 분류기를 결합&lt;/li&gt;
  &lt;li&gt;Bagging – 각각의 분류기는 모두 같은 유형의 알고리즘 기반, 모델을 다양하게 만들기 위해 데이터를 재구성 (랜덤포레스트)&lt;/li&gt;
  &lt;li&gt;Boosting – 맞추기 어려운 데이터에 대해 좀 더 가중치를 두어 학습 (Adaboost, Gradient Boosting)&lt;/li&gt;
  &lt;li&gt;Stacking – 모델의 output 값을 새로운 독립변수로 사용&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136071927-2b58ace4-f191-497f-b214-4077ff7aad64.png&quot; alt=&quot;ensemble1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앙상블의 조건입니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;랜덤-포레스트&quot;&gt;&lt;span style=&quot;color:lightgreen&quot;&gt;랜덤 포레스트&lt;/span&gt;&lt;/h6&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;랜덤 포레스트는 조금씩 다른 결정 트리의 묶음입니다.      &lt;br /&gt;
데이터의 일부에 과대적합되는 경향을 이용하여 서로 다른 방향으로 과대적합된 트리를 많이 만들어 그 결과를 평균냄으로써 예측 성능은 유지되면서 결과적으론 과대적합이 줄어드는 아이디어에 기초합니다.&lt;br /&gt;
결정 트리를 많이 만들면서 각 트리는 타깃 예측을 잘 해야 하고 다른 트리와 구별되어야 합니다.&lt;br /&gt;
따라서 무작위성을 주입하는데 트리를 만들 때 사용하는 데이터 포인트를 무작위로 선택하거나 분할 테스트에서 특성을 무작위로 선택하는 방법을 이용합니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;랜덤-포레스트-구축&quot;&gt;&lt;span style=&quot;color:olive&quot;&gt;랜덤 포레스트 구축&lt;/span&gt;&lt;/h6&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;from sklearn.ensemble import RandomForestClassifier (or RandomForestRegressor)&lt;br /&gt;
n_estimators로 생성할 트리의 개수를 정합니다.&lt;br /&gt;
부트스트랩 샘플은 n_samples개의 데이터 포인트 중에서 n_samples 횟수만큼 무작위로 중복 가능하게 반복 추출하는 것을 의미합니다.&lt;br /&gt;
따라서 데이터 셋이 원래 크기와 같지만 누락되거나 중복되는 데이터가 만들어집니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136072938-20fe1a7b-c4e1-4a72-b7ac-08c4dbc16744.png&quot; alt=&quot;ensemble2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;각 노드에서 전체 특성을 대상으로 최선의 테스트를 찾는 것이 아닌 알고리즘이 각 노드에서 후보 특성을 무작위로 선택한 후 이 후보들 중에서 최선의 테스트를 찾습니다. (max_features) &lt;br /&gt;
부트스트랩 샘플링을 통해 트리가 조금씩 다른 데이터셋을 이용해 만들어지도록 합니다.&lt;br /&gt;
각 노드에서 특성의 일부만 사용하기 때문에 트리의 각 분기는 각기 다른 특성 부분 집합을 사용됩니다.&lt;/p&gt;

&lt;p&gt;max_features=n_features는 특성 선택에 무작위성이 들어가지 않습니다. (부트스트랩 샘플링에는 무작위성 그대로 입니다.)&lt;br /&gt;
max_feature=1 트리의 분기는 테스트할 특성을 고를 필요가 없게 되고 무작위로 선택한 특성의 임계값 찾기만 하면 됩니다.&lt;br /&gt;
max_feature이 커지면 랜덤 포레스트 트리들은 매우 비슷하고 가장 두드러진 특성으로 데이터에 잘 맞춰질 것이고 작으면 트리들은 서로 많이 달라지고 각 트리는 데이터에 맞추기 위해 깊이가 깊어지게 됩니다.&lt;/p&gt;

&lt;p&gt;랜덤 포레스트 예측의 경우 알고리즘이 모델에 있는 모든 트리의 예측을 만듭니다.&lt;br /&gt;
회귀의 경우 이 예측들을 평균하여 최종 예측을 만듭니다.&lt;br /&gt;
분류의 경우 약한 투표 전략을 사용합니다.&lt;br /&gt;
약한 투표 전략은 각 알고리즘이 가능성 있는 출력 레이블의 확률을 제공하고 예측한 확률을 평균으로 가장 높은 확률을 가진 클래스가 예측값이 됩니다.&lt;br /&gt;
참고로 강한 투표 전략은 다수의 분류기가 결정한 예측값을 최대로 하는 것을 말합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136073413-36133d42-fdab-4036-a310-3ed03cac37be.png&quot; alt=&quot;ensemble3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;랜덤-포레스트-분석&quot;&gt;&lt;span style=&quot;color:darkblue&quot;&gt;랜덤 포레스트 분석&lt;/span&gt;&lt;/h6&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;랜덤 포레스트 훈련 모델&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_moons&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_moons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;noise&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stratify&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;forest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;forest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136073874-0e16312c-47e8-4567-b4a4-545001f1d605.png&quot; alt=&quot;ensemble4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;부트스트랩 샘플링 때문에 한쪽 트리에 나타나는 훈련 포인트가 다른 트리에는 포함되지 않을 수 있어 각 트리는 불완전하지만 랜덤포레스트의 결과는 좋은 결정경계를 보여줍니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136074021-efc81063-ca11-437d-83f1-0c8b171a01e6.png&quot; alt=&quot;ensemble5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단일 트리와 다르게 랜덤 포레스트에서 가장 특성 중요도가 높은 특성은 worst perimeter입니다.&lt;br /&gt;
랜덤 포레스트에서 더 많은 특성이 0 이상의 중요도를 갖고 따라서 더 넓은 시각으로 데이터를 바라볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;forest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;forest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;훈련 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;테스트 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;훈련&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;테스트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.972&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;랜덤 포레스트에선 훈련 데이터 정확도가 100% 이지만 단일 트리에 비해서 테스트 정확도가 상승한 것을 확인 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;장단점과-매개변수-1&quot;&gt;&lt;span style=&quot;color:chocolate&quot;&gt;장단점과 매개변수&lt;/span&gt;&lt;/h6&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;장점&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;매개변수 튜닝을 많이 하지 않습니다. &lt;br /&gt;
데이터의 스케일에 구애받지 않습니다. &lt;br /&gt;
단일 트리의 단점을 보완하고 장점을 그대로 가지고 있습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;단점&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;랜덤 포레스트의 트리는 특성의 일부만 사용하므로 결정 트리보다 더 깊어지는 경향이 있습니다.&lt;br /&gt;
다른 random_state를 지정하면 전혀 다른 모델이 만들어집니다.&lt;br /&gt;
텍스트 데이터와 같은 차원이 높고 희소한 데이터에 잘 작동하지 않습니다.&lt;br /&gt;
선형 모델에 비해 많은 메모리를 사용하며 훈련과 예측이 느림&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;매개변수&lt;/th&gt;
      &lt;th&gt;설명&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;n_estimators&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 결정트리의 갯수를 지정&lt;br /&gt;- Default = 10 (0.22버전부터 100)&lt;br /&gt;- 무작정 트리 갯수를 늘리면 성능 좋아지는 것 대비 시간이 걸릴 수 있음&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;min_samples_split&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용&lt;br /&gt;- Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;min_samples_leaf&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수&lt;br /&gt;- min_samples_split과 함께 과적합 제어 용도&lt;br /&gt;- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;max_features&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 최적의 분할을 위해 고려할 최대 feature 개수&lt;br /&gt;- Default = ‘auto’ (결정트리에서는 default가 none이었음)&lt;br /&gt;- int형으로 지정 →피처 갯수 / float형으로 지정 →비중&lt;br /&gt;- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정 (RandomForestClassifier-sqrt(n_feature), RandomForestRegressor-n_feature)&lt;br /&gt;- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;max_depth&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 트리의 최대 깊이&lt;br /&gt;- default = None&lt;br /&gt;→ 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할&lt;br /&gt;- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;max_leaf_nodes&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;리프노드의 최대 개수&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;N_estimatiors는 클수록 좋고 max_features와 max_depth와 같은 사전 가지치기 옵션은 단일 트리와 같이 주어집니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;그레이디언트-부스팅-회귀-트리&quot;&gt;&lt;span style=&quot;color:fuchsia&quot;&gt;그레이디언트 부스팅 회귀 트리&lt;/span&gt;&lt;/h6&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136076700-214be147-9549-47c5-9681-f20564458f85.png&quot; alt=&quot;gradient&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이름은 회귀이지만 회귀와 분류 모두 사용됩니다. (GradientBoostingClassifier, GradientBoostingRegressor)&lt;br /&gt;
그레이디언트 부스팅은 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듭니다.&lt;br /&gt;
따라서 기본적으로 무작위성이 없습니다.&lt;br /&gt;
대신 강력한 사전 가지치기가 사용되고 깊지 않은 트리를 사용합니다.&lt;br /&gt;
각 트리는 데이터의 일부에 대해서만 예측을 잘 수행하여 트리가 많이 추가될수록 성능이 향상됩니다.&lt;br /&gt;
이때 손실 함수를 정의하고 경사 하강법을 사용해서 다음 값을 보정합니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136076846-03cd6618-ee17-4afd-869f-09ed4543156f.png&quot; alt=&quot;gradient1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;random_state=0 만 입력했을 때&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GradientBoostingClassifier&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;​&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;​&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;​&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;훈련 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;테스트 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;훈련&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;테스트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.965&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;random_state=0, max_depth=1 을 입력했을 때&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;​&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;훈련 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;테스트 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;훈련&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.991&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;테스트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.972&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;random_state=0, learning_rate=0.01을 입력했을 때&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;훈련 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;테스트 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;훈련&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.988&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;테스트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.965&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;훈련 세트의 정확도가 100%로 과대적합이 된 모델은 max_depth나 learning_rate로 보완할 수 있습니다.&lt;br /&gt;
Random_state는 고정시켜야 같은 모델이 나오는 것을 볼 수 있습니다.&lt;br /&gt;
Learning_rate는 오차에 곱을 해서 예측값을 업데이트 해주는 값입니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136077751-6aba3074-c68d-4789-a9af-3d657f671ab2.png&quot; alt=&quot;gradient2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136077761-e5e43c6c-e62a-4c10-a7e6-96e584cec03e.png&quot; alt=&quot;gradient3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;랜덤 포레스트에 비해 그레이디언트 부스팅은 특성들이 더 적습니다.&lt;br /&gt;
안정성에서는 랜덤 포레스트가 더 좋지만 그레이디언트가 성능적으로 더 좋은 모습을 보여줄수 있습니다.&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;evidence&quot;&gt;참고&lt;/span&gt;&lt;br /&gt;
XGBoost&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;XGBoost는 데이터 별 오류를 다음 round 학습에 반영 시킨다는 측면에서 기존 Gradient Boosting과 큰 차이는 없음&lt;br /&gt;
Gradient Boosting과 달리 학습을 위한 목적식(loss function)에 Regularization term이 축가되어 모델이 과적합 되는 것을 방지해줌&lt;br /&gt;
Regularization term을 통해 XGBoost는 복잡한 모델에 패널티를 부여함&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136078326-964b2d5e-2e10-47a7-9300-e2cd383c810c.png&quot; alt=&quot;gradient4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;LighGBM&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;XGBoost와 다르게 lear-wise loss 사용 (loss를 더 줄일 수 있음)&lt;br /&gt;
XGBoost 대비 2배 이상 빠른 속도 (동일 파라미터 기준)&lt;br /&gt;
과대적합에 민감하여, 대량의 학습데이터를 필요로 함&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136078359-4d3bf782-bcbe-4e5d-b101-b5ccae3808f4.png&quot; alt=&quot;gradient5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h5 id=&quot;장단점과-매개변수-2&quot;&gt;&lt;span style=&quot;color:teal&quot;&gt;장단점과 매개변수&lt;/span&gt;&lt;/h5&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;장점&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;이진 특성이나 연속적인 특성에도 잘 작동합니다. &lt;br /&gt;
데이터의 스케일에 구애받지 않습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;단점&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;매개변수의 조정이 필수입니다.&lt;br /&gt;
휸련시간이 깁니다.  &lt;br /&gt;
차원이 높고 희소한 데이터에 잘 작동하지 않습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;N_estimators가 클수록 랜덤 포레스트는 좋았지만 그래이디언트 부스팅에서는 과대적합될 가능성이 높아집니다.&lt;br /&gt;
N_estimator을 정하고 난 뒤에 learning_rate를 정하게 되는데 learning_rate를 낮추면 비슷한 복잡도의 모델을 만들기 위해 더 많은 트리를 추가해야합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;매개변수&lt;/th&gt;
      &lt;th&gt;설명&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;n_estimators&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 트리의 개수를 지정&lt;br /&gt;- 커지면 모델이 복잡해지고 과대적합 가능성 높아짐&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;learning_rate&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 관례상 n_estimators를 맞추고 learning_rate를 찾음&lt;br /&gt; - 이전 트리의 오차를 보정하는 정도&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;n_iter_no_change / validation_fraction&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;-조기 종료를 위한 매개변수 (default값: n_iter_no_change =None (조기 종료 x), validation_fraction=0.1) &lt;br /&gt;- validation_fraction 비율만큼 검증 데이터로 사용하여 n_iter_no_change 만큼 반복하여 향상되지 않으면 훈련 종료&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;max_depth / max_leaf_nodes&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;-각 트리의 복잡도를 낮춤 &lt;br /&gt;- max_depth는 보통 매우 작게 설정하며 트리의 깊이가 5보다 깊어지지 않게 함&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;배깅-엑스트라-트리-에이다부스트&quot;&gt;배깅, 엑스트라 트리, 에이다부스트&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;color:maroon&quot;&gt;Bagging&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;from sklearn.ensemble import BaggingClassifier&lt;br /&gt;
배깅은 중복을 허용한 랜덤샘플링으로 만든 훈련 세트를 사용해 분류기를 각기 다르게 학습합니다.&lt;br /&gt;
랜덤포레스트는 배깅의 일종이지만 설명변수도 무작위로 선택하는 것이 차이가 있습니다.&lt;br /&gt;
predict_proba() 지원하면 메서드를 통해 확률값을 평균하여 예측을 수행합니다. (지원하지 않는다면 가장 빈도가 높은 클래스 레이블)&lt;br /&gt;
oob_score=True로 지정하면 매개변수는 부트스트래핑에 포함되지 않은 샘플로 훈련된 모델을 평가할 수 있습니다. (OOB 오차, default=False)&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaggingClassifier&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bagging&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaggingClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oob_score&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bagging&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;훈련 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bagging&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;테스트 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bagging&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xc_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yc_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OOB 샘플의 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bagging&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;oob_score_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;훈련&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.953&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;테스트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.951&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;OOB&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;샘플의&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.946&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;배깅은 랜덤포레스트와 달리 max_samples에 부트스트랩 샘플의 크기를 정할 수 있습니다. &lt;br /&gt;
또한 로지스틱 회귀가 들어갈 수도 있고 결정 트리가 들어갈 수도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:lime&quot;&gt;Extra Tree&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;후보 특성을 무작위로 분할한 다음 최적의 분할을 찾습니다.&lt;br /&gt;
엑스트라 트리도 랜덤 포레스트와 비슷하지만 splitter=‘random’을 사용합니다. 랜덤 포레스트는 splitter=‘best’가 고정입니다.&lt;br /&gt;
Splitter=‘best’의 의미는 모든 변수의 정보 이득을 계산하고 그중 가장 설명력이 높은 변수를 선택하는 것입니다.&lt;br /&gt;
또한 부트스트랩 샘플링을 적용하지 않습니다. &lt;br /&gt;
무작위성을 증가시키면 모델 편향은 늘어나지만 분산이 감소하는 모습을 보입니다.&lt;br /&gt;
개별 트리는 매우 복잡하지만 결정 경계는 안정적입니다.&lt;br /&gt;
계산 비용은 위 splitter에서의 feature의 차이 때문에 랜덤 포레스트보다 적지만  일반화 성능을 높이려면 많은 트리를 만들어야합니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ExtraTreesClassifier&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xtree&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ExtraTreesClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xtree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;훈련 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xtree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;테스트 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xtree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xc_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yc_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;훈련&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;테스트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.972&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:gray&quot;&gt;Adaptive Boosting&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;이전의 모델이 잘못 분류한 샘플에 가중치를 높여서 다음 모델을 훈련합니다.&lt;br /&gt;
훈련된 각 모델은 성능에 따라 가중치 부여합니다.&lt;br /&gt;
예측을 만들 때는 모델이 예측한 레이블을 기준으로 모델의 가중치를 합산하여 가장 높은 값을 가진 레이블을 선택합니다.&lt;br /&gt;
AdaBoostClassifier은 기본값으로 DecisionTreeClassifier(max_depth=1)를 갖습니다.&lt;br /&gt;
AdaBoostRegressor은 기본값으로 DecisionTreeRegressor(max_depth=3)을 갖습니다. (base_estimator을 이용하여 다른 모델 지정 가능)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136081550-a82cd75f-c6fd-49d2-83e9-8fedd01c4d5e.png&quot; alt=&quot;ada1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;에이다 부스팅의 원리와 수식
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54880474/136081678-0d0819ea-b048-43f2-a786-c470918bd8d1.png&quot; alt=&quot;ada2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;예측정확도와 가중치의 곱의 합이 되어 높은 정확도를 만들게 됩니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdaBoostClassifier&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdaBoostClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;훈련 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yc_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;테스트 세트 정확도: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xc_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yc_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;훈련&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;테스트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;세트&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;정확도&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.986&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;커널-서포트-벡터-머신&quot;&gt;커널 서포트 벡터 머신&lt;/h3&gt;

&lt;p&gt;커널 서포트 벡터 머신은 보통 SVM이라고 한다.&lt;br /&gt;
입력 데이터에서 단순한 초평면으로 정의되지 않는 더 복잡한 모델을 만들 수 있도록 확장한 것이다.&lt;br /&gt;
분류와 회귀 모두 사용 가능하다. (SVC는 분류, SVR은 회귀)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.svm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearSVC&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#선형 모델
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LinearSVC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.svm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# kernel, C, gamma 파라미터 존재
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h6 id=&quot;선형-모델과-비선형-특성&quot;&gt;선형 모델과 비선형 특성&lt;/h6&gt;
&lt;p&gt;선형 모델은 직선으로만 데이터 포인트를 나눌 수 있어 밑에 같은 데이터는 잘 들어맞지 않는다.&lt;br /&gt;
SVM 모델은 3차원에서 2차원으로 투영해본다면 더이상 선형 모델이 아니다.&lt;/p&gt;

&lt;h6 id=&quot;커널-기법&quot;&gt;커널 기법&lt;/h6&gt;
&lt;p&gt;커널 기법은 실제로 데이터를 확장하지 않고 확장된 특성에 대한 데이터 포인트들의 거리를 계산한다.&lt;br /&gt;
$ (특성1)^2 * (특성2)^5 $하는 다항식 커널이 있고 가우시안 커널로 불리우는 RBF 커널이 있다.&lt;br /&gt;
가우시안 커널은 차원이 무한한 특성 공간에 매핑하는 것이다.&lt;br /&gt;
모든 차수의 모든 다항식을 고려하지만 특성의 중요도는 고차항이 될수록 줄어든다.&lt;/p&gt;

&lt;h6 id=&quot;svm-이해하기&quot;&gt;SVM 이해하기&lt;/h6&gt;
&lt;p&gt;두 클래스 사이에 경계한 데이터 포인트들을 서포트 벡터라고 한다.&lt;br /&gt;
새로운 데이터 포인트에 대해 예측하려면 각 서포트 벡터와의 거리를 측정한다.&lt;br /&gt;
서포트 벡터의 중요도는 훈련 과정에서 학습하는데 dual_coef_ 속성에 저장된다.&lt;/p&gt;
&lt;h1 id=&quot;가우시안-커널-공식-사진&quot;&gt;가우시안 커널 공식 사진&lt;/h1&gt;
&lt;p&gt;가우시안 커널에 의해 계산되며 $ X_1, X_2 $는 데이터 포인트이며 $ ||X_1 - X_2|| $는 유클리디안 거리이고 $Γ$ 은 가우시안 커널의 폭을 제어하는 매개변수이다.&lt;/p&gt;

&lt;h6 id=&quot;svm-매개변수-튜닝&quot;&gt;SVM 매개변수 튜닝&lt;/h6&gt;
&lt;p&gt;$Γ$는 가우시안 커널 폭의 역수에 해당하는데 하나의 훈련 샘플이 미치는 영향의 범위를 결정한다.(1~0 사이의 범위이다.)&lt;br /&gt;
작은 값은 넓은 영역을 뜻하고, 큰 값은 영향이 미치는 범위가 제한적이다.&lt;br /&gt;
즉 커널의 반경이 클수록 훈련 샘플의 영향 범위도 커진다.&lt;br /&gt;
작은 $Γ$ 값은 모델의 복잡도를 낮출 수 있다.&lt;br /&gt;
C 매개 변수는 규제 매개변수이다. dual_coef_값을 제한합니다.&lt;br /&gt;
작은 C는 매우 제약이 큰 모델을 만들고 각 데이터 포인트의 영향력이 작다.&lt;br /&gt;
C를 증가시키면 이 포인트들이 영향을 크게 줘서 결정 경계를 휘게 만든다.&lt;/p&gt;

&lt;h6 id=&quot;svm을-위한-데이터-전처리&quot;&gt;SVM을 위한 데이터 전처리&lt;/h6&gt;
&lt;p&gt;커널 SVM에서는 데이터셋의 특성 자릿수가 완전히 다르면 영향을 크게 미친다.&lt;br /&gt;
따라서 특성 값을 평균이 0이고 단위 분산이 되도록 하거나, 0과 1 사이로 맞추는 방법을 많이 사용한다.(StandardScaler와 MinMaxScalar)&lt;/p&gt;

&lt;h6 id=&quot;장단점과-매개변수-3&quot;&gt;장단점과 매개변수&lt;/h6&gt;
&lt;p&gt;SVM은 저차원과 고차원의 데이터에 모두 잘 작동하지만 샘플이 많으면 잘 맞지 않는다.&lt;br /&gt;
또한 전처리와 매개변수 설정에 신경을 많이 써야하는데 그래서 랜덤 포레스트나 그레이디언트 부스팅과 같은 전처리가 거의 필요 없는 트리 기반 모델이 선호된다.&lt;br /&gt;
SVM은 분석도 어려워서 예측이 어떻게 결정되었는지 설명하기가 난해하다.&lt;br /&gt;
하지만 모든 특성이 비슷한 단위이고 스케일이 비슷하다면 시도해볼 만하다.&lt;br /&gt;
중요한 매개변수는 C이고 어떤 커널을 사용할지와 각 커널에 따른 매개변수이다.&lt;br /&gt;
RBF는 $Γ$ 매개변수를 갖지만 다른 커널 종류도 많다.&lt;/p&gt;

&lt;h3 id=&quot;신경망-딥러닝&quot;&gt;신경망 (딥러닝)&lt;/h3&gt;
&lt;p&gt;다층 퍼셉트론은(MLP)는 간단하게 분류와 회귀에서 쓰일 수 있다.&lt;/p&gt;

&lt;h6 id=&quot;신경망-모델&quot;&gt;신경망 모델&lt;/h6&gt;
&lt;p&gt;MLP는 여러 단계를 거처 결정을 만들어내는 선형 모델의 일반화된 모습이다.&lt;/p&gt;
&lt;h1 id=&quot;선형-회귀-모델의-예측-공식-사진&quot;&gt;선형 회귀 모델의 예측 공식 사진&lt;/h1&gt;
&lt;p&gt;$\hat Y $는 x[0]에서 x[p]까지의 입력특성과 학습된 계수의 가중치의 합이다.&lt;/p&gt;

&lt;h1 id=&quot;퍼셉트론-사진&quot;&gt;퍼셉트론 사진&lt;/h1&gt;
&lt;p&gt;왼쪽 노드는 입력 특성을 나타내며 연결선은 학습된 계수를 표현하고 오른쪽 노드는 입력의 가중치 합, 즉 출력을 나타낸다.&lt;br /&gt;
MLP는 가중치 합을 만드는 과정이 여러 번 반복되며 먼저 중간 단계를 구성하는 은닉 유닛을 계산하고 이를 이용하여 최종 결과를 산출하기 위해 다시 가중치 합을 계산한다.&lt;/p&gt;
&lt;h1 id=&quot;다중-퍼셉트론-사진&quot;&gt;다중 퍼셉트론 사진&lt;/h1&gt;
&lt;p&gt;각 은닉 유닛의 가중치 합을 계산한 후 결과에 비선형 함수인 렐루나 하이퍼볼릭 탄젠트, 시그모이드 함수를 적용합니다.&lt;/p&gt;
&lt;h1 id=&quot;회귀-분석-사진&quot;&gt;회귀 분석 사진&lt;/h1&gt;
&lt;p&gt;w는 입력 x와 은닉층 h 사이의 가중치이고, v는 은닉층 h와 출력 $\hat Y$ 사이의 가중치입니다.&lt;br /&gt;
w와 v는 훈련 데이터에서 학습하고 x는 입력 특성이며 $ \hat Y $는 계산된 출력, h는 중간 계산값 입니다.&lt;/p&gt;

&lt;h6 id=&quot;신경망-튜닝&quot;&gt;신경망 튜닝&lt;/h6&gt;
&lt;p&gt;더 복잡도가 낮은 모델을 만들고 싶다면 hidden_layer_size를 통해 은닉 유닛의 개수를 줄인다.&lt;br /&gt;
은닉 유닛을 추가하거나, 은닉층을 추가하거나 활성화함수를 바꾸면 더 매끄러운 결정 경계를 얻을 수도 있다.&lt;br /&gt;
선형 분류와 리지 회귀 처럼 L2 페널티를 사용해서 가중치를 0에 가깝게 감소시킬 수도 있다.(default는 매우 낮다)&lt;br /&gt;
신경망에서는 학습을 시작하기 전에 가중치를 무작위로 설정하며 이 무작위한 초기화가 모델의 학습에 영향을 준다.&lt;br /&gt;
따라서 같은 매개변수를 사용하더라도 초깃값이 다르면 모델이 많이 달라질 수 있다.&lt;br /&gt;
신경망도 입력 특성이 평균은 0 분산이 1이 되도록 변형하는 것이 좋다.&lt;br /&gt;
은닉 유닛에서 작은 가중치를 가진 특성은 모델에 덜 중요하다고 추론할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.neural_network&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLPClassifier&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# MLP분류
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MLPClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_layer_sizes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_itter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;#solver에 최적화 알고리즘,activation에 활성화 함수 ,hidden_layer_size로 은닉 유닛의 개수 설정(default=100),max_itter은 반복 횟수,alpha는 L2 페널티
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h6 id=&quot;장단점과-매개변수-4&quot;&gt;장단점과 매개변수&lt;/h6&gt;
&lt;p&gt;머신러닝 알고리즘을 뛰어넘는 성능을 보일 수 있지만 학습이 오래걸리고 데이터 전처리를 주의해서 해야한다.&lt;br /&gt;
모든 특성이 같은 의미를 가지면 SVM, 다른 종류의 특성이라면 트리 기반 메딜이 더 잘 작동할 수 있다.&lt;/p&gt;

&lt;h6 id=&quot;신경망의-복잡도-추정&quot;&gt;신경망의 복잡도 추정&lt;/h6&gt;
&lt;p&gt;가장 중요한 매개변수는 은닉층의 개수와 각 은닉층의 유닛 수이다.&lt;br /&gt;
복잡도에 관해 연관된 측정치는 학습된 가중치 또는 계수의 수이다.&lt;br /&gt;
특성이 100개 은닉 유닛 100개인 이진 분류라면 입력층과 첫 번째 은닉층 사이에는 편향을 포함하여 $ 100 * 100 + 100 = 10100 $개의 가중치가 있습니다.&lt;br /&gt;
은닉층과 출력층 사이에 $ 100 * 1 + 1 = 101 $개의 가중치가 더 있어 가중치는 10201개 이다.&lt;br /&gt;
이렇게 가중치는 은닉층을 추가할수록 훨씬 커지게 된다.&lt;br /&gt;
매개변수를 조정하는 일반적인 방법은 충분히 과대적합되어 문제를 해결할만한 큰 모델을 만든 뒤 훈련 데이터가 충분히 학습될 수 있다고 생각되면 신경망 구조를 줄이거나 규제 강화를 위해 alpha 값을 증가시켜 일반화 성능을 향상시킨다.&lt;br /&gt;
층의 개수, 층당 유닛 개수, 규제, 비선형성으로 모델 구성을 할 수 있으며, solver 매개변수를 통해서 학습시키는 방법을 지정할 수 있다.&lt;br /&gt;
solver의 경우 기본값은 adam이고 데이터 스케일에 민감하다.&lt;br /&gt;
lbfgs는 안정적이지만 규모가 크면 시간이 오래 걸린다&lt;br /&gt;
sgd는 momentum과 nesterovs_momentom의 영향을 받는데 다른 여러 매개변수와 함께 튜닝하여 최선의 결과를 만들 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;분류-예측의-불확실성-추정&quot;&gt;분류 예측의 불확실성 추정&lt;/h3&gt;
&lt;p&gt;decision_function과 predict_proba로 추정 할 수 있다.&lt;/p&gt;

&lt;h6 id=&quot;결정-함수&quot;&gt;결정 함수&lt;/h6&gt;
&lt;p&gt;decision_function의 반환값의 크기는 (n_samples,)이며각 샘플이 하나의 실수 값을 반환한다.&lt;br /&gt;
모델이 데이터 포인트가 양성 클래스인 클래스 1에 속한다고 믿는 정도이다.&lt;br /&gt;
즉, 음수값은 다른 클래스에 속함을 의미한다.&lt;br /&gt;
값의 범위는 데이터와 모델 파라미터에 따라 달라지게 된다.&lt;/p&gt;

&lt;h6 id=&quot;예측-확률&quot;&gt;예측 확률&lt;/h6&gt;
&lt;p&gt;predict_proba의 출력은 각 클래스에 대한 확률이고 이진 분류에서 이 값의 크기는 항상 (n_samples,2)이다.&lt;br /&gt;
두 클래스의 확률 합은 1이므로 두 클래스 중 하나는 50% 이상의 확신을 가질 것이고 그 클래스가 예측값이 된다.&lt;br /&gt;
데이터에 있는 불확실성이 얼마나 이 값에 잘 반영되는지는 모델과 매개변수 설정에 달렸다.&lt;br /&gt;
그래서 과대적합된 모델 혹은 잘못된 예측도 예측의 확신이 강한 편이다.&lt;br /&gt;
복잡도가 낮을 수록 예측에 불확실성이 더 많다.&lt;br /&gt;
불확실성과 모델의 정확도가 동등하면 이 모델이 보정되었다고 한다.&lt;/p&gt;

&lt;h6 id=&quot;다중-분류에서의-불확실성&quot;&gt;다중 분류에서의 불확실성&lt;/h6&gt;
&lt;p&gt;다중 분류에서도 decision_funcion과 predict_proba를 사용할 수 있다.&lt;br /&gt;
decision_function에서는 (n_samples, n_classes)가 결과값이 된다.&lt;br /&gt;
글 클래스에 대한 확신 점수를 담고 그 수치가 크면 그 클래스일 가능성이 크다.&lt;br /&gt;
데이터 포인트마다 점수들에서 가장 큰 값을 찾아 예측 결과를 재현할 수 있다.&lt;br /&gt;
predict_proba는 (n_samples,n_classes)가 출력값이 된다.&lt;br /&gt;
마찬가지로 각 데이터 포인트에서 클래스 확률의 합은 1이다.&lt;br /&gt;
argmax 함수를 적용해서 예측 결과를 재현할 수 있지만 클래스가 문자열이거나 정수형을 사용하지만 연속적이지 않고 0부터 시작하지 않을 수 있다.&lt;br /&gt;
따라서 predict 결과와 decision_function, predict_proba의 결과를 비교하기 위해서는 분류기의 classes_ 속성을 사용해 클래스의 실제 이름을 얻어야 한다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>주건나</name>
        
        
      </author>

      

      
        <category term="study" />
      

      
        <summary type="html">머신러닝 공부 관련 글 머신러닝 정리 (1)-지도학습 (1) 머신러닝 정리 (2)-지도학습 (2) 머신러닝 정리 (3)-비지도학습 (1) 머신러닝 정리 (2) - 지도학습 (2) 본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다. 또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다. 글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다. 코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다. 소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">머신러닝 정리 (1) 지도학습 (1)</title>
      <link href="https://keonju2.github.io/study-ML1" rel="alternate" type="text/html" title="머신러닝 정리 (1) &lt;br&gt; 지도학습 (1)" />
      <published>2021-09-27T10:00:00+09:00</published>
      <updated>2021-09-27T10:00:00+09:00</updated>
      <id>https://keonju2.github.io/study-ML1</id>
      <content type="html" xml:base="https://keonju2.github.io/study-ML1">&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;머신러닝 공부 관련 글&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML1&quot;&gt;머신러닝 정리 (1)-지도학습 (1)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML2&quot;&gt;머신러닝 정리 (2)-지도학습 (2)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML3&quot;&gt;머신러닝 정리 (3)-비지도학습 (1)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;머신러닝-정리-1---지도학습-1&quot;&gt;머신러닝 정리 (1) - 지도학습 (1)&lt;/h1&gt;
&lt;p&gt;본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.&lt;br /&gt;
또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.&lt;br /&gt;
글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.&lt;br /&gt;
코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;소스 코드: &lt;a href=&quot;https://github.com/rickiepark/introduction_to_ml_with_python&quot;&gt;https://github.com/rickiepark/introduction_to_ml_with_python&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;지도학습-1&quot;&gt;지도학습 (1)&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;분류와 회귀&lt;/li&gt;
  &lt;li&gt;일반화, 과대적합, 과소적합
    &lt;ol&gt;
      &lt;li&gt;모델복잡도와 데이터셋 크기의 관계&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;지도 학습 알고리즘
    &lt;ol&gt;
      &lt;li&gt;예제에 사용할 데이터셋&lt;/li&gt;
      &lt;li&gt;k-nn 모델&lt;/li&gt;
      &lt;li&gt;선형 모델&lt;/li&gt;
      &lt;li&gt;Naive Bayes 분류기&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;분류와-회귀&quot;&gt;분류와 회귀&lt;/h3&gt;

&lt;p&gt;분류란? 미리 정의된 가능성 있는 여러 클래스 레이블 중 하나를 예측하는 것이다.&lt;br /&gt;
이진 분류: 예, 아니오로 구분할 수 있다. ex) 이 이메일은 스팸인가요?&lt;br /&gt;
다중 분류: 셋 이상의 클래스로 분류된다. ex) 붓꽃 데이터&lt;/p&gt;

&lt;p&gt;회귀란? 부동소수점수(수학으로 말하면 실수)를 예측하는 것이다.&lt;br /&gt;
어떤 사람의 여러 조건들을 통해 연간 소득을 예측하는 것과 같은 문제이다.&lt;/p&gt;

&lt;p&gt;출력 값에 연속성이 있다면 회귀 문제 없다면 분류 문제이다.&lt;/p&gt;

&lt;h3 id=&quot;일반화-과대적합-과소적합&quot;&gt;일반화, 과대적합, 과소적합&lt;/h3&gt;

&lt;p&gt;모델이 처음 보는 데이터에 대해 정확히 예측할 수 있다면 훈련 세트(train_set)에서 테스트 세트(test_set)으로 일반화되었다고 한다.&lt;br /&gt;
훈련 세트에 대해서 정확히 예측하고 테스트 세트에서도 정확히 예측하길 바란다.&lt;br /&gt;
하지만 모델이 복잡하다면 훈련 세트에서만 정확한 모델이 될 수 있다.&lt;br /&gt;
예를 들어 “내 주변 20대가 모두 아이폰을 쓰기 때문에 다른 20대도 모두 아이폰을 살 것이다.”라는 예측을 한다면 훈련 세트가 내 주변 20대가 될 것이고 테스트 세트가 다른 모든 20대가 될 것이다.&lt;br /&gt;
이처럼 알고리즘이 새로운 데이터를 잘 처리하는지 측정하는 방법은 테스트 세트로 평가를 해야한다.&lt;br /&gt;
이 때, 너무 복잡한 모델을 만들어 훈련 세트에 집중되어 테스트 세트에 일반화가 부족하다면 과대적합(overfitting)이라 한다.&lt;br /&gt;
반대로 너무 간단한 모델이라 훈련 세트에도 잘 맞지 않다면 과소적합(underfitting)이라 한다.&lt;/p&gt;

&lt;h5 id=&quot;모델복잡도와-데이터셋-크기의-관계&quot;&gt;모델복잡도와 데이터셋 크기의 관계&lt;/h5&gt;

&lt;p&gt;모델의 복잡도는 훈련 데이터 셋에 담긴 입력 데이터의 다양성과 관련이 있다.&lt;br /&gt;
데이터셋에 데이터 포인트가 다양하면 과대적합 없이 복잡한 모델을 만들 수 있다.&lt;br /&gt;
따라서 중복이거나 비슷한 데이터를 모으는 것은 도움이 되지않는다.&lt;br /&gt;
위의 예를 생각해보면 내 주변 20대라는 특징말고 대학 동기, 친구라는 데이터를 얻더라도 모두 20대라는 범주안에 들어갈테니 불필요한 데이터라 할 수 있다.&lt;br /&gt;
따라서 좋은 데이터를 많이 얻는 것이 좋다.&lt;/p&gt;

&lt;h3 id=&quot;지도-학습-알고리즘&quot;&gt;지도 학습 알고리즘&lt;/h3&gt;

&lt;p&gt;각 모델의 장단점과 어떤 데이터와 어울리는지, 매개변수와 옵션의 의미를 알아보도록 하자.&lt;br /&gt;
scikit-learn 문서를 참고하면 더 자세한 정보를 얻을 수 있다.&lt;/p&gt;

&lt;h5 id=&quot;예제에-사용할-데이터셋&quot;&gt;예제에 사용할 데이터셋&lt;/h5&gt;

&lt;p&gt;forge 데이터셋은 인위적으로 만든 이진 분류 데이터셋이다.&lt;/p&gt;

&lt;h5 id=&quot;k-nn-모델&quot;&gt;k-NN 모델&lt;/h5&gt;

&lt;p&gt;k-NN 모델은 단순히 데이터셋을 분류하는 것이다.&lt;br /&gt;
k개의 레이블 중에서 어느 쪽에 더 가까운 것인지 투표를 하여 결정한다고 생각하면 이해하기 편하다.&lt;br /&gt;
k=1일 때는 가장 가까운 것이 빨간색이었다면, k=3일 때는 파란색 두 개와 빨간색 한 개가 가까울 수도 있다.&lt;br /&gt;
그렇게 된다면 k=3일 때는 파란색으로 분류가 된다.&lt;/p&gt;

&lt;p&gt;KNeighborClassifier(n_neighbors)을 통해 분류 모델을 만들 수 있다.&lt;br /&gt;
이를 통해 확인할 수 있는 것은 k의 값이 커질수록 보다 단순한 모델이 만들어질 수 있다는 것이다.&lt;br /&gt;
하지만 반드시 k가 커진다고 좋은 모델은 아니다. 정확도가 낮아질 수 있기 때문이다.&lt;/p&gt;

&lt;p&gt;KNeighborsRegressor(n_neighbors)을 통해 회귀 모델 또한 만들 수 있다.&lt;br /&gt;
회귀 모델에서도 k를 너무 적게 쓴다면 모든 데이터를 지나가고 불안정한 모델이 만들어진다.&lt;/p&gt;

&lt;h6 id=&quot;장단점과-매개변수&quot;&gt;장단점과 매개변수&lt;/h6&gt;
&lt;p&gt;가장 중요한 매개변수는 거리를 재는 방법과 k값이다.&lt;br /&gt;
보통 거리를 재는 방법은 유클리디안 거리 방식을 사용한다.&lt;br /&gt;
장점은 이해하기가 쉬운 모델이고 조정을 많이 하지않아도 좋은 성능을 발휘할 수 있다는 것이다.&lt;br /&gt;
단점은 훈련 세트가 크면 예측이 느려지고 전처리 과정이 중요하다는 것이다.&lt;br /&gt;
데이터가 특성이 많거나 대부분이 0인 데이터셋에서는 잘 작동하지 않는다.&lt;/p&gt;

&lt;h5 id=&quot;선형-모델&quot;&gt;선형 모델&lt;/h5&gt;

&lt;p&gt;선형 모델은 $y=w[0]*x[0]+b$와 같은 모델을 갖는 예측 함수이다.&lt;br /&gt;
y는 예측값, w와 b는 모델이 학습할 파라미터, x는 데이터의 특성이다. 위 식은 특성이 하나인 데이터 셋의 선형 함수이다.&lt;/p&gt;

&lt;h6 id=&quot;선형-회귀최소제곱법&quot;&gt;선형 회귀(최소제곱법)&lt;/h6&gt;
&lt;p&gt;선형 회귀는 예측과 훈련 세트에 있는 타깃 사이의 평균제곱오차(MSE)를 최소화하는 파라미터를 찾는 것이다.&lt;br /&gt;
평균제곱오차는 예측값과 타깃값의 차이를 제곱하여 더한 후에 샘플의 개수로 나눈 것이다.&lt;br /&gt;
매개변수가 없는 것이 장점이지만 복잡도를 제어할 방법도 없다.&lt;br /&gt;
LinearRegression()&lt;/p&gt;

&lt;h6 id=&quot;리지-회귀&quot;&gt;리지 회귀&lt;/h6&gt;
&lt;p&gt;리지 회귀도 예측 함수를 사용하지만 가중치의 절댓값을 가능한 작게 만드는 목적을 갖는다.&lt;br /&gt;
이런 제약을 규제라고 하며 L2 규제라고 한다.&lt;br /&gt;
리지는 덜 자유로운 모델이라 과대적합이 적다. 따라서 일반화에 도움이 된다.&lt;br /&gt;
Ridge(alpha)&lt;/p&gt;

&lt;h6 id=&quot;라소&quot;&gt;라소&lt;/h6&gt;
&lt;p&gt;리지의 대안으로 라소가 있다. L1 규제라고도 하며 완전히 제외하는 특성이 생긴다.&lt;br /&gt;
일부 계수가 0이 되고 모델을 이해하기 쉬워지며 중요한 특성을 찾기 쉽ㄴ다.&lt;br /&gt;
max_itter을 조절하여 과소적합을 줄인다.&lt;br /&gt;
Lasso(alpha, max_itter())&lt;/p&gt;

&lt;h6 id=&quot;분류용-선형-모델&quot;&gt;분류용 선형 모델&lt;/h6&gt;
&lt;p&gt;이진 분류의 경우 선형 회귀와 비슷하지만 가중치 합을 그냥 사용하는 대신 예측값을 임계치 0과 비교한다.&lt;br /&gt;
0보다 작으면 -1, 크면 1이라고 예측한다. 결정 경계를 선형 함수로 잡는다.&lt;br /&gt;
LogisticRegression과 LinearSVC가 잘 알려져있는데 규제의 강도를 결정하는 매개변수 C를 주의해야한다.&lt;br /&gt;
C가 높으면 훈련 세트에 최대로 맞추려 노력하고 낮추면 계수 벡터(w)가 0에 가까워지도록 만든다.&lt;br /&gt;
로지스틱 회귀분석을 제외하면 다중 클래스를 대부분 지원하지 않는다.&lt;/p&gt;

&lt;h6 id=&quot;장단점과-매개변수-1&quot;&gt;장단점과 매개변수&lt;/h6&gt;
&lt;p&gt;회귀 모델에서는 alpha, LinearSVC와 LogisticRegression에서는 C가 중요하다.&lt;br /&gt;
alpha가 클수록, C가 작을수록 모델이 단순해진다. 로그 스케일로 최적치를 정한다.&lt;br /&gt;
L1, L2를 결정하는 것도 정해야한다. 중요한 특성이 적으면 L1, 그렇지 않으면 L2를 이용한다.&lt;br /&gt;
선형 모델은 학습 속도와 예측 속도가 빠르다. solver=’sag’ 옵션을 이용하면 더 빨리 처리할 수 있다.&lt;br /&gt;
아니면 SGDClassifier과 SGDRegressor을 이용할 수도 있다.&lt;br /&gt;
또한 선형 모델은 예측이 어떻게 만들어지는지 비교적 이해하기 쉽다. 하지만 계수의 값이 명확하지가 않다.&lt;/p&gt;

&lt;h5 id=&quot;naive-bayes-분류기&quot;&gt;Naive Bayes 분류기&lt;/h5&gt;

&lt;p&gt;나이브 베이즈 분류기는 선형 분류기보다 훈련 속도가 빠르지만 일반화 성능이 뒤쳐진다.&lt;br /&gt;
개별적으로 파라미터를 학습하고 특성에서 클래스별 통계를 단순하게 취합한다.&lt;br /&gt;
GaussianNB, BernoulliNB, MultinomialNB를 scikit-learn에서 구현되어있다.&lt;br /&gt;
GaussianNB는 연속적인, BernoulliNB는 이진 데이터를, MultinomialNB는 카운트 데이터를 적용한다.&lt;br /&gt;
BernoulliNB는 클래스의 특성중 0이 아닌 것이 몇 개인지 센다.&lt;br /&gt;
MultinomialNB는 클래스별 특성의 평균을, GaussianNB는 클래스별로 각 특성의 표준편차와 평균을 저장한다.&lt;br /&gt;
MultinomialNB와 GaussianNB는 선형 모델과 예측 공식이 갖지만 coef_는 기울기 w가 아니라 의미는 다르다.&lt;/p&gt;

&lt;h6 id=&quot;장단점과-매개변수-2&quot;&gt;장단점과 매개변수&lt;/h6&gt;
&lt;p&gt;MultinomialNB와 BernoulliNB는 모델 복잡도를 조절하는 alpha변수가 하나이다.&lt;br /&gt;
alpha가 주어지면 알고리즘이 모든 특성에 양의 값을 가진 데이터 포인트를 alpha 개수만큼 추가한다.&lt;br /&gt;
alpha가 크면 더 완만하고 덜 복잡한 모델이 나오지만 성능 변동은 비교적 크지 않다.&lt;br /&gt;
GaussianNB는 고차원 데이터 셋을 사용한다. 다른 모델은 데이터를 카운트하는 데 사용된다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>주건나</name>
        
        
      </author>

      

      
        <category term="study" />
      

      
        <summary type="html">머신러닝 공부 관련 글 머신러닝 정리 (1)-지도학습 (1) 머신러닝 정리 (2)-지도학습 (2) 머신러닝 정리 (3)-비지도학습 (1)</summary>
      

      
      
    </entry>
  
</feed>
