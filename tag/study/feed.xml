<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://keonju2.github.io/tag/study/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://keonju2.github.io/" rel="alternate" type="text/html" />
  <updated>2021-10-07T05:06:27+09:00</updated>
  <id>https://keonju2.github.io/tag/study/feed.xml</id>

  
  
  

  
    <title type="html">주건나’s Blog | </title>
  

  
    <subtitle>데이터 사이언티스트를 꿈꾸는 블로그입니다.</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">머신러닝 정리 (2) 지도학습 (2)</title>
      <link href="https://keonju2.github.io/study-ML2" rel="alternate" type="text/html" title="머신러닝 정리 (2) &lt;br&gt; 지도학습 (2)" />
      <published>2021-10-04T10:00:00+09:00</published>
      <updated>2021-10-04T10:00:00+09:00</updated>
      <id>https://keonju2.github.io/study-ML2</id>
      <content type="html" xml:base="https://keonju2.github.io/study-ML2">&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;머신러닝 공부 관련 글&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML1&quot;&gt;머신러닝 정리 (1)-지도학습 (1)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML2&quot;&gt;머신러닝 정리 (2)-지도학습 (2)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;머신러닝-정리-2---지도학습-2&quot;&gt;머신러닝 정리 (2) - 지도학습 (2)&lt;/h1&gt;
&lt;p&gt;본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.&lt;br /&gt;
또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.&lt;br /&gt;
글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.&lt;br /&gt;
코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;소스 코드: &lt;a href=&quot;https://github.com/rickiepark/introduction_to_ml_with_python&quot;&gt;https://github.com/rickiepark/introduction_to_ml_with_python&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;지도학습-2&quot;&gt;지도학습 (2)&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;결정 트리&lt;/li&gt;
  &lt;li&gt;결정 트리의 앙상블&lt;/li&gt;
  &lt;li&gt;배깅, 엑스트라 트리, 에이다부스트
    &lt;ol&gt;
      &lt;li&gt;배깅&lt;/li&gt;
      &lt;li&gt;엑스트라 트리&lt;/li&gt;
      &lt;li&gt;선형 모델&lt;/li&gt;
      &lt;li&gt;에이다부스트&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;커널 서포트 벡터 머신&lt;/li&gt;
  &lt;li&gt;신경망 (딥러닝)&lt;/li&gt;
  &lt;li&gt;분류 예측의 불확실성 추정
    &lt;ol&gt;
      &lt;li&gt;결정 함수&lt;/li&gt;
      &lt;li&gt;예측 확률&lt;/li&gt;
      &lt;li&gt;다중 분류에서의 불확실성&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;결정-트리&quot;&gt;결정 트리&lt;/h3&gt;

&lt;h3 id=&quot;결정-트리의-앙상블&quot;&gt;결정 트리의 앙상블&lt;/h3&gt;

&lt;h3 id=&quot;배깅-엑스트라-트리-에이다부스트&quot;&gt;배깅, 엑스트라 트리, 에이다부스트&lt;/h3&gt;

&lt;h4 id=&quot;배깅&quot;&gt;배깅&lt;/h4&gt;

&lt;h4 id=&quot;엑스트라-트리&quot;&gt;엑스트라 트리&lt;/h4&gt;

&lt;h4 id=&quot;선형-모델&quot;&gt;선형 모델&lt;/h4&gt;

&lt;h4 id=&quot;에이다부스트&quot;&gt;에이다부스트&lt;/h4&gt;

&lt;h1 id=&quot;파이썬기반-머신러닝3-데청캠&quot;&gt;파이썬기반 머신러닝(3)-데청캠&lt;/h1&gt;

&lt;h3 id=&quot;커널-서포트-벡터-머신&quot;&gt;커널 서포트 벡터 머신&lt;/h3&gt;

&lt;p&gt;커널 서포트 벡터 머신은 보통 SVM이라고 한다.&lt;br /&gt;
입력 데이터에서 단순한 초평면으로 정의되지 않는 더 복잡한 모델을 만들 수 있도록 확장한 것이다.&lt;br /&gt;
분류와 회귀 모두 사용 가능하다. (SVC는 분류, SVR은 회귀)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.svm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearSVC&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#선형 모델
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LinearSVC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.svm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# kernel, C, gamma 파라미터 존재
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h6 id=&quot;선형-모델과-비선형-특성&quot;&gt;선형 모델과 비선형 특성&lt;/h6&gt;
&lt;p&gt;선형 모델은 직선으로만 데이터 포인트를 나눌 수 있어 밑에 같은 데이터는 잘 들어맞지 않는다.&lt;br /&gt;
SVM 모델은 3차원에서 2차원으로 투영해본다면 더이상 선형 모델이 아니다.&lt;/p&gt;

&lt;h6 id=&quot;커널-기법&quot;&gt;커널 기법&lt;/h6&gt;
&lt;p&gt;커널 기법은 실제로 데이터를 확장하지 않고 확장된 특성에 대한 데이터 포인트들의 거리를 계산한다.&lt;br /&gt;
$ (특성1)^2 * (특성2)^5 $하는 다항식 커널이 있고 가우시안 커널로 불리우는 RBF 커널이 있다.&lt;br /&gt;
가우시안 커널은 차원이 무한한 특성 공간에 매핑하는 것이다.&lt;br /&gt;
모든 차수의 모든 다항식을 고려하지만 특성의 중요도는 고차항이 될수록 줄어든다.&lt;/p&gt;

&lt;h6 id=&quot;svm-이해하기&quot;&gt;SVM 이해하기&lt;/h6&gt;
&lt;p&gt;두 클래스 사이에 경계한 데이터 포인트들을 서포트 벡터라고 한다.&lt;br /&gt;
새로운 데이터 포인트에 대해 예측하려면 각 서포트 벡터와의 거리를 측정한다.&lt;br /&gt;
서포트 벡터의 중요도는 훈련 과정에서 학습하는데 dual_coef_ 속성에 저장된다.&lt;/p&gt;
&lt;h1 id=&quot;가우시안-커널-공식-사진&quot;&gt;가우시안 커널 공식 사진&lt;/h1&gt;
&lt;p&gt;가우시안 커널에 의해 계산되며 $ X_1, X_2 $는 데이터 포인트이며 $ ||X_1 - X_2|| $는 유클리디안 거리이고 $Γ$ 은 가우시안 커널의 폭을 제어하는 매개변수이다.&lt;/p&gt;

&lt;h6 id=&quot;svm-매개변수-튜닝&quot;&gt;SVM 매개변수 튜닝&lt;/h6&gt;
&lt;p&gt;$Γ$는 가우시안 커널 폭의 역수에 해당하는데 하나의 훈련 샘플이 미치는 영향의 범위를 결정한다.(1~0 사이의 범위이다.)&lt;br /&gt;
작은 값은 넓은 영역을 뜻하고, 큰 값은 영향이 미치는 범위가 제한적이다.&lt;br /&gt;
즉 커널의 반경이 클수록 훈련 샘플의 영향 범위도 커진다.&lt;br /&gt;
작은 $Γ$ 값은 모델의 복잡도를 낮출 수 있다.&lt;br /&gt;
C 매개 변수는 규제 매개변수이다. dual_coef_값을 제한합니다.&lt;br /&gt;
작은 C는 매우 제약이 큰 모델을 만들고 각 데이터 포인트의 영향력이 작다.&lt;br /&gt;
C를 증가시키면 이 포인트들이 영향을 크게 줘서 결정 경계를 휘게 만든다.&lt;/p&gt;

&lt;h6 id=&quot;svm을-위한-데이터-전처리&quot;&gt;SVM을 위한 데이터 전처리&lt;/h6&gt;
&lt;p&gt;커널 SVM에서는 데이터셋의 특성 자릿수가 완전히 다르면 영향을 크게 미친다.&lt;br /&gt;
따라서 특성 값을 평균이 0이고 단위 분산이 되도록 하거나, 0과 1 사이로 맞추는 방법을 많이 사용한다.(StandardScaler와 MinMaxScalar)&lt;/p&gt;

&lt;h6 id=&quot;장단점과-매개변수&quot;&gt;장단점과 매개변수&lt;/h6&gt;
&lt;p&gt;SVM은 저차원과 고차원의 데이터에 모두 잘 작동하지만 샘플이 많으면 잘 맞지 않는다.&lt;br /&gt;
또한 전처리와 매개변수 설정에 신경을 많이 써야하는데 그래서 랜덤 포레스트나 그레이디언트 부스팅과 같은 전처리가 거의 필요 없는 트리 기반 모델이 선호된다.&lt;br /&gt;
SVM은 분석도 어려워서 예측이 어떻게 결정되었는지 설명하기가 난해하다.&lt;br /&gt;
하지만 모든 특성이 비슷한 단위이고 스케일이 비슷하다면 시도해볼 만하다.&lt;br /&gt;
중요한 매개변수는 C이고 어떤 커널을 사용할지와 각 커널에 따른 매개변수이다.&lt;br /&gt;
RBF는 $Γ$ 매개변수를 갖지만 다른 커널 종류도 많다.&lt;/p&gt;

&lt;h3 id=&quot;신경망-딥러닝&quot;&gt;신경망 (딥러닝)&lt;/h3&gt;
&lt;p&gt;다층 퍼셉트론은(MLP)는 간단하게 분류와 회귀에서 쓰일 수 있다.&lt;/p&gt;

&lt;h6 id=&quot;신경망-모델&quot;&gt;신경망 모델&lt;/h6&gt;
&lt;p&gt;MLP는 여러 단계를 거처 결정을 만들어내는 선형 모델의 일반화된 모습이다.&lt;/p&gt;
&lt;h1 id=&quot;선형-회귀-모델의-예측-공식-사진&quot;&gt;선형 회귀 모델의 예측 공식 사진&lt;/h1&gt;
&lt;p&gt;$\hat Y $는 x[0]에서 x[p]까지의 입력특성과 학습된 계수의 가중치의 합이다.&lt;/p&gt;

&lt;h1 id=&quot;퍼셉트론-사진&quot;&gt;퍼셉트론 사진&lt;/h1&gt;
&lt;p&gt;왼쪽 노드는 입력 특성을 나타내며 연결선은 학습된 계수를 표현하고 오른쪽 노드는 입력의 가중치 합, 즉 출력을 나타낸다.&lt;br /&gt;
MLP는 가중치 합을 만드는 과정이 여러 번 반복되며 먼저 중간 단계를 구성하는 은닉 유닛을 계산하고 이를 이용하여 최종 결과를 산출하기 위해 다시 가중치 합을 계산한다.&lt;/p&gt;
&lt;h1 id=&quot;다중-퍼셉트론-사진&quot;&gt;다중 퍼셉트론 사진&lt;/h1&gt;
&lt;p&gt;각 은닉 유닛의 가중치 합을 계산한 후 결과에 비선형 함수인 렐루나 하이퍼볼릭 탄젠트, 시그모이드 함수를 적용합니다.&lt;/p&gt;
&lt;h1 id=&quot;회귀-분석-사진&quot;&gt;회귀 분석 사진&lt;/h1&gt;
&lt;p&gt;w는 입력 x와 은닉층 h 사이의 가중치이고, v는 은닉층 h와 출력 $\hat Y$ 사이의 가중치입니다.&lt;br /&gt;
w와 v는 훈련 데이터에서 학습하고 x는 입력 특성이며 $ \hat Y $는 계산된 출력, h는 중간 계산값 입니다.&lt;/p&gt;

&lt;h6 id=&quot;신경망-튜닝&quot;&gt;신경망 튜닝&lt;/h6&gt;
&lt;p&gt;더 복잡도가 낮은 모델을 만들고 싶다면 hidden_layer_size를 통해 은닉 유닛의 개수를 줄인다.&lt;br /&gt;
은닉 유닛을 추가하거나, 은닉층을 추가하거나 활성화함수를 바꾸면 더 매끄러운 결정 경계를 얻을 수도 있다.&lt;br /&gt;
선형 분류와 리지 회귀 처럼 L2 페널티를 사용해서 가중치를 0에 가깝게 감소시킬 수도 있다.(default는 매우 낮다)&lt;br /&gt;
신경망에서는 학습을 시작하기 전에 가중치를 무작위로 설정하며 이 무작위한 초기화가 모델의 학습에 영향을 준다.&lt;br /&gt;
따라서 같은 매개변수를 사용하더라도 초깃값이 다르면 모델이 많이 달라질 수 있다.&lt;br /&gt;
신경망도 입력 특성이 평균은 0 분산이 1이 되도록 변형하는 것이 좋다.&lt;br /&gt;
은닉 유닛에서 작은 가중치를 가진 특성은 모델에 덜 중요하다고 추론할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.neural_network&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLPClassifier&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# MLP분류
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MLPClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_layer_sizes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_itter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;#solver에 최적화 알고리즘,activation에 활성화 함수 ,hidden_layer_size로 은닉 유닛의 개수 설정(default=100),max_itter은 반복 횟수,alpha는 L2 페널티
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h6 id=&quot;장단점과-매개변수-1&quot;&gt;장단점과 매개변수&lt;/h6&gt;
&lt;p&gt;머신러닝 알고리즘을 뛰어넘는 성능을 보일 수 있지만 학습이 오래걸리고 데이터 전처리를 주의해서 해야한다.&lt;br /&gt;
모든 특성이 같은 의미를 가지면 SVM, 다른 종류의 특성이라면 트리 기반 메딜이 더 잘 작동할 수 있다.&lt;/p&gt;

&lt;h6 id=&quot;신경망의-복잡도-추정&quot;&gt;신경망의 복잡도 추정&lt;/h6&gt;
&lt;p&gt;가장 중요한 매개변수는 은닉층의 개수와 각 은닉층의 유닛 수이다.&lt;br /&gt;
복잡도에 관해 연관된 측정치는 학습된 가중치 또는 계수의 수이다.&lt;br /&gt;
특성이 100개 은닉 유닛 100개인 이진 분류라면 입력층과 첫 번째 은닉층 사이에는 편향을 포함하여 $ 100 * 100 + 100 = 10100 $개의 가중치가 있습니다.&lt;br /&gt;
은닉층과 출력층 사이에 $ 100 * 1 + 1 = 101 $개의 가중치가 더 있어 가중치는 10201개 이다.&lt;br /&gt;
이렇게 가중치는 은닉층을 추가할수록 훨씬 커지게 된다.&lt;br /&gt;
매개변수를 조정하는 일반적인 방법은 충분히 과대적합되어 문제를 해결할만한 큰 모델을 만든 뒤 훈련 데이터가 충분히 학습될 수 있다고 생각되면 신경망 구조를 줄이거나 규제 강화를 위해 alpha 값을 증가시켜 일반화 성능을 향상시킨다.&lt;br /&gt;
층의 개수, 층당 유닛 개수, 규제, 비선형성으로 모델 구성을 할 수 있으며, solver 매개변수를 통해서 학습시키는 방법을 지정할 수 있다.&lt;br /&gt;
solver의 경우 기본값은 adam이고 데이터 스케일에 민감하다.&lt;br /&gt;
lbfgs는 안정적이지만 규모가 크면 시간이 오래 걸린다&lt;br /&gt;
sgd는 momentum과 nesterovs_momentom의 영향을 받는데 다른 여러 매개변수와 함께 튜닝하여 최선의 결과를 만들 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;분류-예측의-불확실성-추정&quot;&gt;분류 예측의 불확실성 추정&lt;/h3&gt;
&lt;p&gt;decision_function과 predict_proba로 추정 할 수 있다.&lt;/p&gt;

&lt;h6 id=&quot;결정-함수&quot;&gt;결정 함수&lt;/h6&gt;
&lt;p&gt;decision_function의 반환값의 크기는 (n_samples,)이며각 샘플이 하나의 실수 값을 반환한다.&lt;br /&gt;
모델이 데이터 포인트가 양성 클래스인 클래스 1에 속한다고 믿는 정도이다.&lt;br /&gt;
즉, 음수값은 다른 클래스에 속함을 의미한다.&lt;br /&gt;
값의 범위는 데이터와 모델 파라미터에 따라 달라지게 된다.&lt;/p&gt;

&lt;h6 id=&quot;예측-확률&quot;&gt;예측 확률&lt;/h6&gt;
&lt;p&gt;predict_proba의 출력은 각 클래스에 대한 확률이고 이진 분류에서 이 값의 크기는 항상 (n_samples,2)이다.&lt;br /&gt;
두 클래스의 확률 합은 1이므로 두 클래스 중 하나는 50% 이상의 확신을 가질 것이고 그 클래스가 예측값이 된다.&lt;br /&gt;
데이터에 있는 불확실성이 얼마나 이 값에 잘 반영되는지는 모델과 매개변수 설정에 달렸다.&lt;br /&gt;
그래서 과대적합된 모델 혹은 잘못된 예측도 예측의 확신이 강한 편이다.&lt;br /&gt;
복잡도가 낮을 수록 예측에 불확실성이 더 많다.&lt;br /&gt;
불확실성과 모델의 정확도가 동등하면 이 모델이 보정되었다고 한다.&lt;/p&gt;

&lt;h6 id=&quot;다중-분류에서의-불확실성&quot;&gt;다중 분류에서의 불확실성&lt;/h6&gt;
&lt;p&gt;다중 분류에서도 decision_funcion과 predict_proba를 사용할 수 있다.&lt;br /&gt;
decision_function에서는 (n_samples, n_classes)가 결과값이 된다.&lt;br /&gt;
글 클래스에 대한 확신 점수를 담고 그 수치가 크면 그 클래스일 가능성이 크다.&lt;br /&gt;
데이터 포인트마다 점수들에서 가장 큰 값을 찾아 예측 결과를 재현할 수 있다.&lt;br /&gt;
predict_proba는 (n_samples,n_classes)가 출력값이 된다.&lt;br /&gt;
마찬가지로 각 데이터 포인트에서 클래스 확률의 합은 1이다.&lt;br /&gt;
argmax 함수를 적용해서 예측 결과를 재현할 수 있지만 클래스가 문자열이거나 정수형을 사용하지만 연속적이지 않고 0부터 시작하지 않을 수 있다.&lt;br /&gt;
따라서 predict 결과와 decision_function, predict_proba의 결과를 비교하기 위해서는 분류기의 classes_ 속성을 사용해 클래스의 실제 이름을 얻어야 한다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>주건나</name>
        
        
      </author>

      

      
        <category term="study" />
      

      
        <summary type="html">머신러닝 공부 관련 글 머신러닝 정리 (1)-지도학습 (1) 머신러닝 정리 (2)-지도학습 (2) 머신러닝 정리 (2) - 지도학습 (2) 본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다. 또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다. 글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다. 코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다. 소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">머신러닝 정리 (1) 지도학습 (1)</title>
      <link href="https://keonju2.github.io/study-ML1" rel="alternate" type="text/html" title="머신러닝 정리 (1) &lt;br&gt; 지도학습 (1)" />
      <published>2021-09-27T10:00:00+09:00</published>
      <updated>2021-09-27T10:00:00+09:00</updated>
      <id>https://keonju2.github.io/study-ML1</id>
      <content type="html" xml:base="https://keonju2.github.io/study-ML1">&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;머신러닝 공부 관련 글&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML1&quot;&gt;머신러닝 정리 (1)-지도학습 (1)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./study-ML2&quot;&gt;머신러닝 정리 (2)-지도학습 (2)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;머신러닝-정리-1---지도학습-1&quot;&gt;머신러닝 정리 (1) - 지도학습 (1)&lt;/h1&gt;
&lt;p&gt;본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.&lt;br /&gt;
또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.&lt;br /&gt;
글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.&lt;br /&gt;
코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;소스 코드: &lt;a href=&quot;https://github.com/rickiepark/introduction_to_ml_with_python&quot;&gt;https://github.com/rickiepark/introduction_to_ml_with_python&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;지도학습-1&quot;&gt;지도학습 (1)&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;분류와 회귀&lt;/li&gt;
  &lt;li&gt;일반화, 과대적합, 과소적합
    &lt;ol&gt;
      &lt;li&gt;모델복잡도와 데이터셋 크기의 관계&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;지도 학습 알고리즘
    &lt;ol&gt;
      &lt;li&gt;예제에 사용할 데이터셋&lt;/li&gt;
      &lt;li&gt;k-nn 모델&lt;/li&gt;
      &lt;li&gt;선형 모델&lt;/li&gt;
      &lt;li&gt;Naive Bayes 분류기&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;분류와-회귀&quot;&gt;분류와 회귀&lt;/h3&gt;

&lt;p&gt;분류란? 미리 정의된 가능성 있는 여러 클래스 레이블 중 하나를 예측하는 것이다.&lt;br /&gt;
이진 분류: 예, 아니오로 구분할 수 있다. ex) 이 이메일은 스팸인가요?&lt;br /&gt;
다중 분류: 셋 이상의 클래스로 분류된다. ex) 붓꽃 데이터&lt;/p&gt;

&lt;p&gt;회귀란? 부동소수점수(수학으로 말하면 실수)를 예측하는 것이다.&lt;br /&gt;
어떤 사람의 여러 조건들을 통해 연간 소득을 예측하는 것과 같은 문제이다.&lt;/p&gt;

&lt;p&gt;출력 값에 연속성이 있다면 회귀 문제 없다면 분류 문제이다.&lt;/p&gt;

&lt;h3 id=&quot;일반화-과대적합-과소적합&quot;&gt;일반화, 과대적합, 과소적합&lt;/h3&gt;

&lt;p&gt;모델이 처음 보는 데이터에 대해 정확히 예측할 수 있다면 훈련 세트(train_set)에서 테스트 세트(test_set)으로 일반화되었다고 한다.&lt;br /&gt;
훈련 세트에 대해서 정확히 예측하고 테스트 세트에서도 정확히 예측하길 바란다.&lt;br /&gt;
하지만 모델이 복잡하다면 훈련 세트에서만 정확한 모델이 될 수 있다.&lt;br /&gt;
예를 들어 “내 주변 20대가 모두 아이폰을 쓰기 때문에 다른 20대도 모두 아이폰을 살 것이다.”라는 예측을 한다면 훈련 세트가 내 주변 20대가 될 것이고 테스트 세트가 다른 모든 20대가 될 것이다.&lt;br /&gt;
이처럼 알고리즘이 새로운 데이터를 잘 처리하는지 측정하는 방법은 테스트 세트로 평가를 해야한다.&lt;br /&gt;
이 때, 너무 복잡한 모델을 만들어 훈련 세트에 집중되어 테스트 세트에 일반화가 부족하다면 과대적합(overfitting)이라 한다.&lt;br /&gt;
반대로 너무 간단한 모델이라 훈련 세트에도 잘 맞지 않다면 과소적합(underfitting)이라 한다.&lt;/p&gt;

&lt;h5 id=&quot;모델복잡도와-데이터셋-크기의-관계&quot;&gt;모델복잡도와 데이터셋 크기의 관계&lt;/h5&gt;

&lt;p&gt;모델의 복잡도는 훈련 데이터 셋에 담긴 입력 데이터의 다양성과 관련이 있다.&lt;br /&gt;
데이터셋에 데이터 포인트가 다양하면 과대적합 없이 복잡한 모델을 만들 수 있다.&lt;br /&gt;
따라서 중복이거나 비슷한 데이터를 모으는 것은 도움이 되지않는다.&lt;br /&gt;
위의 예를 생각해보면 내 주변 20대라는 특징말고 대학 동기, 친구라는 데이터를 얻더라도 모두 20대라는 범주안에 들어갈테니 불필요한 데이터라 할 수 있다.&lt;br /&gt;
따라서 좋은 데이터를 많이 얻는 것이 좋다.&lt;/p&gt;

&lt;h3 id=&quot;지도-학습-알고리즘&quot;&gt;지도 학습 알고리즘&lt;/h3&gt;

&lt;p&gt;각 모델의 장단점과 어떤 데이터와 어울리는지, 매개변수와 옵션의 의미를 알아보도록 하자.&lt;br /&gt;
scikit-learn 문서를 참고하면 더 자세한 정보를 얻을 수 있다.&lt;/p&gt;

&lt;h5 id=&quot;예제에-사용할-데이터셋&quot;&gt;예제에 사용할 데이터셋&lt;/h5&gt;

&lt;p&gt;forge 데이터셋은 인위적으로 만든 이진 분류 데이터셋이다.&lt;/p&gt;

&lt;h5 id=&quot;k-nn-모델&quot;&gt;k-NN 모델&lt;/h5&gt;

&lt;p&gt;k-NN 모델은 단순히 데이터셋을 분류하는 것이다.&lt;br /&gt;
k개의 레이블 중에서 어느 쪽에 더 가까운 것인지 투표를 하여 결정한다고 생각하면 이해하기 편하다.&lt;br /&gt;
k=1일 때는 가장 가까운 것이 빨간색이었다면, k=3일 때는 파란색 두 개와 빨간색 한 개가 가까울 수도 있다.&lt;br /&gt;
그렇게 된다면 k=3일 때는 파란색으로 분류가 된다.&lt;/p&gt;

&lt;p&gt;KNeighborClassifier(n_neighbors)을 통해 분류 모델을 만들 수 있다.&lt;br /&gt;
이를 통해 확인할 수 있는 것은 k의 값이 커질수록 보다 단순한 모델이 만들어질 수 있다는 것이다.&lt;br /&gt;
하지만 반드시 k가 커진다고 좋은 모델은 아니다. 정확도가 낮아질 수 있기 때문이다.&lt;/p&gt;

&lt;p&gt;KNeighborsRegressor(n_neighbors)을 통해 회귀 모델 또한 만들 수 있다.&lt;br /&gt;
회귀 모델에서도 k를 너무 적게 쓴다면 모든 데이터를 지나가고 불안정한 모델이 만들어진다.&lt;/p&gt;

&lt;h6 id=&quot;장단점과-매개변수&quot;&gt;장단점과 매개변수&lt;/h6&gt;
&lt;p&gt;가장 중요한 매개변수는 거리를 재는 방법과 k값이다.&lt;br /&gt;
보통 거리를 재는 방법은 유클리디안 거리 방식을 사용한다.&lt;br /&gt;
장점은 이해하기가 쉬운 모델이고 조정을 많이 하지않아도 좋은 성능을 발휘할 수 있다는 것이다.&lt;br /&gt;
단점은 훈련 세트가 크면 예측이 느려지고 전처리 과정이 중요하다는 것이다.&lt;br /&gt;
데이터가 특성이 많거나 대부분이 0인 데이터셋에서는 잘 작동하지 않는다.&lt;/p&gt;

&lt;h5 id=&quot;선형-모델&quot;&gt;선형 모델&lt;/h5&gt;

&lt;p&gt;선형 모델은 $y=w[0]*x[0]+b$와 같은 모델을 갖는 예측 함수이다.&lt;br /&gt;
y는 예측값, w와 b는 모델이 학습할 파라미터, x는 데이터의 특성이다. 위 식은 특성이 하나인 데이터 셋의 선형 함수이다.&lt;/p&gt;

&lt;h6 id=&quot;선형-회귀최소제곱법&quot;&gt;선형 회귀(최소제곱법)&lt;/h6&gt;
&lt;p&gt;선형 회귀는 예측과 훈련 세트에 있는 타깃 사이의 평균제곱오차(MSE)를 최소화하는 파라미터를 찾는 것이다.&lt;br /&gt;
평균제곱오차는 예측값과 타깃값의 차이를 제곱하여 더한 후에 샘플의 개수로 나눈 것이다.&lt;br /&gt;
매개변수가 없는 것이 장점이지만 복잡도를 제어할 방법도 없다.&lt;br /&gt;
LinearRegression()&lt;/p&gt;

&lt;h6 id=&quot;리지-회귀&quot;&gt;리지 회귀&lt;/h6&gt;
&lt;p&gt;리지 회귀도 예측 함수를 사용하지만 가중치의 절댓값을 가능한 작게 만드는 목적을 갖는다.&lt;br /&gt;
이런 제약을 규제라고 하며 L2 규제라고 한다.&lt;br /&gt;
리지는 덜 자유로운 모델이라 과대적합이 적다. 따라서 일반화에 도움이 된다.&lt;br /&gt;
Ridge(alpha)&lt;/p&gt;

&lt;h6 id=&quot;라소&quot;&gt;라소&lt;/h6&gt;
&lt;p&gt;리지의 대안으로 라소가 있다. L1 규제라고도 하며 완전히 제외하는 특성이 생긴다.&lt;br /&gt;
일부 계수가 0이 되고 모델을 이해하기 쉬워지며 중요한 특성을 찾기 쉽ㄴ다.&lt;br /&gt;
max_itter을 조절하여 과소적합을 줄인다.&lt;br /&gt;
Lasso(alpha, max_itter())&lt;/p&gt;

&lt;h6 id=&quot;분류용-선형-모델&quot;&gt;분류용 선형 모델&lt;/h6&gt;
&lt;p&gt;이진 분류의 경우 선형 회귀와 비슷하지만 가중치 합을 그냥 사용하는 대신 예측값을 임계치 0과 비교한다.&lt;br /&gt;
0보다 작으면 -1, 크면 1이라고 예측한다. 결정 경계를 선형 함수로 잡는다.&lt;br /&gt;
LogisticRegression과 LinearSVC가 잘 알려져있는데 규제의 강도를 결정하는 매개변수 C를 주의해야한다.&lt;br /&gt;
C가 높으면 훈련 세트에 최대로 맞추려 노력하고 낮추면 계수 벡터(w)가 0에 가까워지도록 만든다.&lt;br /&gt;
로지스틱 회귀분석을 제외하면 다중 클래스를 대부분 지원하지 않는다.&lt;/p&gt;

&lt;h6 id=&quot;장단점과-매개변수-1&quot;&gt;장단점과 매개변수&lt;/h6&gt;
&lt;p&gt;회귀 모델에서는 alpha, LinearSVC와 LogisticRegression에서는 C가 중요하다.&lt;br /&gt;
alpha가 클수록, C가 작을수록 모델이 단순해진다. 로그 스케일로 최적치를 정한다.&lt;br /&gt;
L1, L2를 결정하는 것도 정해야한다. 중요한 특성이 적으면 L1, 그렇지 않으면 L2를 이용한다.&lt;br /&gt;
선형 모델은 학습 속도와 예측 속도가 빠르다. solver=’sag’ 옵션을 이용하면 더 빨리 처리할 수 있다.&lt;br /&gt;
아니면 SGDClassifier과 SGDRegressor을 이용할 수도 있다.&lt;br /&gt;
또한 선형 모델은 예측이 어떻게 만들어지는지 비교적 이해하기 쉽다. 하지만 계수의 값이 명확하지가 않다.&lt;/p&gt;

&lt;h5 id=&quot;naive-bayes-분류기&quot;&gt;Naive Bayes 분류기&lt;/h5&gt;

&lt;p&gt;나이브 베이즈 분류기는 선형 분류기보다 훈련 속도가 빠르지만 일반화 성능이 뒤쳐진다.&lt;br /&gt;
개별적으로 파라미터를 학습하고 특성에서 클래스별 통계를 단순하게 취합한다.&lt;br /&gt;
GaussianNB, BernoulliNB, MultinomialNB를 scikit-learn에서 구현되어있다.&lt;br /&gt;
GaussianNB는 연속적인, BernoulliNB는 이진 데이터를, MultinomialNB는 카운트 데이터를 적용한다.&lt;br /&gt;
BernoulliNB는 클래스의 특성중 0이 아닌 것이 몇 개인지 센다.&lt;br /&gt;
MultinomialNB는 클래스별 특성의 평균을, GaussianNB는 클래스별로 각 특성의 표준편차와 평균을 저장한다.&lt;br /&gt;
MultinomialNB와 GaussianNB는 선형 모델과 예측 공식이 갖지만 coef_는 기울기 w가 아니라 의미는 다르다.&lt;/p&gt;

&lt;h6 id=&quot;장단점과-매개변수-2&quot;&gt;장단점과 매개변수&lt;/h6&gt;
&lt;p&gt;MultinomialNB와 BernoulliNB는 모델 복잡도를 조절하는 alpha변수가 하나이다.&lt;br /&gt;
alpha가 주어지면 알고리즘이 모든 특성에 양의 값을 가진 데이터 포인트를 alpha 개수만큼 추가한다.&lt;br /&gt;
alpha가 크면 더 완만하고 덜 복잡한 모델이 나오지만 성능 변동은 비교적 크지 않다.&lt;br /&gt;
GaussianNB는 고차원 데이터 셋을 사용한다. 다른 모델은 데이터를 카운트하는 데 사용된다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>주건나</name>
        
        
      </author>

      

      
        <category term="study" />
      

      
        <summary type="html">머신러닝 공부 관련 글 머신러닝 정리 (1)-지도학습 (1) 머신러닝 정리 (2)-지도학습 (2)</summary>
      

      
      
    </entry>
  
</feed>
