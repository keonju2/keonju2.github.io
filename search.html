<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
    
    <!-- custom.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />
    
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- 웹폰트 추가-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">

    <!-- syntax.css 추가-->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="데이터 사이언티스트를 꿈꾸는 블로그입니다." />
    <link rel="shortcut icon" href="https://keonju2.github.io/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="https://keonju2.github.io/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="주건나's Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="데이터 사이언티스트를 꿈꾸는 블로그입니다." />
    <meta property="og:url" content="https://keonju2.github.io/search" />
    <meta property="og:image" content="https://keonju2.github.io/assets/built/images/blog-cover1.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/monkeykeonju" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="데이터 사이언티스트를 꿈꾸는 블로그입니다." />
    <meta name="twitter:url" content="https://keonju2.github.io/" />
    <meta name="twitter:image" content="https://keonju2.github.io/assets/built/images/blog-cover1.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="주건나's Blog" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "주건나's Blog",
        "logo": "https://keonju2.github.io/"
    },
    "url": "https://keonju2.github.io/search",
    "image": {
        "@type": "ImageObject",
        "url": "https://keonju2.github.io/assets/built/images/blog-cover1.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://keonju2.github.io/search"
    },
    "description": "데이터 사이언티스트를 꿈꾸는 블로그입니다."
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="https://keonju2.github.io/">주건나's Blog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-R" role="menuitem"><a href="/tag/programming/">프로그래밍 언어</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/activity/">대외활동</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/license/">자격증</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/study/">이론 공부</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Tag별 Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
                <a class="social-link social-link-fb" href="https://facebook.com/monkeykeonju" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>
</a>
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "activity-devocean19": {
        "title": "DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계2023년의 딥러닝과 LLM 생태계이번 세미나는 제가 가장 관심있어하는 LLM과 딥러닝에 관한 이야기를 해주셨습니다.거대 언어 모델에 대한 이해, 개발 요소, 민주화, 변화들, 상용화와 도전 과제에 관한 이야기를 해주셨습니다.https://devocean.sk.com/vlog/view.do?id=429&amp;vcode=A03순서는  2023년 거대 언어 모델의 대두  거대 언어 모델 기반의 응용 사례  거대 언어 모델 기반 서비스의 난제  2023년 초, 중순의 변화의 시사점과 향후 방향2023년 거대 언어 모델의 대두생성 언어 모델은 최근 세분화된 모델에서 벗어나 결과물을 생성할 수 있는 모델들이 각광받고 있습니다.2018년 Transformer, BERT 같은 모델들이 나오게 됐고 2020년 GPT-3 모델이 등장하게 되었습니다.2021~2022년엔 모델을 키우고, 10B 이상의 모델들이 나오고 (RLHF의 이득을 가장 많이 보는) 100B (거대 언어모델의 동작을 가르는 지점)이라고 합니다.이를 통해 OPT, GLM 같은 모델들이 등장하게 되었습니다. 이 단계에서는 Zero-shot 번역, Galactica, Chat GPT 등 서비스가 등장하게 됩니다. (Instruct GPT)LLM은 챗봇이 아니다.챗봇은 실제 대화하는 것이 아니라 여러 대화 모델들을 인풋으로 뒤에 생성된 내용들이 명확해진다고 합니다.이 과정에서 프롬프트가 사용되며, RAG(Retrieval-Augumented Generation) 같은 형태로 활용되고 있습니다.사전 훈련 모델과 기반 모델인 Foundation Model이라고 하는 데 이를 다양한 용도에 맞춰 파인 튜닝과 In-context Learning을 진행합니다.가장 많이 발표되고 있는 부분이 사전 훈련 언어모델로 PaLM2, Claude, Falcon LLM, LLama2와 같은 모델들이 있습니다.거대 언어 모델 기반 서비스의 난제GPU, NPU와 같은 하드웨어에서 문제가 발생하고 있습니다. 이는 거대 언어 모델 스케일과도 연관되어있는데, PaLM을 훈련할때는 A100 GPU 112장이, Chat GPT 인퍼런스에는 A100 GPU 10장이, GPT-3 인퍼런스에는 A100 GPU 4장이 필요하다고 합니다.거대 언어모델 용어말뭉치, 지식 베이스, In context Learning 같은 방법들이 이를 극복하기 위한 수단으로 활용되고 있습니다.네트워크 또한 NVLink, NVSwitch와 같은 기술들의 발전, 분산 파일 시스템 같은 데이터 확장성 등이 연구되고 있습니다.최근 파인 튜닝으로는 LoRA와 같은 방법을 통해 Adaptor을 바꾸거나 MegatronLM 같은 미리 만들어진 모델들을 자동으로 할 수 있다고 합니다.2023년 초, 중순의 변화의 시사점과 향후 방향Chat GPT의 사업성은 비용이 굉장히 많이 드는 사업이라고 합니다.또한 공개가능한 모델들이 등장하고 있습니다.",
        "url": "/activity-devocean19"
    }
    ,
    
    "activity-devocean18": {
        "title": "SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.다름이 아니라 이번 SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 열어서 공유 드립니다.이번 TECH SUMMIT은 특히 AI, Cloud에 관한 주제가 많더라구요!!SKT, 쉴더스, C&amp;C, 브로드밴드 등 SK 그룹사의 에이닷, 환경, 헬스케어, 보안, 모빌리티, 제조, 반도체 등 AI를 활용한 다양한 주제의 전시회와 세션들이 준비되어 있다고 합니다!또한 전문가와 밋업도 선착순 50명을 받는다고 하네요!!관심 있으신 분들은 많은 신청 바랍니다~~~~2023년, SK의 AI 기술은 어떤 미래를 그리고 있을까요?오늘의 기술이 만들어가는 더 편리하고 안전한 내일의 모습을 선보입니다.최고의 기술 기업과 전문가들이 함께 바꾸고 있는 우리의 일상그 경험과 노하우를 나누는 자리에 여러분을 초대합니다.SK TECH SUMMIT 2023:날짜: 11. 16(목), 17(금) 10:00-18:00  :장소: COEX SEOULSK TECH SUMMIT 2023에 참가해야 하는 이유  ∨ SK의 엔지니어들이 발표하는 치열한 도전과 노하우   ∨ 생생한 경험으로 가득한 12개 테마 기술 전시  ∨ 함께 소통하는 요즘 개발자들의 커리어와 개발 문화  :파란색_하트: SK TECH SUMMIT 2023의 특별한 재미  ∨ 행사장 곳곳에 숨겨진 QR코드를 모으면 선물이!  ∨ 푸짐한 상품의 LUCKY DRAW와 하루 두 번 룰렛 타임~!  ∨ 기술 전문가와의 커피챗까지!  #SKTECHSUMMIT #테크서밋 #기술컨퍼런스 #개발자컨퍼런스 #AI컨퍼런스 #SK #SKT![SK TECH SUMMIT 2023_홍보이미지3](https://github.com/keonju2/keonju2.github.io/assets/54880474/72a866bd-db31-4229-a81b-4f8b8082f933)",
        "url": "/activity-devocean18"
    }
    ,
    
    "activity-devocean17": {
        "title": "DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례9월 테크 세미나 주제는 In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례였습니다.In-Memory Data Grid에 대한 내용과 하이닉스에서 어떤 식으로 활용되고 있는지에 대한 설명을 해주시는 세미나였습니다.https://devocean.sk.com/vlog/view.do?id=428&amp;vcode=A03순서는  SK 하이닉스 Data hub 시스템 소개  Data hub 시스템에서의 In-Memory Data Grid 도입 필요성  In-Memory Data Grid 구조 소개와 SK하이닉스 신규 메모리 적용 사례역순으로 진행되었습니다.In-Memory Data Grid 구조 소개와 SK하이닉스 신규 메모리 적용 사례In-Memory Data Grid란 데이터 통합 관리를 위한 computing engine이 탑재된 In-Memory distributed caching software 기술입니다.이종 데이터 소스로부터 통합 view를 제공하고 다수의 서버들을 연결하여 클러스터를 구축하고 대용량 메모리 풀을 형성, 데이터 로딩을 위해 대용량 메모리를 요구합니다.이종 데이터 소스는 SQL, NoSQL, Hadoop 같은 것들이 있습니다.Hazelcast IMDG라는 플랫폼을 사용하였는데 이는 서버마다 자바 프로세스를 생성하고 네트워크를 연결하여 클러스터를 생성합니다.다양한 언어 클라이언트 연결을 지원하고 이기종 DB와 연동합니다.클러스터는 임베디드 모드와 클라이언트/서버 모드로 분류됩니다.SQL 엔진으로 Apache Calcite를 탑재하여 키/벨류 API를 지원하고 대용량 데이터 로딩에 유리합니다.Data Partitioning은 IMap, Java Map의 분산형으로 271개의 데이터 파티션이 생성되고, 파티션 1개는 256개가 디폴트로 설정됩니다.오픈소스와 엔터프라이즈 에디션이 있으며 JVM on-heap 메모리 설정, HDMS Off-heap 스토리지 메모리, 메모리 포맷, 라이센스 키가 주요 설정입니다.SQL을 활용하여 IMap을 생성하고 JDBC연결을 통해 SQL 질의가 가능합니다.Management Center을 통해 파티션 분산, 가비지 콜렉터 등을 확인할 수 있습니다.Data hub 시스템에서의 In-Memory Data Grid 도입 필요성CXL이란 Compute Express Link의 약자로 PCle slot을 이용하는 차세대 고성능 interface입니다.DRAM과 같이 휘발성 메모리이며 내부에서는 DDR5 표준 DRAM을 사용합니다.SK 하이닉스 CXL 메모리의 특징으론 Local DRAM에 추가적으로 메모리 bandwith 및 capacity를 확장하며 서버 추가 없이 메모리를 확장할 수 있습니다.이를 통해 전사 실시간 데이터 공유 플랫폼인 Data Hub 환경에 Data Hub를 모사하여 DDR5 Dram과 CXL 메모리가 탑재된 서버에 소규모 Hazelcast IMDG Cluster을 구축하였습니다.이는 SK TECH SUMMIT(11월 16일)에 추가로 발표하신다고 하셨습니다!SK 하이닉스 Data hub 시스템 소개Data Hub는 메모리 시스템 부서와 데이터 엔지니어링 팀의 니즈에 맞게 되어 PoC를 진행하게 되었다고 합니다.전사 데이터 Interface 단일 Hub 역할을 수행하는 실시간 데이터 공유 플랫폼으로 DB CRUD, 기타 SQL 문 등 오라클을 대체할 수 있는지에 대해 검증을 진행했습니다.다만 Sub Query와 Scalar 쿼리를 지원하지 않아 불편함이 있을 수 있다고 합니다.검증 결과 실제 사용성에 아쉬움이 있다고 합니다.이번 세미나는 메모리와 Data Engineering이 결합된 세미나였어서 이해하기 굉장히 힘들었던 것 같습니다. SK 하이닉스에서 자체적으로 메모리 시스템 부서와 데이터 엔지니어링 팀끼리 PoC를 진행하고 실제 객관적인 성능 테스트를 진행하여 평가했던 점이 인상깊었습니다.",
        "url": "/activity-devocean17"
    }
    ,
    
    "activity-devocean16": {
        "title": "DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계업무 생산성 향상을 위한 생성형 AI 사용8월 테크 세미나 주제는 다가오는 업무 생산성 향상을 위한 생성형 AI 사용이었습니다.https://devocean.sk.com/vlog/view.do?id=426&amp;vcode=A03지난 세미나에 이어 이번 세미나도 AI에 관한 내용이었습니다. 생성형 AI에 대한 내용 과 구글의 생성형 AI Duet, AI를 이용한 업무에 대한 내용으로 조코디 님께서 발표를 진행해주셨습니다. 게임과 퀴즈를 이용한 재미있는 강의가 인상깊었고 편안하게 들을 수 있는 내용들으로 구성해주신 점이 좋았습니다.순서는  생성형 AI란?  Google의 생성형 AI Duet  Duet을 이용한 업무 생산성 향상  AI와 함께 일하기 위한 준비  결론순으로 진행되었습니다.Pre-Train, Fine Tune, 파운데이션 모델, 프롬프트 엔지니어, LLM, NN, GPU, 멀티모달 등 AI를 전혀 모르더라도 이해하기 쉽게 설명해주셨던 점이 좋았습니다.LLM을 사용할 때 주의해야할 할루시네이션이나, 보안 문제에 대해서도 이야기해주었고 조직 리터러시에서 어떤 식으로 활용 방안을 늘리기 위한 해커톤을 제안하기도 하였습니다.앞으로 AI가 발전하면서 AI를 활용하지 못한다면 업무 효율성이 나오기 힘들며, 뤼튼, askup, ChatGPT, Traw.ai, 클로바x를 활용한 다양한 서비스들에 대해 소개해 주셨습니다.Duet AI가 구글 워크스페이스에 서비스된다고 하는데, 글쓰기, 슬라이드 그림 그리기, 내용 구성(데이터를 분석), 비디오와 사운드 대화에서 소통 연결같은 역할에 활용할 수 있다고 소개해주셨습니다. (구글 슬라이드나 문서에서 활용되면 MS 코파일럿같은 역할을 할까요??)",
        "url": "/activity-devocean16"
    }
    ,
    
    "activity-devocean15": {
        "title": "다시듣는 Tech 세미나 지식그래프 알아보기",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계지식그래프 알아보기https://devocean.sk.com/vlog/view.do?id=350&amp;vcode=A03발표 순서는 다음과 같았습니다.1부 지식그래프  지식표현  지식그래프 소개  지식그래프는 어떻게 구성되는가?2부 지식그래프와 정보 교환  데이터와 지식  우리는 같은 것을 이야기하는가?  지식그래프를 통한 정보 교환1부 지식그래프정보를 잘 표현하기 위해서는 내가 표현한 의미를 다른 사람들에게 의미가 올바르게 전달되는가?에 대한 논의가 필요하다고 하였습니다. 즉, 어떻게 구조화를 시키고 어떻게 중복되는 데이터를 잘 통합할 수 있을까?에 대한 고민이 필요하다고 하셨습니다. 그런 하나의 방안으로 지식그래프가 활용된다고 하였습니다.지식 그래프 : 지식을 표현하는 데이터로 그래프 형태를 가진다.지식그래프는 하나의 그래프 안에 엑셀, csv 형태 등 다양한 형태가 저장되고, 이를 설명하기 위한 information model, 사람과 기계의 접근을 뜻하는 Access, 찾기위한 Find, 그래프 형태 분석의 Network Analysis 같은 특징이 있다고 합니다.구성 요소는 구조와 형태에 활용되는 의미망과 프레임, 정보 전달과 의미를 제공하는 체계화와 개념화로 구성됩니다. 이를 위해 온톨로지를 통해 만들어나간다고 합니다.의미망은 개체나 개념을 의미하는 노드와 노드 간의 관계를 보여주는 링크로 구성됩니다.프레임은 Network의 노드 대신에 프레임을 활용하여 대상의 특성 값을 저장하는 여러 slot을 구성합니다.구성하는 기술로는 시맨틱웹을 통해 만들어진 표준을 활용한다고 합니다.구축하기 위해서 모델러가 다양한 데이터를 분석하여 모델링한 뒤, 엔지니어가 조회하여 TopBraid Composer을 통해 변환 규칙을 만들어 트리플 변환 엔진(RDFizer)에 매핑하여 변환한 뒤 트리플 저저장소에 저장한다고 합니다.지식그래프와 정보 교환지식이란 정보를 체계화하고 개념화 한 것 입니다.이를 공통적인 내용을 가지고 공유하기 위해 URI처럼 축약 표현을 사용하고, RDF를 통해 명칭과 표기를 통해 표현을 합니다.SKOS를 통해 타입같은 개념용어를 지정하는 어휘 체계를 표현합니다.SPARQL을 통해 CRUD를 진행합니다.결론지식그래프라는 단어는 들어보고 어떻게 활용하는지는 접해봤지만 데보션영 워크샵을 통해 지식그래프를 실제 활용하려는 팀의 프로젝트를 보고 한번쯤 어떤 툴을 활용해서 적용할 수 있는지 궁금했습니다. 그래서 마침 예전 세미나가 있길래 들어봤는데 굉장히 쉽고 예시를 통해 설명해주셔서 전반적인 내용에 대해 정리할 수 있었던 좋은 내용이었던 것 같습니다.",
        "url": "/activity-devocean15"
    }
    ,
    
    "activity-devocean14": {
        "title": "다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계AI Agent 기반 문제 정의 방법과 해결 방안 모색https://devocean.sk.com/vlog/view.do?id=305&amp;vcode=A03발표는 다음과 같은 순서로 진행되었습니다.  정의          Agent ⊂ AI        AI 배경 지식 (이론)          Symbolic AI vs. Machine Learning      BDI (Belief Desire Intention) 이론      BDI vs. FSM (Finite State Machine)        PRS (Procedure Reasoing System) / IRS (Inteligent Reasoning System)          Examples        문제를 해결하는 방법          Agent-based Modeling vs. Equation-based Modeling      AI Agent의 정의는 굉장히 광범위하지만 학문적으로 어떻게 정의되고 응용하고 있는지를 다뤄주셨습니다.정의AI는 기계가 생각할 수 있는가?로 시작하여 생각하는 과정을 어떻게 모방할 수 있는가?를 고민하는 분야가 있다고 합니다.  Agent는 사용자를 대신하여 원하는 작업을 자동으로 해결하여 주는 시스템을 의미합니다. 즉 유기적으로 연관되어 있는 시스템을 의미하는데 광범위하기 때문에 자연어 처리나 지식표현같은 인지적 측면, 전문가 시스템, 분산 협동 같은 다양한 범위로 확장이 되고 있습니다. 그래서 Agent는 AI와 시스템의 결합이라고 하셨습니다.AI 배경지식보통 AI가 머신러닝을 포함하고 딥러닝이 그 안에 속한다고 배웠지만, 머신러닝은 데이터를 다루고 표현하는 분야, 딥러닝은 multi layer을 통해 더 많은 특징을 표현하지만 AI는 미생물 시스템을 통한 규칙 기반 intelligence입니다.즉 조건과 명령의 조합으로 Rule을 만들지만 Heuristic Intelligence 분야도 있다고 하였습니다.그래서 여기서 지식에서 찾는 방법을 의미하고 여기서 Knowledge는 Symbolic AI에서 연구되고 있습니다.Symbolic AI는 Rule Engine이 핵심이고 대표적으로 Expert System이, Knowledge Graph를 통해 Symbol간의 관계를 컴퓨터에 매핑합니다.하지만 Symbol 자체의 의미가 없으며 Monotonic한 특성을 가지고 있습니다.Agent의 특징으로 자율성, 지능, 협동성, 사교성이 있다고 합니다.사용자의 간섭이 없이 스스로 판단하고, 사용자의 의도를 파악하여 계획을 세우고 지식을 터득합니다. 공통의 목적을 달성하기 위한 프로토콜이 있어야하며, 통신을 통해 도움을 받을 수 있어야합니다.여기서 확장되어 반응성, 의무, 의지, 목표, 지식을 가진 것을 BDI 에이전트의 특징입니다.BDI Agent는 에이전트가 가진 가질 수 있는 정보를 의미하는 Belief, 도달하고자하는 상태인 Desire, 행위인 Intention, 트리거인 Events가 있습니다. 이를 통해 방대한 Agent 범위를 개발할 수 있는 구체화를 하는 단계로 만들어졌습니다.그렇다면 FSM과의 차이는 FSM은 시스템 관점에서 결과를 제공하지만, BDI는 개인 행동 관점에서 결과를 제공합니다. 즉 시뮬레이션에서 BDI Agent 활용이 더 정확한 결과를 제공합니다. 다만 연산 시간이나 메모리가 더 많이 사용된다고 합니다.FSM의 감성같이 인간의 행동을 표현하기 어려운 분야에 대해 BDI가 더 효과적이라고 합니다.PRS (Procedure Reasoing System) / IRS (Inteligent Reasoning System)BDI를 구동하기 위해 절차적으로 진행될 수 있고 이는 병렬적으로 구현하기 어렵다고 합니다. 여기서는 목표 달성을 위한 에이전트의 의사 결정 과정을 설명합니다. 실행 순서와 조건, 사용한 데이터가 됩니다.문제를 해결하는 방법Agent-based Modeling vs. Equation-based Modeling는 결국 접근 방법의 차이라고 하였습니다. Agent는 행위에 대해 관찰하게 되고 Equation은 개체 수 변화, 성공률 등 관계 중심으로 관찰하게 됩니다.즉 Symbol의 의미를 가지지 않고 지식은 Monotonic하고, 절차 지향성이라는 문제가 있다고 합니다. 다만 학습 단계에서 Heuristic한 관점들이 들어가고 이런 것들을 극복하기 위해 Symbolic AI는 가치가 있다고 합니다. Symbol에 의미를 부여하기 위해 Ontology라는 형식으로 극복하려는 노력도 있습니다. 이를 통해 버전을 관리하고 이는 Monotonic을 극복하려고 합니다.다만 Equation 모델은 수식을 통해 증명이 되지만 Agent는 모델러의 역량과 경험에서 오는 것을 많이 필요로 한다는 단점이 있습니다.결론항상 수식으로만 보던 AI 모델 외에 Agent라는 새로운 분야에 대해서 들을 수 있는 재미있는 세미나였습니다. 여기서 약간 궁금한 점은 강화학습 분야의 Agent 또한 이런 형태의 Agent와 동일한 의미를 갖고 활용하는 용어인가? 라는 궁금증을 가지며 나중에 더 지식 그래프 같은 분야가 발전하게 된다면 어떤 AI 모델이 나올지 궁금하다는 생각이 들었습니다.",
        "url": "/activity-devocean14"
    }
    ,
    
    "activity-devocean13": {
        "title": "DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계DEVOCEAN YOUNG 8월 전용 밋업 후기이번 8월 전용 밋업은 현직자 분들의 대학생 때 알아두면 좋을 것들에 대한 내용을 담아서 진행하였습니다.  요즘 PM의 효과적인 프로젝트 관리  지극히 개인적인 취업준비 &amp; 신입 개발자로 살아남기  폐쇄망을 여행하는 히치차이커를 위한 안내서  지금 알고 있는 걸 그때도 알았더라면네가지 주제로 진행되었습니다.지난번은 취업, DATA였다면 이번엔 PM, 취업, 폐쇄망, 등 다양한 주제를 다뤄주셨습니다.특히 PM에서 사용했으면 하는 툴, 1년 6개월간의 취업 기간, 폐쇄망에서 왜 작업을 해야하나? 같은 주제를 소개해주셔서 재미있게 들었습니다..!!요즘 PM의 효과적인 프로젝트 관리프로젝트 성과를 높이기 위한 전략은 다음 요인이 중요하다고 하셨습니다.  핵심 목표에 집중하자  과제별 일정은 중요하다  검증하고 확인하자  배웠는지 생각하자계획 설정 중심의 프로젝트는 주간 보고는 계획 중심으로, 목표 세분화, 시스템 경영 단계가 있다고 하셨습니다.가장 중요한 것은 목표와 계획 수립, 커뮤니케이션, 협업 세가지이며 도구로는 Confluence, Jira, Gitlab을 추천하고 각 도구에 대한 활용 방법을 소개해주었습니다.앞으로 진로를 위해 다양한 협업 도구를 학습해봐야겠다고 느꼈습니다. 아무래도 PM이라는 직무를 직접적으로 경험해보지 못해서 프로젝트에 대해 두루뭉실한 느낌을 받았었는데 이번 기회에 실제 현업에서 사용하는 내용에 대해 들을 수 있었던 점이 좋았습니다.지극히 개인적인 취업준비 &amp; 신입 개발자로 살아남기4학년으로 가장 궁금했던 세미나이기도 했습니다. 코딩테스트, 기술 면접, 그리고 신입 개발자 마인드셋에 대해 들을 수 있었습니다. 취준에 조금은 길잡이가 될 수 있는 좋은 세미나였다고 생각합니다.폐쇄망을 여행하는 히치차이커를 위한 안내서폐쇄망은 정말 답답하다는 것을 공공기관 인턴을 진행하면서 느꼈고, 은행에서 일을 하면서도 느끼게 되었지만 법적으로 어쩔 수 없이 진행하는 것에 대해 알고있었습니다. 그래서 해당 세미나에서는 시스템 이해도를 높이고 참고서를 만드는 것이 중요하다고 하였습니다. 타 회사에서는 폐쇄망 환경에서 배포 시스템을 만드는 식으로 해결하기도 한다고 하였습니다. 폐쇄망에 취직하게 되면 꼭 수행해야할 점이라고 생각이 들었습니다!지금 알고 있는 걸 그때도 알았더라면마지막은 취업을 준비하면서 멘탈 관리, 취업 후 마인드셋에 대해 이야기해주셨습니다. 서탈은 항상 멘탈이 갈리게되는 주요 요인이라고 생각했지만.. 잘 이겨낼 수 있는 팁?을 알려주셔서 잘 와닿았던 것 같습니다.",
        "url": "/activity-devocean13"
    }
    ,
    
    "activity-devocean12": {
        "title": "SKT AI 서비스 기획 CAMP",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계SKT에서 AI 서비스기획을 배우고 싶은 사람을 모집합니다!(~8/9)나만의 아이디어를 세상에 구현하고 싶은,꿈 많은 예비기획자를 위한 프로그램을 소개합니다!매일매일 새로운 기술들이 쏟아지는 AI를 이용해서 나만의 서비스를 기획하고 싶은 사람!멘토링 없는 공모전보다 진짜 현직자들의 멘토링을 받고 싶은 사람!다양한 분야에 인공지능을 실제로 활용하는 현직자들을 만나고 싶은 사람!기획자로써 취직하는데 네트워킹이 하고 싶은 사람!기획이 하고싶은데 AI를 몰라 어떻게 기획할지 모르겠다면 지원하세요!!✅ CAMP 내용Mission 수행하기 (사전과제) : 나만의 기획 아이디어를 펼쳐보기Junior 기획자와의 대화 : 서비스 기획자 선배들에게 진로, 취업, 역량향상 꿀팁 얻기현직 서비스 담당자의 멘토링 : 어디에서도 들을 수 없는, 생생한 현업의 기획과정을 들여다보기!✅ 일정참가 지원 : ~8.9(수)참가자 발표 : 8.14(화)CAMP 시행 : 8.23(수)✅ 홈페이지에서 확인하기 : https://rb.gy/fee2h#서비스기획 #기획자 SKT #SKT텔레콤 #에이닷 #NUGU하고싶지만 인턴십 마지막 날이라 지원하지 못하는 것은 너무 슬프다ㅠ요즘 인턴십을 하면서 생각보다 기획이란 직무가 재밌다고 생각이 많이 들었는데..LLM을 공부하면서 어쩌면 코딩보다 나는 기획이 더 잘맞을 수도 있다는 생각이 들었다.특히 SKT는 LLM VISION 등 여러 분야에서 실제 서비스에 접목시키고 있으니..!!더 기대가 됐지만 아쉽게 수료식이라..ㅠ다른 많은 사람들이 할수 있었으면 좋겠다..!!",
        "url": "/activity-devocean12"
    }
    ,
    
    "activity-devocean11": {
        "title": "DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계다가오는 Automated AI 시대, 그 기반 기술과 적용사례7월 테크 세미나 주제는 다가오는 Automated AI 시대, 그 기반 기술과 적용사례이번 테크 세미나 주제는 오랜만에..!!제가 가장 관심있어하는 AI와 관련된 주제가 나왔습니다. ㅎㅎ특히 Auto ML과 XAI에 관한 이야기로 진행되어 너무 재미있고 유익한 시간이었습니다.https://devocean.sk.com/vlog/view.do?id=425&amp;vcode=A03순서는  Hyper-parameter Optimization  XAI  Use-Case순으로 진행되었습니다.Hyper-parameter Optimization먼저 Auto ML의 Coverage는 점차 넓어지고 있으며 현재는 데이터 이해부터 모델 평가까지 비즈니스 이해를 제외하고 굉장히 넓게 활용되고 있다고 합니다.데이터의 Hyper Parameter을 정하는 것은 작업자가 직접 세팅해줘야하는 학습 데이터에 의해 변경되지 않는 값인데 이를 최적화시키는 것을 탐색해주는 역할을 합니다.조합, Grid Search, Random Search는 매우 많은 숫자를 탐색해야하지만 적절한 탐색 범위를 제한해야하거나 느리고 최적의 값이 아닐 수 있다는 단점이 있습니다.그렇기 때문에 이런 최적의 Hyper Parmaeter을 찾는 방법으로 Successive Halving Algorithm을 소개해주셨습니다.SHA는 미리 Early Stopping을 수행하여 절반으로 줄여가며 평가를 진행하는 방법입니다.아직 한번도 Auto ML을 활용해본 경험이 없어서 다음 프로젝트에서는 한번 사용해보고 싶습니다. ㅎㅎXAIXAI는 데이터를 활용하면 항상 궁금증이 생길 수 밖에 없는 부분이라고 생각합니다.Black-Box Model을 해석하고, 동일한 기준으로 알고리즘을 해석하기 위해 연구되고 있습니다.예를 들어, 특정 Feature을 섞으면 모델 성과가 어떻게 바뀔지(Permutation Importance), 현재 모델의 예측력을 활용하면서 다른 모델의 설명력을 활용하는 방법(Surrogate Model)에 대해 알아보는 Concept에 대해 설명해주셨습니다.대표적인 방법으로 LIME과 SHAP에 대해 소개해주셨으며 제가 사용해본 방법론 두가지의 이론적 배경을 다시 복습할 수 있는 좋은 기회였다고 생각합니다.그 외에도, PDP(Partial Dependence Plt)이라는 다른 Feature을 고정하고 관심 Feature을 구간별로 변화시키면서 Target의 변화를 관찰하는 방법론에 대해 알아보았습니다.Use-CaseAuto ML은 커버리지를 상당히 늘릴 수 있으며 최소 구축 비용으로 신뢰할 수 있는 결과를 가져올 수 있는 방법으로 많이 활용하고 있습니다.강점으로 최소의 Resource, 최대의 효과, 검증된 알고리즘, 신뢰도 높은 결과물, 과제 아이디어와 데이터만 준비하면 활용 가능하다는 장점이 있습니다.결론아무래도 질문이 Auto ML이 데이터 과학자를 대체할 수 있을까? 에 대한 질문이 많이 등장하였습니다.정답은 사실 모르겠지만 현업에서 바라보는 관점은 주니에 데이터 과학자들에게 영향을 줄 수 있지 않을까? 라는 대답을 해주셨습니다.Auto ML과 XAI를 따로 배우지 않고 공부를 딥하게 해보진 못했지만 이번 강의를 통해 어떤 개념으로 활용되고 있는지 알수 있었던 좋은 기회였습니다.",
        "url": "/activity-devocean11"
    }
    ,
    
    "activity-ossca2": {
        "title": "OSSCA 오픈소스 컨트리뷰션 아카데미 Huggingface 한글화팀 2주차 후기",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     OSSCA 오픈소스 컨트리뷰션 아카데미 Huggingface 한글화팀 발대식 및 1주차 후기     OSSCA 오픈소스 컨트리뷰션 아카데미 Huggingface 한글화팀 2주차 후기OSSCA 오픈소스 컨트리뷰션 아카데미 Huggingface 한글화팀 2주차 활동 후기본격적으로 2주차에 번역 업무를 맡아 진행하게 되었다. 솔직히 영어를 굉장히 못하는 입장에서 기술 문서를 번역하는 것은 꽤나 상당한 시간이 소비되는 일이었다. 안그래도 지난주 면접 등 다양한 일이 겹쳐 시간을 많이 할애할 수 없었는데 특히 표현적 어려움이 컸던 것 같다.나는 https://huggingface.co/docs/transformers/main/en/add_tensorflow_model 해당 문서 번역을 도전했는데 상당히 내용도 길고, git, pytorch, tensorflow에 대한 내용들이 꽤나 들어있었다. 예를 들어 decorated라는 표현을 어떤 식으로 해석을 해야할까? wheel은 어떤 식으로 표현을 해야할까? 두 단어 모두 꾸며진, 바퀴 식으로 해석은 사용자에게 올바른 의미를 전달하지 못한다는 것이 문제였다. 이러한 몇가지 단어들에 대한 고민을 하고, 표현에 대한 고민을 하다보니 재미있었고 문서를 자주 읽게 되어 좋은 내용을 습득할 수 있었다.다만, Git은 여전히 어려운 것 같다. PR만 4번 진행했는데 왜 미리보기 문서가 만들어지지 않을까…..라는 고민으로 거의 하루를 소비했다.마무리로 번역과 Git 두 작업을 통해 많은 것을 배울 수 있었던 1주일이었다.",
        "url": "/activity-ossca2"
    }
    ,
    
    "activity-ossca1": {
        "title": "OSSCA 오픈소스 컨트리뷰션 아카데미 Huggingface 한글화팀 발대식 및 1주차 후기",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     OSSCA 오픈소스 컨트리뷰션 아카데미 Huggingface 한글화팀 발대식 및 1주차 후기     OSSCA 오픈소스 컨트리뷰션 아카데미 Huggingface 한글화팀 2주차 후기OSSCA 오픈소스 컨트리뷰션 아카데미 Huggingface 한글화팀 합격 후기4학년 2학기도 마친 뒤 취업 준비를 하며 무엇을 하면 성장할 수 있을지 계속 고민하고 있었다. 면접, 서류, 자격증도 모두 준비해야하지만 준비 과정에서 재미가 없었고 때마침 오픈소스 컨트리뷰션을 할 수 있는 대외활동이 있다고해서 무작정 지원하게 되었다.OSSCA 공식 홈페이지 바로가기처음 데이터 분석했던 것이 리뷰 키워드 분석이었고, 워드 클라우드 시각화가 시각화의 처음이었다. 그래서 자연스럽게 NLP가 데이터 사이언스를 준비하면서도 눈에 들어오고 가장 재미있는 부분 중 하나였다. 그리고 Huggingface는 그러한 다양한 프로젝트에서 나의 개발시간을 단축시켜주는 좋은 수단이기도 했지만 어쩌면 Pytorch 능력을 그만큼 깎았던 것이 아닐까? 라는 생각도 드는 프레임워크였다. 하지만 오픈소스에서 Huggingface, Pytorch, Prompt Engineering이 눈에 들어왔으나 내가 재밌는 것을 하자!라는 마음에서 Huggingface에 지원하게 되었다.마감일에 겨우 작성해서 냈고, 취준을 하는 중이기 때문에 많은 시간을 할애가 불가능하다고 작성하였지만, 운이 좋게도 합격하게 되었다. 아마도 내가 좋아하는 것이고 나도 많은 도움을 받았기에 다음 Huggingface 사용자를 위한 지식 전달이라고 생각하면 꾸준히 참여할 수 있다는 생각이 들었다.발대식 후기7월 8일 발대식에 갔고 예전에 한국산림과학회 공동 학술대회를 진행했던 한국과학기술회관 강당에서 진행되었다. 약 20개 팀이 참가한 만큼 많은 사람들이 있었고 우리 팀에도 거의 20명 가까운 사람들이 모였던 것 같다. 간단하게 소개와 공간 대여를 통해 아이스 브레이킹을 진행하였으며 앞으로의 진행 계획에 대해 소개받을 수 있었다.Huggingface 한글화 -&gt; 미니 프로젝트 과정으로 다양한 NLP 외 프로젝트들도 구상중이었다고 들었다.1주차 후기1주차는 주로 설문 조사 후 일정 공유, Git을 이용하여 PR하기, Chat GPT를 이용한 번역으로 이루어졌다.나는 Tensorflow 모델로 변환하는 글을 맡아 번역을 했고 해당 문서는 Tensorflow lite로 GPT-2 Fine-Tuning 모델을 만들 때 몇번이고 읽었었던 문서였다. 이런 문서를 번역하는게 새삼 새롭게 느껴졌다.또한 매일 점심시간 퀴즈를 멘토님께서 주셨는데 새로운 내용을 알게된 것도 많았고 특히 용어를 다시금 정립할 수 있었던 것이 좋았다.역시 오픈소스는 열정있는 사람들과 함께한다는 점이 가장 큰 장점이라고 생각한다. 현재 2개의 오픈소스에 참여하고 있지만, 두 팀 모두 팀장님들이 특히 열정적이라 많은 것들을 배우고 있다. 특히 부족한 영어실력이 가장 눈에 띄지만 나도 조그만 기여를 한다는 점이 가장 좋았던 것 같다.GDSC 이후 오랜만에 모각코도 진행했다.",
        "url": "/activity-ossca1"
    }
    ,
    
    "activity-devocean10": {
        "title": "DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계DEVOCEAN YOUNG 7월 전용 밋업 후기이번 7월 전용 밋업은 현직자 분들의 대학생 때 알아두면 좋을 것들에 대한 내용을 담아서 진행하였다.  커리어 전환 스토리 (영업맨 왕대리는 어떻게 데보션 전문가가 되었을까?)  개발자의 성장과 오픈소스, 그리고 커뮤니티  컴공 후배들이 3~4학년 때 가장 많이 하는 질문 Top 10  생생한 취업 후기네가지 주제로 진행되었습니다.주로 데이터/AI 관련 질문도 많이 있어서 특히 관심이 가는 주제였습니다. 데보션 영 전용 밋업은 개발자를 꿈꾸는 대학생, 취준생들이 궁금할만한 점을 딱딱 준비해주신 것 같아서 정말 좋았습니다!!영업맨 왕대리는 어떻게 데보션 전문가가 되었을까?역시 갑자기 기회가 찾아왔을 때 잡기 위한 준비는 미리 해둬야한다고 요즘 많이 느끼고 있었습니다. 세미나를 통해 준비해야할 내용들에 대해 복기할 수 있었습니다!😊구체적으로는 다음과 같이 요약할 수 있었습니다.      관심좋아하는 일 찾기 / 관심분야 넓히기간접 경험하기매일 일기 쓰기        준비관심 분야를 찾았다면 ‘한 가지’를 정해서 꾸준히기회를 잘 활용하자        실행나의 준비 된 모습을 적극 Appeal 하자네트워킹  개발자의 성장과 오픈소스 그리고 커뮤니티오픈소스는 공부할 때 굉장히 많은 도움이 되는데 막상 참여하거나 운영할 때 맞닥뜨릴 문제점이나 가져야할 마음가짐을 듣게 되어 소중한 시간이었습니다. 😄  좋은 사람을 만날 수 있다.  혼자가 아닌 함께 성장컴공 후배들이 3~4학년 때 가장 많이 하는 질문 Top 10어딜가나 듣게되는 10개의 질문에 대해서 질문들의 핵심에 대해 이야기해주신 것 같습니다. 항상 고민하게 되는 점들에 대해 어느정도 가이드라인이 될 수 있는 시간이었습니다. (그리고 준비가 부족한 저에게 반성의 시간을..🥲)  자격증 - 정처기?  개발자에게 필요한 덕목 - 소통  코딩테스트와 프로젝트  코딩테스트 준비 방법  면접 준비 방법  포트폴리오 작성 팁  범용 기술 vs 최신 기술  개발자가 된 것에 대한 후회  스타트업 vs 대기업  어느 정도까지 할 줄 알아야 할까?생생한 취업 준비과정과 팁취업 준비 과정에 대해서 구체적으로 말씀해주셔서 좋았습니다. 저 또한 대학원 진학과 취업을 항상 고민하고 있었는데 많이 와닿는 내용이었고 공감이 되었습니다!🙂  대학원 준비  취업 준비  싸피 준비  인턴 준비  취업네분의 신입 개발자로써 마음가짐, 성장할 수 있는 방법, 준비하는 과정에서 정말 필요한 것들에 대해 듣게되어 곧 졸업하는 입장에서 너무 소중한 시간이었습니다.좋은 세미나 열어주신 데보션과 연사님들에게 정말 감사드립니다 :)",
        "url": "/activity-devocean10"
    }
    ,
    
    "activity-devocean9": {
        "title": "DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계웹 프론트엔드 성능 최적화 방법 및 적용 사례6월 테크 세미나 주제는 웹 프론트엔드 성능 최적화 방법 및 적용 사례에 관한 내용이었습니다.세션 1.  웹 프론트엔드 성능 최적화를 해야 하는 당위성  웹 성능 최적화를 수행하는 이유  웹 성능 최적화 방법  웹 성능 최적화 경험 및 팁 공유세션 2 : ifland 웹 프론트엔드 성능 최적화 적용 사례  ifland 및 ifland studio 소개  ifland studio 웹 성능 최적화 사례 공유  웹 성능 최적화 경험 및 팁 공유로 이루어진 세미나였습니다.https://devocean.sk.com/vlog/view.do?id=423&amp;vcode=A03세션 1. 웹 프론트엔드 성능 최적화를 해야 하는 당위성웹 프론트엔드 최적화는 사용자와 상호작용에 밀접한 관계를 가짐.Site speed가 0.1초 향상될 때 평균 주문 가치가 높아짐Largest Contentful Paint : 로딩 속도 측정First Input Delay : 상호작용성 측정Cumulative Layout Shift : 시각 안정성 측정INP : FID의 대체로 전반적인 사용자와 상호작용을 관찰하여 페이지 민감도를 평가하는 요소Core Web Vitals, Real User Monitoring, Page speed audit reports 등 다양한 Tool로 사용자 경험에 대한 평가를 진행할 수 있음LCP 리소스는 HTML 소스에서 찾을 수 있도록 하며, 우선순위를 갖고 TTFB를 최적화하기 위한 CDN을 사용하여 최적화가 가능CLS 최적화는 컨텐츠의 명시적 크기 설정, 레이아웃을 유발하는 CSS 사용 애니메이션 지양, bfcache에 대한 자격 요건 충족 등을 통해 가능FID는 긴 작업을 피하고, 불필요한 Javascript와 대규모 렌더링 업데이트를 피함으로써 최적화 가능세션 2 : ifland 웹 프론트엔드 성능 최적화 적용 사례ifland : Social Metaverse Service (공동 공간 ifsquare + 개인 공간 ifhome)서비스 진입점이 가장 많이 사용하는 지역 -&gt; 최적화 대상Measure -&gt; Analyze -&gt; Optimize의 반복Measure : 최적화 적용 전 측정Render Blocking Resource, Script Loading option, Image 용량 등 다양한 문제-&gt; 모바일 웹에서도 문제 발생 여부 확인최적화 후 약 30% 이상의 감소 효과를 볼 수 있음마무리웹 프론트엔드 개발자는 성능 개선을 통해 서비스 향상과 비즈니스 향상에 기여할 수 있으며 측정 가능한 지표를 정의하고 사용자 중심 개선이 필요하다.항상 경량화를 염두해두고 개발 코드를 작성하자.느낀점작년 고급 웹프로그래밍에서 프론트엔드와 백엔드를 경험할 때는 작동만 되게끔 코드를 작성하기 급했었는데 실제 현업에서는 이러한 고민들이 있다는 것을 알게 되었고 서비스에서 큰 영향을 미친다는 것을 배웠습니다.실제로 웹 프로젝트는 아니었지만, AI 모델 서빙 프로젝트를 진행할 때마다 모델 추론 시간과 프론트엔드에서의 출력 시간을 줄이기 위해 고민했던 기억이 떠올랐습니다. 당시에 그래서 저는 모델을 최대한 경량화 시키고자 했으며 백엔드에서는 모델을 미리 저장해두고 출력을 진행하는 방식으로 진행했었습니다. 하지만 어느 정도가 적당한거야? 라는 의문을 가졌었는데 LCP, FID, CLS 등 이런 값들을 통해 웹 퍼포먼스를 측정한다는 것이 가장 인상 깊었습니다.앞으로 프로젝트에서 개발 완성이 목표가 아닌 지속적인 개선을 위한 프로젝트도 진행하고 싶다는 생각이 들었습니다.",
        "url": "/activity-devocean9"
    }
    ,
    
    "license-adsp7": {
        "title": "ADSP 합격 후기",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP 합격 후기2021년 처음 데이터 분야를 공부 시작하고 두번의 ADSP를 불합격하였다.두번 모두 58점을 맞았는데 항상 1,2 과목을 턱걸이했던 것이 컸던 것 같다.핑계를 대자면 자격증 공부를 전혀 하지않았고 기본 지식으로 보았지만 기초 지식이 부족했던 것도 같다.그 이후 프로젝트는 많이 진행했지만 시험은 신청하지 않았다.하지만 졸업을 앞두고 이제는 자격증이 있어야하지 않을까라는 생각에 신청했다.이번 시험을 신청하고 나서도 특별히 공부를 하진 않았고 예전에 사뒀던 책 1회독, 책에 실린 문제 1번씩 풀어보고 말았다.그래서 합격에 대해 불안감이 있었지만 88점으로 합격하였다.이 책의 2021 버전을 통해 공부하였는데 왕복 통학시간에도 공부하게 앱이 제공되는 점이 가장 좋았던 것 같다.드디어 첫 자격증을 땄으나 아직 더 많이 준비할 것들이 많아서 더 공부를 많이 해야겠다.",
        "url": "/license-adsp7"
    }
    ,
    
    "activity-devocean8": {
        "title": "SKT AI Campus 본사 방문 투어 후기",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계SKT AI Campus 본사 방문 투어 후기6월 16일 금요일에 SKT AI Campus 본사 방문투어에 참여하였다.데보션 영을 하면서 SKT 본사는 5번정도 찾아갔지만 T.um 견학과 현직자와의 대화를 위해 신청하였다.4월쯤 SKT AI Fellowship 신청을 위해 홈페이지를 방문했다가 6월달로 신청했는데 데보션 영에서도 참여할 사람을 모집해서 데보션 영들과 함께 갔다올 수 있었다.신청 링크https://www.sktuniv.com/a5d1558d-c2a2-455e-8bd6-237bf059a21e매달 약 20명을 모집한다.2시간 반동안 진행되며 30분간의 SKT 소개, 60분간의 T.um 체험, 1시간 반 정도의 현직자와의 대화를 진행하였다.SKT 소개SKT 소개는 현재 SKT 사업들 (자랑)을 소개하고 어떤 식으로 AI를 이용하고 있는지 소개하는 시간이었다.그리고 SKT의 업무 문화에 대해서도 이야기를 들었는데 복지로 유명한만큼 만족스러운 복지가 운영되고 있는 것 같았다.현재 SKT 10년째 사용하는 고객으로 퀴즈도 맞춰서 상품도 받았다.아무래도 여러 기술적 분야로 확장해가고 있는 부분이 많아서 흥미롭게 들을 수 있는 이야기였다.T.um 체험SKT가 꿈꾸는 미래 기술에 대해 소개하고 체험하는 프로그램이었다.이동통신 기술을 통해 구현될 수 있는 내용들을 가지고 만든 여러 미래기술이었는데 재미있었다.특히 VR처럼 가상 환경에서 손 움직임을 인식하고 그러는 프로그램은 재미있었다.현직자와의 대화마지막으론 SKT 현직자와 이야기 진행하였다.두분이 나오셨고 한분은 추천 시스템, 한분은 비즈니스 데이터 사이언티스트 쪽 업무를 맡고 계셨다.두분 모두 내가 전공하고 있는 산업공학과 출신으로 학부생 신분으로 취직에 성공하셨다고 했다.그래서 뭔가 공감되는 부분도 많았지만 당시 취업 시장과 현재 취업 시장이 데이터 분야에서 더 세분화되어서 달라진 부분도 많았던 것 같다.하지만 공부하는 방법, 특히 프로젝트에서 중요한 점이 인상깊었고 현업에서는 어떻게 업무를 수행하는지 알수 있었기 때문에 매우 유익한 시간이었다.마무리거의 5시 다 되어서 끝났지만 재밌고 유익한 시간들이 많이 있었다.취업을 준비하면서 느끼는 것들에 대해 현직자들의 생각을 알아볼 수 있었고 데이터 사이언티스트가 되고나서 알아야할 것들도 배울 수 있어서 좋은 시간이었다.",
        "url": "/activity-devocean8"
    }
    ,
    
    "activity-devocean7": {
        "title": "DEVOCEAN YOUNG 2기 5월 활동 후기",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계DEVOCEAN YOUNG 5월 후기5월 미션은  DEVOCEAN 소개 영상 만들기  DEVOCEAN 소개 영상 만들기2  개발관련 일정 로드맵 및 R&amp;R 짜기  클라우드 비용 최적화 세미나로 진행되었다.1, 2주차 DEVOCEAN 소개 영상 만들기는 말그대로 대학생 감성으로 소개 영상 만들기를 진행하였다.그래서 조원들과 만나서 광고를 패러디하여 영상을 제작하기로 하였다. 나는 SKT하면 떠올랐던 그리고 사람과 함께한다는 점이 비슷했던 사람_다시보기 광고를 따라 만들었다. 오글거리기도 했지만 재밌었고 다른 조들도 매우 고퀄리티의 영상을 제작한 것을 보고 놀랐다…3주차는 개발 일정 로드맵이었으나 우리는 역할이 이미 뚜렷하게 구분되어있어서 금방 끝낼 수 있었다. 아마도 많은 시간을 크롤링과 모델링에 쓰지않을까 생각이 들었다.4주차 세미나는 이미 적었으니 생략. 평소 클라우드로 DL을 학습하면 많은 비용이 나가길래 개발 환경에서는 어떻게 최적화 할 수 있을지 궁금했는데 재미있었던 세미나였다.https://keonju2.github.io/activity-devocean3그 외에도 2년동안 팔로우하고 있던 (참여는 바쁘다고 못했지만) 가짜연구소 컨퍼런스에 참석하였다. 예전에 게더 타운에서 호그와트 컨셉으로 했을 때 참여했는데 재미있게 들어서 나도 스터디하고 싶다는 생각을 많이 했지만 바쁘다는 핑계로 아직 못했다. 나도 나중에 참여해야지.. 세상엔 진짜 갓생 사는 사람들이 많고 공유를 좋아하는 사람들이 많다는 생각을 했다. 나와 비슷한 관심 주제를 갖는 사람들을 통해 나도 다시 마음가짐을 할 수 있었던 좋은 컨퍼런스였던 것 같다.https://devocean.sk.com/search/techBoardDetail.do?ID=164944그래서 이번에는 T Tower을 3번이나 방문하게 되었는데 매우 쾌적하고 좋았다… 스터디룸도 지원해주시고 맛있는 것도 사주는 DEVOCEAN YOUNG이 최고다.",
        "url": "/activity-devocean7"
    }
    ,
    
    "activity-devocean6": {
        "title": "DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화DEVOCEAN 5월 Tech 세미나로 클라우드 비용 최적화에 관련된 주제를 들었습니다.1번 세션은 TANGO 비용최적화 운영 EC2/RDS Auto Stop/Start,2번 세션은 [Devocean 세미나] 클라우드 비용 최적화 - EKS 비용 최적화를 주제로 진행하였습니다.https://devocean.sk.com/vlog/view.do?id=422&amp;vcode=A03TANGO 비용최적화 운영 EC2/RDS Auto Stop/Start  TANGO on AWS  Cloud 비용 최적화 사례, Off time 최적화          EC2, RDS는 되고      MSK, DMS는 안되고        Cloud 비용 최적화 사례, 그 외  Cloud 운영 방식 – 자동화/IaC 그리고 시각화TANGO on AWSTANGO는 (T-Advanced Network Generation OSS) SKT 망 관리를 지원하는 통합 시스템으로 장비 관리, 감시, 분석 망 설계 구축 기능이 있다.Dev account, Stg account, Prd account 목적으로 AWS 계정을 구분하여 활용하고 미사용 시간대에 리소스 최적화를 중점으로 고민하였다고 하였다.Cloud 비용 최적화 사례, Off time 최적화앞서 말했든 Dev/Stg AWS 계정은 Off time이 존재하고 EC2, RDS 리소스 Stop/Start를 적용한다.몇가지 주의사항을 두고 자체 Lambda를 구성하였다고 하였다.Stop 지원이 안되는 리소스는 Modifiy로 수동 관리한다고 하였다.Cloud 비용 최적화 사례, 그 외비용 절감 효과의 차이는 있었지만 유효했던 비용 최적화 추가 사례에 대해서 소개해주셨다.▪ MSK 리소스 최적화▪ EC2, EKS, RDS Graviton 전환▪ RDS 설정 최적화▪ S3 스토리지 디렉토리 구조 단순화▪ Datadog에서 Cloudwatch 수집 주기 조정다음 사례들을 통해 비용 최적화에 효과가 있었다고 합니다.Cloud 운영 방식자동화/IaC 그리고 시각화에서는 Cloud의 모든 리소스는 Code 기반 관리라고 하였고, 자동화 관리를 통해 이를 시각화 하는 노력이 필요하다고 하였습니다.클라우드 비용 최적화 - EKS 비용 최적화위 세션은 개념 설명 느낌이었다면, 두번째 세션은 조금 더 실습 위주였다.EKS 비용 최적화를 위해  불필요한  EKS 리소스를 최소화 하기  EKS 가성비 향상시키기  지속 가능한 EKS 비용 최적화 환경 구성하기를 주제로 소개해 주셨다.불필요한 EKS 리소스 최소화 하기Auto Scaling (Node, Pod)EKS Auto Stop &amp; Start (kube-downscaler)Right Sizing (Node, Pod, Disk)미사용 리소스 제거EKS Auto Stop &amp; Start불필요한 시간대 개발/검증 환경 EKS 리소스 최소화새벽 시간대 (22:00~07:00) EKS Pod 를 0로 만들기 → kube-downscalerkube-downscaler원하는 시간대에 Kubernetes Workload Scale down 및 pause 할 수 있음Deployment, Statefulsets, HPA, CronJob 등 지원비용절감은 Node Scale-in 에 의해서 발생하므로, 사전에 Node Auto Scaling 구성이 필수Helm을 통한 설치도입하기 위해 여러 고려사항이 필요하다.EKS 가성비 향상시키기지속적으로 업데이트 하기 : Docker -&gt; Containerd가격정책 활용하기 : RI(Reserved Instance) 과 Saving Plan가성비 좋은 Instance Type 사용하기 : Spot Instance, Burst Instance Type지속 가능한 EKS 비용 최적화 환경 구성하기 (kubecost)Application 기능 변경H/W(Instance Type, Disk 등) 변경오픈소스, 클러스터 업데이트데이터 변경후기요즘 개발 동아리를 하면 항상 Devops에 관한 이야기가 나오고 클라우드에 관심이 있는 주변 사람들도 많이 생겼습니다. 그래서 저도 계속 관심이 가고 AI 개발과 서빙에도 클라우드를 사용하기 위해 공부를 하고 있습니다.많은 내용을 이해하지는 못했지만 앞세션에서 실제 클라우드를 어떻게 비용 최적화를 하는지 대략적으로 알수 있어서 좋았습니다..!! 내용적으로 TANGO가 무엇인지 비용 최적화 사례와 OFF TIME에는 어떻게 운영하는지 배울 수 있었습니다. 주로 시간에 따라 스케쥴을 정하고(워라벨이 좋다면 정하기가 쉽다) 자동화를 하지만 Slack같은 툴을 이용하여 가시성을 확보하셨다고 하였습니다.두번째에서는 실제로 적용 효과를 보여주시면서 설명해주시고 다양한 솔루션을 알수 있어서 좋았습니다..!! 앱 변경없이 인프라만으로 개선 사항할 수 있다는 점에 대해서 배울 수 있었습니다. 사전질문에서 첫번째 질문도 많이 와닿아서 좋았으며 학생 입장에서의 질문들과 현업에서 여러 선택의 트레이드 오프가 있는 것 같아서 흥미로운 질문도 많았던 것 같습니다.Devocean 세미나는 현직자를 대상으로 하기에 항상 조금 어려운 감이 있지만 현업에서 어떻게 해결하는지 궁금증을 해결할 수 있는 좋은 세미나라고 생각합니다.",
        "url": "/activity-devocean6"
    }
    ,
    
    "activity-devocean5": {
        "title": "DEVOCEAN YOUNG 2기 4월 활동 후기",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계DEVOCEAN YOUNG 4월 후기4월 미션은  과제 수행기 인사이드에 작성  조별 스터디 도서 선정  조별 개발 주제 선정  ChatGPT로 인한 새로운 패러다임 세미나로 진행되었다.1주차 미션은 환경을 위한 인공지능 수업에서 진행한 USDM, NLDAS 데이터를 이용한 가뭄 예측으로 Unet 알고리즘에 대해서 소개하였다. 당시 수업시간에는 Baseline 코드에서 적은 데이터에 대해 예측을 잘하지 못하는 모습을 보였고 그래서 직접 Loss Function을 수정하여 개선하고자 실험해보았다. Weight Categorical Cross Entropy, Focal Loss 등 알고리즘을 작성하고 Log Scale을 취해 가중치를 조정하는 등 여러 시도를 해본 결과 대체로 적은 데이터셋에서 개선된 모습을 보였지만 너무 False positive인 결과가 많이 나오는 모습을 보였다. 하지만 그냥 구현된 Loss Function외에 다른 방법을 적용해본 것 자체로 많은 도움이 됐던 것 같다.https://devocean.sk.com/internal/board/viewArticleForAjax.do?id=1648452주차 미션은 도서 선정이었는데 도서 선정 후 책은 지원금으로 제공해주셨다. 우리는 클린 코더를 선택했고 클린 코더, 클린 코드, 클린 아키텍쳐 등 여러 클린 시리즈의 시작점을 읽을 수 있는 기회라고 생각되어 선택하게 되었다. 그 외에도 여러 책들을 소개해주었는데 저장해두었다가 나중에 하나씩 읽어보려고 한다.3주차 미션은 조별 개발 주제 선정이었고 우리는 DEVOCEAN 홈페이지에 Stackoverflow 기능을 추가하는 것을 목표로 하였다. 구현된 프레임워크를 몰라 아직 구체적인 계획은 세우지 않았지만, 키워드 추출, 검색기능 개선 등 Task를 맡을 수 있을 것이란 생각이 들었다.4주차 미션은 세미나 후기 작성이었고 이는 이전 글을 통해 작성하였다.https://keonju2.github.io/activity-devocean4그 외에도 Kubernates 테크 데이, 스프링캠프 2023도 있었지만 나는 일정과 기술스택이 안맞기 때문에 다른 사람들이 참여해야한다고 생각하여 참여하지 않았다. 그리고 데보션 전문가들과 스터디도 할 수 있다고 하셨는데 해야지해야지하다가 참여하지 못했지만 어떤 목표가 생기면 내가 개설해서 해보고싶다.또한 AI Factory에서 진행한 LangChain 세미나 후기, ChatGPT를 이용한 데이터 분석 도메인 지식 보충하기 등 여러 글을 작성해보았다.AIfactory LangChain 세미나 후기 및 소개 - https://skdevocean.page.link/A5svLKyJna2BRiwM8chat GPT로 데이터 분석 과제 도메인 지식 보충하기 - https://skdevocean.page.link/Ei6xDhq3pqF6meJUA",
        "url": "/activity-devocean5"
    }
    ,
    
    "activity-devocean4": {
        "title": "DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계DEVOCEAN 4월 Tech 세미나https://devocean.sk.com/vlog/view.do?id=419&amp;vcode=A03DEVOCEAN APP 후기ChatGPT로 인한 새로운 패러다임이번 세미나는 예비군 훈련으로 인해 본방은 사수하지 못했지만 대세인 ChatGPT라길래 다시보기를 통해 복습하였다.Udemy와 함께하고 첫번째 세션에서는 “OpenAI의 최신 발전과 현실적 활용 방안”을 주제로 콜트 스틸님을 두번째 세션에서는 “누구나 할 수 있는 ChatGPT를 활용한 데이터 사이언스”라는 주제로 토니님이 강연을 해주셨는데 다시보기는 영어로 제공되었어서 약간 천천히 들었다.세션은 다음과 같았고 실습도 많이 진행되서 ChatGPT를 이용한 다양한 활용방안에 대해 고민할 수 있었던 세미나였다.세션 1: GPT-4와 Chat-API 세계 탐구  Intro to OpenAI products and APIs  Comparing GPT 3, 3.5, and 4  Introducing the Chat API with GPT 4  Important API parameters - Tokens, Temperature, Frequency Penalty  Use Cases - Summarization, Information Extraction, Sentiment Analysis, Translation  Project Demos세션 2: 데이터 과학을 위한 ChatGPT  Intro into Generative AI and prompt engineering  How Gen AI can save dozens of hours a month on data science projects  How Gen AI can help those who don’t have experience in data science, execute projects in it  Data exploration and analysis  Collaboration and communication - Using ChatGPT in data visualization, Communicating insights and defending them, Project meetings preparation",
        "url": "/activity-devocean4"
    }
    ,
    
    "activity-devocean3": {
        "title": "DEVOCEAN YOUNG 2기 3월 활동 후기",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계DEVOCEAN YOUNG 3월 후기DEVOCEAN YOUNG 활동은 매주 미션과 세미나를 통해 진행된다.이번주는  조 소개 컨텐츠 제작  앱에 나만의 앱 소개하기  조별 앱 수정 아이디어 발표하기  Datalog 활용사례 세미나로 진행되었다.1주차 미션은 조 소개 컨텐츠 제작이었는데 조원들과 함께 인생네컷도 찍고 발표자료도 준비하면서 조를 소개하는 미션이었다. 정말 많은 능력자들과 갓생사는 사람들이 있다는 것을 다시 한번 느꼈다.https://devocean.sk.com/internal/board/viewArticleForAjax.do?id=1646632주차 미션은 나만의 앱 소개하기였고 나는 대학 시절 가장 많이 사용했던 앱인 삼성 노트를 소개하였다. 평소 아무 생각없이 사용했었는데 다른 사람한테 소개하는 것에서 이 기능 저 기능 더 사용하니까 개선사항도 보이고 몰랐던 기능들도 자세히 알게 되었다.https://skdevocean.page.link/a9wpCRWBd8Zs5KmW93주차 미션은 앱 수정 아이디어였고 발표를 들으니 비슷비슷한 것들이 많았다. 이런 개선사항을 고치는 것에 직접 참여할 수 있는 기회를 주는 것이 설레지만 한번도 해보지 않았던 일이기 때문에 걱정도 되었다.4주차 미션은 세미나 후기를 작성하는 것이었고 후기는 미리 작성해두었다. 강의도 다시 들을 수 있으니까 나중에 필요하면 다시 들어야겠다.https://keonju2.github.io/activity-devocean2후기이것저것 할게 많았던 3월이지만 부담스럽지 않고 재미있었던 3월이었다. 발대식 회식도 재미있었고 조별 컨텐츠도 비슷한 주제를 하고있는 사람들과 만날 수 있는 좋은 기회 같다!",
        "url": "/activity-devocean3"
    }
    ,
    
    "activity-devocean2": {
        "title": "DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정https://devocean.sk.com/vlog/view.do?id=417&amp;vcode=A03  DEVOCEAN 3월 Tech 세미나로는 Datadog이라는 End-to-End Full Stack 모니터링 솔루션을 소개하는 세미나를 들었다.Datadog의 세가지 핵심 가치는 Traces, Metrics, Logs이며 각각 서비스 전반에 걸친 원인, Trends/Pattern 인식, 이슈 및 장애 분석을 주 목적으로 사용된다고 하였다.즉 클라우드 모니터링 시스템을 안정적이고 효과적으로 구축 및 운영하도록 지원하는 서비스이다.1차는 Datadog에 대한 포괄적인 설명을 해주었으며 2차는 여러가지 활용 방안을 소개해주었다.특징으로는  실시간 검색은 100% Trace 표시  Github에 대한 소스 코드 통합 지원  프로덕션 환경에서 상시 코드 프로파일링  Request Level에서 코드 최적화  코드 성능을 위한 자동화된 인사이트등이 있었다.DataDog은 클라우드 모니터링을 하는 서비스이며 운영부터 개발까지 모두 다루며 로그 정보, 서비스 전반의 Trace, 패턴과 트랜드같은 Metrics 세가지를 핵심으로 삼는다는 것이 가장 중요하다고 하였습니다.그래서 앞 부분은 Datadog에 대한 전반적인 소개, 특징 장점들에 대해 언급하셧으며 뒷부분에서는 개발에 조금 더 가까운, Front-End와 Back-End에서 어떤 식으로 사용하는 지에 대한 설명이었습니다.클라우드 어플리케이션에서는  변화에 민감하여 다양한 특성들은 복원력과 확장력이 중요하다고 하셨습니다. 그래서 로그 분석의 이상 탐지나, 코드 최적화, API Test, 사용자 오류까지 넓은 분야를 하나의 어플리케이션에서 공유한다고 생각하였습니다.후기아직은 운영부터 개발까지 참여하여 모니터링을 직접적으로 사용해본 경험이 없어 있었다면 더 많은 내용을 이해할 수 있었을 것 같다는 생각이 들었습니다. AI 모델 개발을 위해 몇번의 GCP와 같은 서비스에서 로그를 보면서 중요성을 잘 못느꼈지만 현업에서는 이런 클라우드 어플리케이션을 통한 유지가 중요하다는 것을 느꼈습니다.하지만 발표자분께서 저같은 개발 어린이들도 이해할만큼 재밌고 쉽게 설명해주신것 같습니다. 하지만 질문 타임은 아무래도 현업과 관련된 질문들이 많아 어렵다고 생각하였습니다..많은 것을 느낄 수 있었던 테크 세미나였습니다 :)",
        "url": "/activity-devocean2"
    }
    ,
    
    "activity-devocean1": {
        "title": "DEVOCEAN YOUNG 2기 합격 및 발대식 후기",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     DEVOCEAN YOUNG 2기 합격 및 발대식 후기     DEVOCEAN 3월 Tech 세미나 - Datadog의 Front-End에서 Back-End까지의 여정     DEVOCEAN YOUNG 2기 3월 활동 후기     DEVOCEAN 4월 Tech 세미나 - ChatGPT로 인한 새로운 패러다임     DEVOCEAN YOUNG 2기 4월 활동 후기     DEVOCEAN 5월 Tech 세미나 - 클라우드 비용 최적화     DEVOCEAN YOUNG 2기 5월 활동 후기     SKT AI Campus 본사 방문 투어 후기      DEVOCEAN 6월 Tech 세미나 - 웹 프론트엔드 성능 최적화 방법 및 적용 사례     DEVOCEAN YOUNG 7월 전용 밋업 대학생 세미나 후기     DEVOCEAN 7월 Tech 세미나 - 다가오는 Automated AI 시대, 그 기반 기술과 적용사례     SKT AI 서비스 기획 CAMP     DEVOCEAN YOUNG 8월 전용 밋업 대학생 세미나 후기     다시듣는 Tech 세미나 AI Agent 기반 문제 정의 방법과 해결 방안 모색     다시듣는 Tech 세미나 지식그래프 알아보기     DEVOCEAN 8월 Tech 세미나 - 업무 생산성 향상을 위한 생성형 AI 사용     DEVOCEAN 9월 Tech 세미나 - In-Memory Data Grid 기반  Smart Factory 아키텍처링 연구 사례     SK 그룹에서 개발자 컨퍼런스 SK TECH SUMMIT을 개최합니다.     DEVOCEAN 10월 Tech 세미나 - 2023년의 딥러닝과 LLM 생태계DEVOCEAN YOUNG 2기 합격 및 발대식 후기이제 4학년 2학기가 된 후 대면 수업은 하나밖에 없어서 여러 대외활동들을 또 찾아보고 있었다.특히 기업이 진행하고 있는 대외활동을 무작정 다 지원하고 있었는데 SKT에서 운영하고 있는 개발 커뮤니티인 DEVOCEAN에 대학생 개발 서포터즈를 모집한다는 홍보가 있길래 지원하게 되었다.요즘 많은 것들에서 불합격이 나와 기분이 안좋았는데 합격 소식을 듣고 왜? 라는 생각이 먼저 들었다.방치된 블로그를 되살릴 수 있도록 좋은 기회로 만들고 싶다는 생각 또한 들었다.DEVOCEAN이란?DEVOCEAN 링크Developers’ Ocean : 개발자들을 위한 영감의 바다를 의미한다고 한다.SK 그룹의 대표 개발자 커뮤니티이며 내/외부 개발자 간 소통과 성장을 위한 플랫폼을 상징한다고 위 홈페이지에 소개되어있다.요즘 굉장히 많은 개발 커뮤니티가 생겨나고 있는데 DEVOCEAN은 기업에서 개발자들로부터 만들어져 외부로 확산해나가는 부분이 인상깊었다.나 또한 여러 동아리와 인공지능 커뮤니티를 팔로우하고 있지만 DEVOCEAN이 가진 차별점은 그런 부분이었던 것 같다.타 기업들의 개발 관련 행사들은 Meet Up과 같은 컨퍼런스 행사나 해당 기업들이 작성한 인사이트 게시글들을 메일로 제공하는 부분이지만 DEVOCEAN은 매일 새로운 게시글을 메일이나 어플 알림을 통해 제공하며, 전문가들에게 멘토링 신청도 가능하다는 점에서 특별한 매력으로 다가왔다.DEVOCEAN YOUNG 지원 계기원래 여러 개발 관련 기업들의 컨퍼런스나 인사이트 보고서 같은 자료들을 메일로 구독하고 있었다.나도 언젠가는 이런 기업들에 들어가서 저런 발표들을 하고 싶다라는 목표를 가지고 있기 때문에 최대한 많이 들으려고 하고 있었다.아직 학부생으로써 여러 동아리들을 하였지만 이런 큰 블로그에 게시글을 작성할 수 있다는 것만으로도 특별한 경험이 될 수 있다고 생각하여 지원하게 되었다.특히 NLP를 좋아하는 나로써 Ko-GPT2나 KoBERT같은 언어 모델들을 발표하고, 에이닷 같은 챗봇 어플 개발자들을 만날 수 있다는 생각에 지원한 것이 가장 컸다.해당 공고가 뜨기 전에 에이닷을 구경하면서 굉장히 매력적으로 느꼈기 때문이고, 나 또한 프로젝트로 Ko-GPT2를 이용한 챗봇 서비스를 구현중이었기 때문이다.또한 감사하게도 합격을 하게되어 방치된 블로그도 다시 작성해보려고 한다.진행했던 프로젝트들도 다시 정리하고 코딩테스트 문제 풀이도 다시 올려야겠다.DEVOCEAN YOUNG 발대식 및 앞으로 진행 방향발대식은 을지로에 위치한 SKT 타워에서 진행하였는데 역시 대기업 구경은 재밌다는 생각이 들었다.지난번 판교에 있는 하이닉스에 들어갔을 때도 느꼈지만 출입부터 까다롭다…약 1시간동안은 DEVOCEAN YOUNG 활동들에 대해서 들었고 1시간은 자기소개 하는 시간을 가졌다.진짜 많은 사람들이 GDSC와 멋사를 하는 것이 인상에 깊었다. 그 뒤 조편성을 진행하였고 우리는 프론트 1, 백엔드 1, AI 2의 진로를 희망하는 사람들의 구성을 가지고 한 조가 되었다.앞으로 마칠 때까지 해당 조별 혹은 개인별로 활동하며 매주 미션들이 주어진다.발대식을 마치고는 회식을 했는데 팀원들과 현직에 계시는 담당자 분들에게 평소 SKT에 대해서 궁금했던 점과 개발자로써 궁금했던 점들을 물어보느라 마지막까지 남아있었다.앞으로도 재밌는 이야기, 많은 행사들에 함께 참여할 수 있도록 열심히 활동하고 싶다.",
        "url": "/activity-devocean1"
    }
    ,
    
    "activity-first": {
        "title": "2022년 마이데이터 국민참여단 후기",
            "author": "keonju",
            "category": "",
            "content": "대외활동 게시글 목록     2022년 마이데이터 국민참여단     2023년 DEVOCEAN YOUNG 2기 합격 및 발대식 후기2022년 마이데이터 국민참여단 후기한국데이터산업진흥원에서 진행하는 마이데이터 국민참여단에 참여하였다.데이터청년캠퍼스 이후로 한국데이터산업진흥원의 여러 행사들을 팔로잉하고 있었는데 마이데이터를 이용하는 여러 사업과 관련 공부를 할 수 있다는 설명에 고민없이 신청하였다.서비스 체험 후기나는 아롬정보기술의 데이터잇이라는 앱을 사용해보는 기회를 얻었다.해당 앱은 공공, 의료, 문화 분야의 개인데이터를 활용하여 마이데이터를 종합관리 할 수 있는 서비스를 제공한다.주로 설문 데이터를 수집하고, 데이터를 판매할 수 있는 플랫폼이라고 볼 수 있다.화면 구성메인 페이지는 다음과 같다.보이는 것처럼 현재 만들 수 있는 데이터와 판매 모집이 메인인 앱이다.하지만 아쉽게도 내가 사용하는 기간동안 진행된 판매는 없었다.수집 데이터는 몇가지가 있어서 간단하게 체험해보았다.주로 설문조사가 있었고 설문조사의 길이는 별로 길지 않아 금방 참여할 수 있었다.설문은 연구자들이 연구하고자하는 내용에 관한 것이었고 길다면 약간 거부감이 있을 수 있지만 진행하기 쉽게 UI도 좋았고 손쉽게 할 수 있었다는 점이 매우 좋았다.눈에도 잘 띄고 어플이 느리지도 않았던 점이 설문조사의 귀찮음을 줄여줬던 것 같다.또한 구글 핏을 이용하여 걸음수도 데이터를 수집할 수 있었다.나는 항상 갤럭시 워치로 걸음수를 측정하는데, 이런 기록들도 마이데이터로써 가치를 띈다는 것이 신기했다.아직은 많은 데이터가 거래되진않아서 나의 데이터도 팔고, 포인트 전환도 하고 싶었지만 아직 포인트는 L point로만 전환할 수 있었다.아마도 마이데이터가 좀 더 활성화되고 현재 의료, 문화 분야의 연구자들이 해당 플랫폼을 사용하도록 홍보가 활성화된다면 더 많은 거래가 이루어지는 플랫폼이 될 수 있다고 생각이 들었다.어플의 UI가 어렵지않게 구성되어 처음 사용하는 사람도 빨리 익힐 수 있었으며 연구자 입장에서는 별다른 홍보 없이 좋은 설문 결과를 얻을 수 있다는 생각이 들었다.자주 들어가면서 다른 설문이나 데이터가 있나? 하고 들어가 보았지만 자주 등록이 되지않는 점은 아쉬운 점이었다.앞으로도 어플이 많이 활성화 된다면 지속적으로 사용하고 싶다.",
        "url": "/activity-first"
    }
    ,
    
    "study-book1": {
        "title": "책 서평 (1) &lt;br&gt; 텐초의 파이토치 딥러닝 특강",
            "author": "keonju",
            "category": "",
            "content": "서평 관련 글    텐초의 파이토치 딥러닝 특강  골든래빗 출판사로부터 책을 제공받아 작성했습니다.책 구매처http://www.yes24.com/Product/Goods/111102197일단 저는 딥러닝 학과 수업을 들은 학생의 관점으로 서평을 작성하였습니다.왜 신청했나요?대학교 수업을 들으면서 대부분 Tensorflow를 사용하여 강의를 진행하였다.하지만 현업과 논문에서 Pytorch가 추세라는 이야기를 많이 들어 Pytorch를 공부하고 싶었다.하지만 대부분 Pytorch 혹은 딥러닝 책은 너무 이론에 치우치거나, Pytorch에 대한 설명이 부족했다.예를 들면, Dataloader에 관한 내용 설명 없이 Dataloader을 사용한다거나, “init“이나 “forward”에 대한 설명 없이 모델을 만들곤한다.그래서 다양한 책을 찾아보다가 텐초의 파이토치 딥러닝 특강의 목차 구성이 인상깊어서 신청하게 되었다.이 책의 구성이 책은 코랩을 기본 개발환경으로 하고 실습을 진행한다.그 점이 다른 책과 다른 장점이라고 생각한다. 어떤 책들은 개발환경이 로컬이라 개발환경 설정에 많은 시간을 보내기 때문이다.또한 학생의 입장에서 그래픽카드를 보유하고 실습을 빠르게 진행하기에는 코랩 환경이 훨씬 합리적이라고 생각한다.딥러닝 입문하기딥러닝 입문하기에서는 학교에서 배웠던 기본 개념을 복습이 가능했다는 점과 파이토치의 특징, 그리고 딥러닝 문제 해결에 중요한 프로세스나 시각화를 간단히 다시 살펴볼 수 있었다.마찬가지로 경사하강법, 오차역전파와 같은 내용들은 앞에 배치하여 딥러닝을 접해봤던 사람의 입장으로써 실습에 더 집중할 수 있다는 생각이 들었다.본문이 책은 CNN, RNN부터 ResNet과 같은 알고리즘, 이미지 처리를 위한 U-Net과 오토인코더, 텍스트 처리를 위한 LSTM과 Attention, 그리고 GAN과 같은 내용이 담겨있었다.이러한 내용들은 딥러닝 입문 단계에서 배우는 퍼셉트론에서 한단계 진화한 내용들로 더 깊은 모델들을 배우기 위한 발판이 될 수 있는 내용들이라 생각되어 좋았다.부록좀 더 고난이도의 내용인 트랜스포머, GPT, BERT, VIT에 대한 내용이 나오며 초심자에게 어려울 수 있는 오차 역전파 가중치 업데이트 과정이나 개발 환경 구축을 부록에 뺀 내용은 이 책이 타겟으로 하고있는 독자층이 분명하다는 것을 알수 있었다.후기이 책은 분명히 초급자도 상급자도 타겟으로 하는 책이 아니다.딥러닝에 대해 한번쯤 인터넷에 있는 많은 기초 강의들 중 하나를 들었다면 거기서 부족했던 부분을 채울 수 있는 훌륭한 책이라고 생각된다.새로 나온 함수들에 대한 설명이 충분하며 각 코드들에 대한 설명이 구체적으로 나와있기 때문에 논리적 설명도 충분하다고 생각된다.물론 “이 책이 초급자에게 권할 수 있는 책이냐?” 한다면 자신이 딥러닝에 대한 공부 의지가 있다면 교과서로 쓸 수 있을 정도의 내용이라고 생각한다.공부하면서 코드를 사용한 이유에 대한 궁금증을 잘 해결하지 못하는 학생이라면 이 책을 추천한다.",
        "url": "/study-book1"
    }
    ,
    
    "programming-kaggle5": {
        "title": "캐글 (5) &lt;br&gt; 메타 데이터를 이용한 &lt;br&gt; 데이터 관찰 및 준비",
            "author": "keonju",
            "category": "",
            "content": "kaggle 관련 글    캐글 (1) Simple Matplotlib &amp; Visualization Tips 공부하기    캐글 (2) 타이타닉 튜토리얼 1,2 공부하기    캐글 (3) 타이타닉 생존자 EDA 부터 분류까지    캐글 (4) 타이타닉 생존자 앙상블과 스태킹까지    캐글 (5) 메타 데이터를 이용한 데이터 관찰 및 준비https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python캐글의 Porto Seguro’s Safe Driver Prediction 데이터를 통해 메타 데이터를 이용하여 데이터 분석 준비하는 방법을 필사했습니다.  Visual inspection of your data  Defining the metadata  Descriptive statistics  Handling imbalanced classes  Data quality checks  Exploratory data visualization  Feature engineering  Feature selection  Feature scaling## 패키지 설치import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.impute import SimpleImputerfrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.preprocessing import StandardScalerfrom sklearn.feature_selection import VarianceThresholdfrom sklearn.feature_selection import SelectFromModelfrom sklearn.utils import shufflefrom sklearn.ensemble import RandomForestClassifierpd.set_option('display.max_columns', 100)train = pd.read_csv('train.csv')test = pd.read_csv('test.csv')데이터 확인하기유사한 그룹에 속하는 feature은 태그가 지정됩니다. (ind,reg,car,calc)feature 이름에서 bin은 이진 특성을, cat은 범주형 특성입니다.그 외에는 연속 또는 순서형 특성입니다.-1은 결측값입니다.train.head()                  id      target      ps_ind_01      ps_ind_02_cat      ps_ind_03      ps_ind_04_cat      ps_ind_05_cat      ps_ind_06_bin      ps_ind_07_bin      ps_ind_08_bin      ps_ind_09_bin      ps_ind_10_bin      ps_ind_11_bin      ps_ind_12_bin      ps_ind_13_bin      ps_ind_14      ps_ind_15      ps_ind_16_bin      ps_ind_17_bin      ps_ind_18_bin      ps_reg_01      ps_reg_02      ps_reg_03      ps_car_01_cat      ps_car_02_cat      ps_car_03_cat      ps_car_04_cat      ps_car_05_cat      ps_car_06_cat      ps_car_07_cat      ps_car_08_cat      ps_car_09_cat      ps_car_10_cat      ps_car_11_cat      ps_car_11      ps_car_12      ps_car_13      ps_car_14      ps_car_15      ps_calc_01      ps_calc_02      ps_calc_03      ps_calc_04      ps_calc_05      ps_calc_06      ps_calc_07      ps_calc_08      ps_calc_09      ps_calc_10      ps_calc_11      ps_calc_12      ps_calc_13      ps_calc_14      ps_calc_15_bin      ps_calc_16_bin      ps_calc_17_bin      ps_calc_18_bin      ps_calc_19_bin      ps_calc_20_bin                  0      7      0      2      2      5      1      0      0      1      0      0      0      0      0      0      0      11      0      1      0      0.7      0.2      0.718070      10      1      -1      0      1      4      1      0      0      1      12      2      0.400000      0.883679      0.370810      3.605551      0.6      0.5      0.2      3      1      10      1      10      1      5      9      1      5      8      0      1      1      0      0      1              1      9      0      1      1      7      0      0      0      0      1      0      0      0      0      0      0      3      0      0      1      0.8      0.4      0.766078      11      1      -1      0      -1      11      1      1      2      1      19      3      0.316228      0.618817      0.388716      2.449490      0.3      0.1      0.3      2      1      9      5      8      1      7      3      1      1      9      0      1      1      0      1      0              2      13      0      5      4      9      1      0      0      0      1      0      0      0      0      0      0      12      1      0      0      0.0      0.0      -1.000000      7      1      -1      0      -1      14      1      1      2      1      60      1      0.316228      0.641586      0.347275      3.316625      0.5      0.7      0.1      2      2      9      1      8      2      7      4      2      7      7      0      1      1      0      1      0              3      16      0      0      1      2      0      0      1      0      0      0      0      0      0      0      0      8      1      0      0      0.9      0.2      0.580948      7      1      0      0      1      11      1      1      3      1      104      1      0.374166      0.542949      0.294958      2.000000      0.6      0.9      0.1      2      4      7      1      8      4      2      2      2      4      9      0      0      0      0      0      0              4      17      0      0      2      0      1      0      1      0      0      0      0      0      0      0      0      9      1      0      0      0.7      0.6      0.840759      11      1      -1      0      -1      14      1      1      2      1      82      3      0.316070      0.565832      0.365103      2.000000      0.4      0.6      0.0      2      2      6      3      10      2      12      3      1      1      3      0      0      0      1      1      0      train.tail()                  id      target      ps_ind_01      ps_ind_02_cat      ps_ind_03      ps_ind_04_cat      ps_ind_05_cat      ps_ind_06_bin      ps_ind_07_bin      ps_ind_08_bin      ps_ind_09_bin      ps_ind_10_bin      ps_ind_11_bin      ps_ind_12_bin      ps_ind_13_bin      ps_ind_14      ps_ind_15      ps_ind_16_bin      ps_ind_17_bin      ps_ind_18_bin      ps_reg_01      ps_reg_02      ps_reg_03      ps_car_01_cat      ps_car_02_cat      ps_car_03_cat      ps_car_04_cat      ps_car_05_cat      ps_car_06_cat      ps_car_07_cat      ps_car_08_cat      ps_car_09_cat      ps_car_10_cat      ps_car_11_cat      ps_car_11      ps_car_12      ps_car_13      ps_car_14      ps_car_15      ps_calc_01      ps_calc_02      ps_calc_03      ps_calc_04      ps_calc_05      ps_calc_06      ps_calc_07      ps_calc_08      ps_calc_09      ps_calc_10      ps_calc_11      ps_calc_12      ps_calc_13      ps_calc_14      ps_calc_15_bin      ps_calc_16_bin      ps_calc_17_bin      ps_calc_18_bin      ps_calc_19_bin      ps_calc_20_bin                  595207      1488013      0      3      1      10      0      0      0      0      0      1      0      0      0      0      0      13      1      0      0      0.5      0.3      0.692820      10      1      -1      0      1      1      1      1      0      1      31      3      0.374166      0.684631      0.385487      2.645751      0.4      0.5      0.3      3      0      9      0      9      1      12      4      1      9      6      0      1      1      0      1      1              595208      1488016      0      5      1      3      0      0      0      0      0      1      0      0      0      0      0      6      1      0      0      0.9      0.7      1.382027      9      1      -1      0      -1      15      0      0      2      1      63      2      0.387298      0.972145      -1.000000      3.605551      0.2      0.2      0.0      2      4      8      6      8      2      12      4      1      3      8      1      0      1      0      1      1              595209      1488017      0      1      1      10      0      0      1      0      0      0      0      0      0      0      0      12      1      0      0      0.9      0.2      0.659071      7      1      -1      0      -1      1      1      1      2      1      31      3      0.397492      0.596373      0.398748      1.732051      0.4      0.0      0.3      3      2      7      4      8      0      10      3      2      2      6      0      0      1      0      0      0              595210      1488021      0      5      2      3      1      0      0      0      1      0      0      0      0      0      0      12      1      0      0      0.9      0.4      0.698212      11      1      -1      0      -1      11      1      1      2      1      101      3      0.374166      0.764434      0.384968      3.162278      0.0      0.7      0.0      4      0      9      4      9      2      11      4      1      4      2      0      1      1      1      0      0              595211      1488027      0      0      1      8      0      0      1      0      0      0      0      0      0      0      0      7      1      0      0      0.1      0.2      -1.000000      7      0      -1      0      -1      0      1      0      2      1      34      2      0.400000      0.932649      0.378021      3.741657      0.4      0.0      0.5      2      3      10      4      10      2      5      4      4      3      8      0      1      0      0      0      0      train.shape(595212, 59)train.drop_duplicates()train.shape(595212, 59)test.shape (892816, 58)train.info()&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 595212 entries, 0 to 595211Data columns (total 59 columns): #   Column          Non-Null Count   Dtype  ---  ------          --------------   -----   0   id              595212 non-null  int64   1   target          595212 non-null  int64   2   ps_ind_01       595212 non-null  int64   3   ps_ind_02_cat   595212 non-null  int64   4   ps_ind_03       595212 non-null  int64   5   ps_ind_04_cat   595212 non-null  int64   6   ps_ind_05_cat   595212 non-null  int64   7   ps_ind_06_bin   595212 non-null  int64   8   ps_ind_07_bin   595212 non-null  int64   9   ps_ind_08_bin   595212 non-null  int64   10  ps_ind_09_bin   595212 non-null  int64   11  ps_ind_10_bin   595212 non-null  int64   12  ps_ind_11_bin   595212 non-null  int64   13  ps_ind_12_bin   595212 non-null  int64   14  ps_ind_13_bin   595212 non-null  int64   15  ps_ind_14       595212 non-null  int64   16  ps_ind_15       595212 non-null  int64   17  ps_ind_16_bin   595212 non-null  int64   18  ps_ind_17_bin   595212 non-null  int64   19  ps_ind_18_bin   595212 non-null  int64   20  ps_reg_01       595212 non-null  float64 21  ps_reg_02       595212 non-null  float64 22  ps_reg_03       595212 non-null  float64 23  ps_car_01_cat   595212 non-null  int64   24  ps_car_02_cat   595212 non-null  int64   25  ps_car_03_cat   595212 non-null  int64   26  ps_car_04_cat   595212 non-null  int64   27  ps_car_05_cat   595212 non-null  int64   28  ps_car_06_cat   595212 non-null  int64   29  ps_car_07_cat   595212 non-null  int64   30  ps_car_08_cat   595212 non-null  int64   31  ps_car_09_cat   595212 non-null  int64   32  ps_car_10_cat   595212 non-null  int64   33  ps_car_11_cat   595212 non-null  int64   34  ps_car_11       595212 non-null  int64   35  ps_car_12       595212 non-null  float64 36  ps_car_13       595212 non-null  float64 37  ps_car_14       595212 non-null  float64 38  ps_car_15       595212 non-null  float64 39  ps_calc_01      595212 non-null  float64 40  ps_calc_02      595212 non-null  float64 41  ps_calc_03      595212 non-null  float64 42  ps_calc_04      595212 non-null  int64   43  ps_calc_05      595212 non-null  int64   44  ps_calc_06      595212 non-null  int64   45  ps_calc_07      595212 non-null  int64   46  ps_calc_08      595212 non-null  int64   47  ps_calc_09      595212 non-null  int64   48  ps_calc_10      595212 non-null  int64   49  ps_calc_11      595212 non-null  int64   50  ps_calc_12      595212 non-null  int64   51  ps_calc_13      595212 non-null  int64   52  ps_calc_14      595212 non-null  int64   53  ps_calc_15_bin  595212 non-null  int64   54  ps_calc_16_bin  595212 non-null  int64   55  ps_calc_17_bin  595212 non-null  int64   56  ps_calc_18_bin  595212 non-null  int64   57  ps_calc_19_bin  595212 non-null  int64   58  ps_calc_20_bin  595212 non-null  int64  dtypes: float64(10), int64(49)memory usage: 267.9 MBfloat인지 int인지 자료형을 알 수 있으며 null 대신 -1이 들어갔으므로 null이 없습니다.메타 데이터메타데이터를 통해 데이터 정보를 저장합니다.변수의 타입과 feature의 특성을 저장합니다.data = []for f in train.columns:    if f == 'target':        role = 'target'    elif f == 'id':        role = 'id'    else:        role = 'input'             if 'bin' in f or f == 'target':        level = 'binary'    elif 'cat' in f or f == 'id':        level = 'nominal'    elif train[f].dtype == float:        level = 'interval'    elif train[f].dtype == int:        level = 'ordinal'            keep = True    if f == 'id':        keep = False        dtype = train[f].dtype        f_dict = {        'varname': f,        'role': role,        'level': level,        'keep': keep,        'dtype': dtype    }    data.append(f_dict)    meta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])meta.set_index('varname', inplace=True)meta                  role      level      keep      dtype              varname                                          id      id      nominal      False      int64              target      target      binary      True      int64              ps_ind_01      input      binary      True      int64              ps_ind_02_cat      input      nominal      True      int64              ps_ind_03      input      nominal      True      int64              ps_ind_04_cat      input      nominal      True      int64              ps_ind_05_cat      input      nominal      True      int64              ps_ind_06_bin      input      binary      True      int64              ps_ind_07_bin      input      binary      True      int64              ps_ind_08_bin      input      binary      True      int64              ps_ind_09_bin      input      binary      True      int64              ps_ind_10_bin      input      binary      True      int64              ps_ind_11_bin      input      binary      True      int64              ps_ind_12_bin      input      binary      True      int64              ps_ind_13_bin      input      binary      True      int64              ps_ind_14      input      binary      True      int64              ps_ind_15      input      binary      True      int64              ps_ind_16_bin      input      binary      True      int64              ps_ind_17_bin      input      binary      True      int64              ps_ind_18_bin      input      binary      True      int64              ps_reg_01      input      interval      True      float64              ps_reg_02      input      interval      True      float64              ps_reg_03      input      interval      True      float64              ps_car_01_cat      input      nominal      True      int64              ps_car_02_cat      input      nominal      True      int64              ps_car_03_cat      input      nominal      True      int64              ps_car_04_cat      input      nominal      True      int64              ps_car_05_cat      input      nominal      True      int64              ps_car_06_cat      input      nominal      True      int64              ps_car_07_cat      input      nominal      True      int64              ps_car_08_cat      input      nominal      True      int64              ps_car_09_cat      input      nominal      True      int64              ps_car_10_cat      input      nominal      True      int64              ps_car_11_cat      input      nominal      True      int64              ps_car_11      input      nominal      True      int64              ps_car_12      input      interval      True      float64              ps_car_13      input      interval      True      float64              ps_car_14      input      interval      True      float64              ps_car_15      input      interval      True      float64              ps_calc_01      input      interval      True      float64              ps_calc_02      input      interval      True      float64              ps_calc_03      input      interval      True      float64              ps_calc_04      input      interval      True      int64              ps_calc_05      input      interval      True      int64              ps_calc_06      input      interval      True      int64              ps_calc_07      input      interval      True      int64              ps_calc_08      input      interval      True      int64              ps_calc_09      input      interval      True      int64              ps_calc_10      input      interval      True      int64              ps_calc_11      input      interval      True      int64              ps_calc_12      input      interval      True      int64              ps_calc_13      input      interval      True      int64              ps_calc_14      input      interval      True      int64              ps_calc_15_bin      input      binary      True      int64              ps_calc_16_bin      input      binary      True      int64              ps_calc_17_bin      input      binary      True      int64              ps_calc_18_bin      input      binary      True      int64              ps_calc_19_bin      input      binary      True      int64              ps_calc_20_bin      input      binary      True      int64      meta[(meta.level == 'nominal') &amp; (meta.keep)].indexIndex(['ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat', 'ps_ind_05_cat',       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11'],      dtype='object', name='varname')pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()                  role      level      count                  0      id      nominal      1              1      input      binary      20              2      input      interval      21              3      input      nominal      16              4      target      binary      1      통계량 살펴보기메타 데이터에서 interval인 값만 찾아서 describe를 사용할 수 있습니다.ps_reg_03, ps_car_12, ps_car_15에 결측값이 있습니다.v = meta[(meta.level == 'interval') &amp; (meta.keep)].indextrain[v].describe()                  ps_reg_01      ps_reg_02      ps_reg_03      ps_car_12      ps_car_13      ps_car_14      ps_car_15      ps_calc_01      ps_calc_02      ps_calc_03      ps_calc_04      ps_calc_05      ps_calc_06      ps_calc_07      ps_calc_08      ps_calc_09      ps_calc_10      ps_calc_11      ps_calc_12      ps_calc_13      ps_calc_14                  count      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000              mean      0.610991      0.439184      0.551102      0.379945      0.813265      0.276256      3.065899      0.449756      0.449589      0.449849      2.372081      1.885886      7.689445      3.005823      9.225904      2.339034      8.433590      5.441382      1.441918      2.872288      7.539026              std      0.287643      0.404264      0.793506      0.058327      0.224588      0.357154      0.731366      0.287198      0.286893      0.287153      1.117219      1.134927      1.334312      1.414564      1.459672      1.246949      2.904597      2.332871      1.202963      1.694887      2.746652              min      0.000000      0.000000      -1.000000      -1.000000      0.250619      -1.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      2.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000              25%      0.400000      0.200000      0.525000      0.316228      0.670867      0.333167      2.828427      0.200000      0.200000      0.200000      2.000000      1.000000      7.000000      2.000000      8.000000      1.000000      6.000000      4.000000      1.000000      2.000000      6.000000              50%      0.700000      0.300000      0.720677      0.374166      0.765811      0.368782      3.316625      0.500000      0.400000      0.500000      2.000000      2.000000      8.000000      3.000000      9.000000      2.000000      8.000000      5.000000      1.000000      3.000000      7.000000              75%      0.900000      0.600000      1.000000      0.400000      0.906190      0.396485      3.605551      0.700000      0.700000      0.700000      3.000000      3.000000      9.000000      4.000000      10.000000      3.000000      10.000000      7.000000      2.000000      4.000000      9.000000              max      0.900000      1.800000      4.037945      1.264911      3.720626      0.636396      3.741657      0.900000      0.900000      0.900000      5.000000      6.000000      10.000000      9.000000      12.000000      7.000000      25.000000      19.000000      10.000000      13.000000      23.000000      v = meta[(meta.level == 'binary') &amp; (meta.keep)].indextrain[v].describe()                  target      ps_ind_01      ps_ind_06_bin      ps_ind_07_bin      ps_ind_08_bin      ps_ind_09_bin      ps_ind_10_bin      ps_ind_11_bin      ps_ind_12_bin      ps_ind_13_bin      ps_ind_14      ps_ind_15      ps_ind_16_bin      ps_ind_17_bin      ps_ind_18_bin      ps_calc_15_bin      ps_calc_16_bin      ps_calc_17_bin      ps_calc_18_bin      ps_calc_19_bin      ps_calc_20_bin                  count      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000      595212.000000              mean      0.036448      1.900378      0.393742      0.257033      0.163921      0.185304      0.000373      0.001692      0.009439      0.000948      0.012451      7.299922      0.660823      0.121081      0.153446      0.122427      0.627840      0.554182      0.287182      0.349024      0.153318              std      0.187401      1.983789      0.488579      0.436998      0.370205      0.388544      0.019309      0.041097      0.096693      0.030768      0.127545      3.546042      0.473430      0.326222      0.360417      0.327779      0.483381      0.497056      0.452447      0.476662      0.360295              min      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000              25%      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      5.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000              50%      0.000000      1.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      7.000000      1.000000      0.000000      0.000000      0.000000      1.000000      1.000000      0.000000      0.000000      0.000000              75%      0.000000      3.000000      1.000000      1.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      10.000000      1.000000      0.000000      0.000000      0.000000      1.000000      1.000000      1.000000      1.000000      0.000000              max      1.000000      7.000000      1.000000      1.000000      1.000000      1.000000      1.000000      1.000000      1.000000      1.000000      4.000000      13.000000      1.000000      1.000000      1.000000      1.000000      1.000000      1.000000      1.000000      1.000000      1.000000      target=1인 비율이 target=0보다 훨씬 작기 때문에 target=1을 오버 샘플링 혹은 target=0으로 언더샘플링 할 수 있습니다.언더 샘플링은 불균형한 데이터 셋에서 높은 비율을 차지하던 클래스의 데이터 수를 줄임으로써 데이터 불균형을 해소하는 아이디어 입니다.하지만 이 방법은 학습에 사용되는 전체 데이터 수를 급격하게 감소시켜 오히려 성능이 떨어질 수 있습니다.오버 샘플링은 낮은 비율 클래스의 데이터 수를 늘림으로써 데이터 불균형을 해소하는 아이디어 입니다.desired_apriori=0.10idx_0 = train[train.target == 0].indexidx_1 = train[train.target == 1].indexnb_0 = len(train.loc[idx_0])nb_1 = len(train.loc[idx_1])undersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)undersampled_nb_0 = int(undersampling_rate*nb_0)print('Rate to undersample records with target=0: {}'.format(undersampling_rate))print('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))undersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)idx_list = list(undersampled_idx) + list(idx_1)train = train.loc[idx_list].reset_index(drop=True)Rate to undersample records with target=0: 0.34043569687437886Number of records with target=0 after undersampling: 195246ps_car_03_cat 및 ps_car_05_cat는 결측값이 있는 비율이 높습니다. 이 변수를 제거합니다.결측값이 있는 다른 범주형 변수의 경우 결측값 -1을 그대로 둘 수 있습니다.ps_reg_03 (continuous)에는 18%에 대한 결측값이 있습니다. 평균으로 대체합니다.ps_car_11 (순서형)에는 결측 값이 5개만 있습니다. 모드로 대체합니다.ps_car_12 (연속)에는 결측값이 1개만 있습니다. 평균으로 대체합니다.ps_car_14 (연속)에는 7%에 대한 결측값이 있습니다. 평균으로 대체합니다.vars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']train.drop(vars_to_drop, inplace=True, axis=1)meta.loc[(vars_to_drop),'keep'] = False  # 메타 데이터 updatemean_imp = SimpleImputer(missing_values=-1, strategy='mean')mode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')train['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()train['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()train['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()train['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()v = meta[(meta.level == 'nominal') &amp; (meta.keep)].indexfor f in v:    dist_values = train[f].value_counts().shape[0]    print('Variable {} has {} distinct values'.format(f, dist_values))Variable ps_ind_02_cat has 5 distinct valuesVariable ps_ind_03 has 12 distinct valuesVariable ps_ind_04_cat has 3 distinct valuesVariable ps_ind_05_cat has 8 distinct valuesVariable ps_car_01_cat has 13 distinct valuesVariable ps_car_02_cat has 3 distinct valuesVariable ps_car_04_cat has 10 distinct valuesVariable ps_car_06_cat has 18 distinct valuesVariable ps_car_07_cat has 3 distinct valuesVariable ps_car_08_cat has 2 distinct valuesVariable ps_car_09_cat has 6 distinct valuesVariable ps_car_10_cat has 3 distinct valuesVariable ps_car_11_cat has 104 distinct valuesVariable ps_car_11 has 4 distinct valuesdef add_noise(series, noise_level):    return series * (1 + noise_level * np.random.randn(len(series)))def target_encode(trn_series=None,                   tst_series=None,                   target=None,                   min_samples_leaf=1,                   smoothing=1,                  noise_level=0):    assert len(trn_series) == len(target)    assert trn_series.name == tst_series.name    temp = pd.concat([trn_series, target], axis=1)    # 평균 계산    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))        prior = target.mean()    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)    ft_trn_series = pd.merge(        trn_series.to_frame(trn_series.name),        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),        on=trn_series.name,        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)    ft_trn_series.index = trn_series.index     ft_tst_series = pd.merge(        tst_series.to_frame(tst_series.name),        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),        on=tst_series.name,        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)    ft_tst_series.index = tst_series.index    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"],                              test[\"ps_car_11_cat\"],                              target=train.target,                              min_samples_leaf=100,                             smoothing=10,                             noise_level=0.01)    train['ps_car_11_cat_te'] = train_encodedtrain.drop('ps_car_11_cat', axis=1, inplace=True)meta.loc['ps_car_11_cat','keep'] = False  # 메타 데이터 업데이트test['ps_car_11_cat_te'] = test_encodedtest.drop('ps_car_11_cat', axis=1, inplace=True)탐색 데이터 시각화범주값으로 결측값을 유지한다면 비율을 확인하기 좋습니다.v = meta[(meta.level == 'nominal') &amp; (meta.keep)].indexfor f in v:    plt.figure()    fig, ax = plt.subplots(figsize=(20,10))    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()    cat_perc.sort_values(by='target', ascending=False, inplace=True)    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])    plt.ylabel('% target', fontsize=18)    plt.xlabel(f, fontsize=18)    plt.tick_params(axis='both', which='major', labelsize=18)    plt.show();&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;&lt;Figure size 432x288 with 0 Axes&gt;구간 변수는 상관 관계를 확인하는 것이 좋습니다.def corr_heatmap(v):    correlations = train[v].corr()    cmap = sns.diverging_palette(220, 10, as_cmap=True)    fig, ax = plt.subplots(figsize=(10,10))    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})    plt.show();    v = meta[(meta.level == 'interval') &amp; (meta.keep)].indexcorr_heatmap(v)s = train.sample(frac=0.1)sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})plt.show()sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})plt.show()sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})plt.show()sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})plt.show()Feature Engineering# 더미 변수 만들기v = meta[(meta.level == 'nominal') &amp; (meta.keep)].indexprint('Before dummification we have {} variables in train'.format(train.shape[1]))train = pd.get_dummies(train, columns=v, drop_first=True)print('After dummification we have {} variables in train'.format(train.shape[1]))Before dummification we have 57 variables in trainAfter dummification we have 121 variables in train# 상호작용 변수 만들기v = meta[(meta.level == 'interval') &amp; (meta.keep)].indexpoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)interactions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))interactions.drop(v, axis=1, inplace=True)print('Before creating interactions we have {} variables in train'.format(train.shape[1]))train = pd.concat([train, interactions], axis=1)print('After creating interactions we have {} variables in train'.format(train.shape[1]))C:\\Users\\keonj\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.  warnings.warn(msg, category=FutureWarning)Before creating interactions we have 121 variables in trainAfter creating interactions we have 352 variables in trainFeature Selection# VarianceThreshold을 통해 분산이 작은 값 제거selector = VarianceThreshold(threshold=.01)selector.fit(train.drop(['id', 'target'], axis=1))f = np.vectorize(lambda x : not x) v = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]print('{} variables have too low variance.'.format(len(v)))print('These variables are {}'.format(list(v)))28 variables have too low variance.These variables are ['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_12', 'ps_car_14', 'ps_car_11_cat_te', 'ps_ind_05_cat_2', 'ps_ind_05_cat_5', 'ps_car_01_cat_1', 'ps_car_01_cat_2', 'ps_car_04_cat_3', 'ps_car_04_cat_4', 'ps_car_04_cat_5', 'ps_car_04_cat_6', 'ps_car_04_cat_7', 'ps_car_06_cat_2', 'ps_car_06_cat_5', 'ps_car_06_cat_8', 'ps_car_06_cat_12', 'ps_car_06_cat_16', 'ps_car_06_cat_17', 'ps_car_09_cat_4', 'ps_car_10_cat_1', 'ps_car_10_cat_2', 'ps_car_12^2', 'ps_car_12 ps_car_14', 'ps_car_14^2']# 랜덤 포레스트를 통한 특성 선택 및 모델 선택X_train = train.drop(['id', 'target'], axis=1)y_train = train['target']feat_labels = X_train.columnsrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)rf.fit(X_train, y_train)importances = rf.feature_importances_indices = np.argsort(rf.feature_importances_)[::-1]for f in range(X_train.shape[1]):    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]])) 1) ps_car_11_cat_te               0.008718 2) ps_car_13 ps_calc_08           0.006701 3) ps_car_13 ps_calc_06           0.006655 4) ps_car_13^2                    0.006577 5) ps_car_13                      0.006575 6) ps_car_13 ps_car_14            0.006478 7) ps_car_12 ps_car_13            0.006459 8) ps_car_13 ps_car_15            0.006277 9) ps_reg_01 ps_car_13            0.00626610) ps_reg_03 ps_car_13            0.00625011) ps_car_14 ps_calc_08           0.00609312) ps_car_14 ps_calc_06           0.00603213) ps_car_14 ps_car_15            0.00588414) ps_reg_03 ps_car_14            0.00580815) ps_car_13 ps_calc_10           0.00578916) ps_car_13 ps_calc_14           0.00576417) ps_reg_03 ps_calc_08           0.00574018) ps_car_13 ps_calc_11           0.00570019) ps_reg_03 ps_car_12            0.00568520) ps_reg_03 ps_calc_06           0.00560921) ps_car_14 ps_calc_10           0.00549422) ps_car_13 ps_calc_07           0.00549323) ps_reg_03 ps_calc_10           0.00547324) ps_car_14 ps_calc_14           0.00546025) ps_reg_01 ps_car_14            0.00542726) ps_car_14 ps_calc_11           0.00541527) ps_car_13 ps_calc_04           0.00540928) ps_reg_03 ps_car_15            0.00536229) ps_reg_03 ps_calc_11           0.00535630) ps_reg_03 ps_calc_14           0.00535231) ps_reg_02 ps_car_13            0.00529532) ps_car_13 ps_calc_09           0.00518933) ps_car_14                      0.00518334) ps_car_14^2                    0.00517335) ps_reg_01 ps_reg_03            0.00513736) ps_car_12 ps_car_14            0.00512637) ps_reg_03 ps_calc_07           0.00511038) ps_car_14 ps_calc_07           0.00508939) ps_reg_03                      0.00505940) ps_car_13 ps_calc_01           0.00504541) ps_reg_03^2                    0.00503642) ps_car_14 ps_calc_04           0.00502443) ps_car_13 ps_calc_13           0.00501244) ps_reg_03 ps_calc_04           0.00500845) ps_car_13 ps_calc_03           0.00498446) ps_car_13 ps_calc_02           0.00497547) ps_car_13 ps_calc_05           0.00494348) ps_reg_03 ps_calc_09           0.00480849) ps_car_14 ps_calc_09           0.00477150) ps_reg_03 ps_calc_13           0.00476551) ps_car_14 ps_calc_02           0.00470852) ps_car_14 ps_calc_13           0.00470253) ps_calc_10 ps_calc_14          0.00465654) ps_car_15 ps_calc_10           0.00465555) ps_car_14 ps_calc_01           0.00464756) ps_calc_10 ps_calc_11          0.00463657) ps_car_14 ps_calc_03           0.00462058) ps_reg_03 ps_calc_02           0.00461959) ps_car_15 ps_calc_08           0.00461660) ps_reg_03 ps_calc_03           0.00460361) ps_calc_08 ps_calc_10          0.00457362) ps_car_14 ps_calc_05           0.00456663) ps_reg_03 ps_calc_05           0.00456564) ps_car_15 ps_calc_14           0.00455665) ps_reg_02 ps_car_14            0.00454366) ps_reg_03 ps_calc_01           0.00453467) ps_car_12 ps_calc_08           0.00451568) ps_calc_08 ps_calc_14          0.00450569) ps_car_12 ps_calc_10           0.00447670) ps_car_12 ps_car_15            0.00446771) ps_calc_11 ps_calc_14          0.00444172) ps_car_15 ps_calc_11           0.00441273) ps_calc_06 ps_calc_10          0.00440874) ps_reg_02 ps_reg_03            0.00440675) ps_car_12 ps_calc_14           0.00439176) ps_car_12 ps_calc_06           0.00437677) ps_ind_15                      0.00435378) ps_car_15 ps_calc_06           0.00434179) ps_calc_06 ps_calc_14          0.00430680) ps_calc_08 ps_calc_11          0.00428281) ps_calc_06 ps_calc_08          0.00425382) ps_car_12 ps_calc_11           0.00418883) ps_calc_07 ps_calc_10          0.00416384) ps_calc_06 ps_calc_11          0.00413485) ps_calc_07 ps_calc_14          0.00407686) ps_calc_02 ps_calc_10          0.00403287) ps_calc_01 ps_calc_14          0.00402588) ps_car_13 ps_calc_12           0.00402589) ps_calc_01 ps_calc_10          0.00401190) ps_reg_01 ps_calc_10           0.00401191) ps_calc_03 ps_calc_10          0.00400892) ps_calc_10 ps_calc_13          0.00398993) ps_calc_04 ps_calc_10          0.00398594) ps_calc_03 ps_calc_14          0.00397195) ps_calc_13 ps_calc_14          0.00397096) ps_car_15 ps_calc_07           0.00394597) ps_calc_02 ps_calc_14          0.00393798) ps_reg_01 ps_calc_14           0.00392599) ps_calc_09 ps_calc_10          0.003895100) ps_calc_04 ps_calc_14          0.003877101) ps_calc_07 ps_calc_08          0.003863102) ps_reg_03 ps_calc_12           0.003849103) ps_calc_01 ps_calc_11          0.003822104) ps_calc_09 ps_calc_14          0.003819105) ps_reg_02 ps_car_15            0.003806106) ps_calc_07 ps_calc_11          0.003779107) ps_reg_02 ps_calc_10           0.003771108) ps_calc_03 ps_calc_11          0.003771109) ps_calc_02 ps_calc_11          0.003757110) ps_reg_02 ps_calc_14           0.003757111) ps_reg_01 ps_car_15            0.003754112) ps_car_14 ps_calc_12           0.003745113) ps_calc_01 ps_calc_08          0.003725114) ps_calc_05 ps_calc_10          0.003722115) ps_car_15 ps_calc_13           0.003717116) ps_car_12 ps_calc_01           0.003715117) ps_calc_11 ps_calc_13          0.003708118) ps_calc_03 ps_calc_08          0.003693119) ps_car_15 ps_calc_02           0.003684120) ps_car_12 ps_calc_07           0.003682121) ps_calc_06 ps_calc_07          0.003680122) ps_car_15 ps_calc_03           0.003676123) ps_car_12 ps_calc_03           0.003671124) ps_car_12 ps_calc_02           0.003670125) ps_car_15 ps_calc_01           0.003668126) ps_calc_08 ps_calc_13          0.003664127) ps_car_15 ps_calc_09           0.003660128) ps_reg_01 ps_car_12            0.003653129) ps_car_15 ps_calc_04           0.003648130) ps_calc_02 ps_calc_08          0.003647131) ps_reg_01 ps_calc_11           0.003636132) ps_calc_04 ps_calc_11          0.003621133) ps_reg_02 ps_calc_11           0.003607134) ps_calc_05 ps_calc_14          0.003590135) ps_calc_09 ps_calc_11          0.003569136) ps_reg_01 ps_calc_08           0.003557137) ps_calc_04 ps_calc_08          0.003544138) ps_reg_02 ps_car_12            0.003506139) ps_car_12 ps_calc_13           0.003504140) ps_reg_02 ps_calc_08           0.003498141) ps_calc_08 ps_calc_09          0.003486142) ps_calc_02 ps_calc_07          0.003485143) ps_calc_06 ps_calc_13          0.003480144) ps_calc_01 ps_calc_06          0.003478145) ps_calc_03 ps_calc_07          0.003461146) ps_calc_03 ps_calc_06          0.003435147) ps_car_12 ps_calc_04           0.003432148) ps_calc_02 ps_calc_06          0.003429149) ps_calc_01 ps_calc_13          0.003413150) ps_reg_02 ps_calc_06           0.003413151) ps_calc_01 ps_calc_07          0.003402152) ps_calc_01 ps_calc_02          0.003385153) ps_calc_02 ps_calc_13          0.003379154) ps_car_15 ps_calc_05           0.003358155) ps_calc_03 ps_calc_13          0.003357156) ps_calc_04 ps_calc_06          0.003357157) ps_calc_02 ps_calc_03          0.003345158) ps_calc_01 ps_calc_03          0.003338159) ps_calc_05 ps_calc_11          0.003313160) ps_calc_05 ps_calc_08          0.003306161) ps_car_12 ps_calc_09           0.003306162) ps_reg_01 ps_calc_06           0.003295163) ps_calc_06 ps_calc_09          0.003271164) ps_calc_03 ps_calc_09          0.003266165) ps_calc_03 ps_calc_04          0.003259166) ps_reg_02 ps_calc_01           0.003258167) ps_calc_02 ps_calc_09          0.003207168) ps_calc_01 ps_calc_09          0.003206169) ps_reg_02 ps_calc_03           0.003195170) ps_reg_02 ps_calc_02           0.003194171) ps_calc_10 ps_calc_12          0.003190172) ps_calc_02 ps_calc_04          0.003190173) ps_calc_01 ps_calc_04          0.003184174) ps_calc_07 ps_calc_13          0.003152175) ps_reg_02 ps_calc_07           0.003144176) ps_calc_12 ps_calc_14          0.003128177) ps_reg_01 ps_calc_07           0.003110178) ps_car_12 ps_calc_05           0.003106179) ps_reg_01 ps_calc_13           0.003098180) ps_calc_02 ps_calc_05          0.003069181) ps_calc_05 ps_calc_06          0.003065182) ps_calc_01 ps_calc_05          0.003065183) ps_calc_03 ps_calc_05          0.003060184) ps_reg_01 ps_calc_03           0.003045185) ps_reg_02 ps_calc_13           0.003041186) ps_calc_09 ps_calc_13          0.003035187) ps_reg_01 ps_calc_01           0.003024188) ps_reg_01 ps_calc_02           0.003007189) ps_calc_04 ps_calc_13          0.002981190) ps_calc_07 ps_calc_09          0.002955191) ps_calc_11 ps_calc_12          0.002941192) ps_calc_04 ps_calc_07          0.002920193) ps_reg_02 ps_calc_04           0.002900194) ps_reg_02 ps_calc_09           0.002900195) ps_ind_01                      0.002898196) ps_car_15 ps_calc_12           0.002890197) ps_reg_01 ps_reg_02            0.002855198) ps_reg_01 ps_calc_09           0.002844199) ps_calc_05 ps_calc_13          0.002819200) ps_reg_01 ps_calc_04           0.002815201) ps_reg_02 ps_calc_05           0.002813202) ps_calc_02 ps_calc_12          0.002800203) ps_calc_08 ps_calc_12          0.002791204) ps_calc_05 ps_calc_07          0.002779205) ps_calc_01 ps_calc_12          0.002768206) ps_calc_03 ps_calc_12          0.002737207) ps_car_12 ps_calc_12           0.002715208) ps_reg_01 ps_calc_05           0.002691209) ps_calc_10                     0.002666210) ps_calc_06 ps_calc_12          0.002662211) ps_calc_10^2                   0.002659212) ps_calc_04 ps_calc_09          0.002657213) ps_calc_14^2                   0.002583214) ps_ind_05_cat_0                0.002573215) ps_calc_14                     0.002549216) ps_calc_05 ps_calc_09          0.002523217) ps_calc_12 ps_calc_13          0.002499218) ps_calc_07 ps_calc_12          0.002487219) ps_calc_04 ps_calc_05          0.002485220) ps_reg_02 ps_calc_12           0.002445221) ps_calc_09 ps_calc_12          0.002351222) ps_reg_01 ps_calc_12           0.002345223) ps_calc_11^2                   0.002291224) ps_calc_11                     0.002261225) ps_calc_04 ps_calc_12          0.002256226) ps_car_15                      0.002243227) ps_car_15^2                    0.002221228) ps_car_12                      0.002203229) ps_car_12^2                    0.002182230) ps_calc_05 ps_calc_12          0.002146231) ps_calc_08                     0.002133232) ps_calc_08^2                   0.002118233) ps_calc_03                     0.001906234) ps_calc_01                     0.001902235) ps_calc_01^2                   0.001899236) ps_calc_03^2                   0.001897237) ps_calc_02^2                   0.001890238) ps_calc_02                     0.001872239) ps_calc_06^2                   0.001859240) ps_calc_06                     0.001849241) ps_reg_02^2                    0.001763242) ps_ind_17_bin                  0.001735243) ps_reg_02                      0.001712244) ps_calc_13^2                   0.001696245) ps_calc_13                     0.001695246) ps_calc_07^2                   0.001607247) ps_calc_07                     0.001594248) ps_reg_01                      0.001457249) ps_reg_01^2                    0.001445250) ps_calc_09^2                   0.001385251) ps_calc_09                     0.001382252) ps_calc_04^2                   0.001287253) ps_calc_04                     0.001279254) ps_calc_05                     0.001251255) ps_calc_05^2                   0.001245256) ps_car_07_cat_1                0.001210257) ps_calc_12^2                   0.001203258) ps_calc_12                     0.001184259) ps_ind_16_bin                  0.001039260) ps_ind_05_cat_6                0.000984261) ps_ind_07_bin                  0.000935262) ps_car_09_cat_1                0.000840263) ps_ind_06_bin                  0.000810264) ps_ind_02_cat_1                0.000746265) ps_car_01_cat_9                0.000733266) ps_calc_17_bin                 0.000731267) ps_ind_04_cat_1                0.000725268) ps_ind_04_cat_0                0.000720269) ps_car_07_cat_0                0.000716270) ps_calc_19_bin                 0.000713271) ps_ind_02_cat_2                0.000712272) ps_calc_18_bin                 0.000710273) ps_car_01_cat_11               0.000710274) ps_calc_16_bin                 0.000702275) ps_ind_05_cat_4                0.000700276) ps_ind_03_7                    0.000691277) ps_ind_03_1                    0.000691278) ps_car_09_cat_2                0.000678279) ps_ind_05_cat_2                0.000672280) ps_ind_08_bin                  0.000672281) ps_car_01_cat_7                0.000672282) ps_ind_03_6                    0.000671283) ps_car_06_cat_1                0.000669284) ps_car_11_3                    0.000646285) ps_car_09_cat_0                0.000643286) ps_ind_03_5                    0.000635287) ps_car_11_2                    0.000629288) ps_calc_15_bin                 0.000627289) ps_ind_03_8                    0.000613290) ps_calc_20_bin                 0.000608291) ps_car_04_cat_2                0.000606292) ps_car_01_cat_4                0.000604293) ps_ind_18_bin                  0.000579294) ps_car_06_cat_11               0.000572295) ps_ind_03_10                   0.000571296) ps_ind_03_3                    0.000563297) ps_ind_03_2                    0.000561298) ps_car_01_cat_8                0.000558299) ps_ind_02_cat_4                0.000557300) ps_car_01_cat_10               0.000555301) ps_ind_09_bin                  0.000550302) ps_ind_02_cat_3                0.000543303) ps_car_06_cat_14               0.000535304) ps_ind_03_9                    0.000534305) ps_car_01_cat_6                0.000526306) ps_ind_03_11                   0.000522307) ps_ind_03_4                    0.000515308) ps_car_01_cat_5                0.000511309) ps_car_02_cat_0                0.000507310) ps_car_02_cat_1                0.000504311) ps_ind_14                      0.000502312) ps_car_06_cat_6                0.000483313) ps_car_06_cat_4                0.000478314) ps_car_08_cat_1                0.000471315) ps_car_01_cat_3                0.000445316) ps_car_01_cat_0                0.000439317) ps_car_06_cat_10               0.000438318) ps_car_06_cat_7                0.000432319) ps_ind_05_cat_1                0.000422320) ps_car_09_cat_3                0.000404321) ps_car_04_cat_1                0.000402322) ps_ind_12_bin                  0.000380323) ps_car_06_cat_9                0.000372324) ps_car_06_cat_15               0.000365325) ps_car_11_1                    0.000363326) ps_car_10_cat_1                0.000362327) ps_car_09_cat_4                0.000355328) ps_ind_05_cat_3                0.000334329) ps_car_01_cat_2                0.000324330) ps_car_06_cat_3                0.000321331) ps_car_06_cat_17               0.000276332) ps_car_06_cat_12               0.000270333) ps_car_06_cat_16               0.000235334) ps_car_01_cat_1                0.000232335) ps_car_04_cat_8                0.000218336) ps_car_04_cat_9                0.000212337) ps_ind_05_cat_5                0.000184338) ps_car_06_cat_13               0.000180339) ps_car_06_cat_5                0.000178340) ps_ind_11_bin                  0.000143341) ps_car_04_cat_6                0.000132342) ps_ind_13_bin                  0.000095343) ps_car_04_cat_3                0.000087344) ps_car_06_cat_2                0.000066345) ps_car_04_cat_5                0.000058346) ps_car_04_cat_7                0.000056347) ps_car_06_cat_8                0.000045348) ps_car_10_cat_2                0.000045349) ps_ind_10_bin                  0.000045350) ps_car_04_cat_4                0.000027sfm = SelectFromModel(rf, threshold='median', prefit=True)print('Number of features before selection: {}'.format(X_train.shape[1]))n_features = sfm.transform(X_train).shape[1]print('Number of features after selection: {}'.format(n_features))selected_vars = list(feat_labels[sfm.get_support()])Number of features before selection: 350C:\\Users\\keonj\\anaconda3\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but SelectFromModel was fitted without feature names  warnings.warn(Number of features after selection: 175# Feature scalingscaler = StandardScaler()scaler.fit_transform(train.drop(['target'], axis=1))array([[-0.90494248, -0.45941104,  1.25877984, ...,  0.40315483,         0.14885213, -0.62460393],       [ 0.24006954,  1.55538958,  1.25877984, ..., -0.17489762,        -0.04208459, -0.33950182],       [ 1.64508122,  1.05168943,  1.25877984, ..., -0.17489762,         0.53072557,  0.77897569],       ...,       [ 1.73477713, -0.9631112 , -0.7944201 , ..., -0.83552899,        -0.99676819, -0.62460393],       [ 1.73485162, -0.9631112 ,  1.25877984, ...,  0.40315483,        -0.36031245, -1.06322256],       [ 1.73512631, -0.45941104, -0.7944201 , ...,  0.40315483,         0.91259901,  0.36228799]])",
        "url": "/programming-kaggle5"
    }
    ,
    
    "programming-baekjoon9": {
        "title": "백준 (9) &lt;br&gt; (1927,11279,11286, &lt;br&gt; 1715,11766)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)    백준 (9) 우선순위 큐 알고리즘 (1927,11279,11286,1715,11766)우선순위 큐https://www.acmicpc.net/problemset?sort=ac_desc&amp;algo=59우선순위 큐에 대해서 5문제를 풀어보았습니다.원래 7문제를 풀어야할 계획이었지만 1655,1202번을 해결하지 못했습니다.우선순위 큐란?우선순위 큐는 힙이라는 자료구조와 같습니다.파이썬에서는 heapq라는 모듈을 지원해주는데 모듈에 있는 함수는 다음과 같습니다.heapq.heappush(a,b): b를 a에 추가 - O(logn)heapq.heappop(a): a에서 가장 작은 원소를 pop한 다음 리턴해주며 비어있으면 IndexError 발생 - O(logn)heqpq.heapify(a): 리스트 a를 즉각적으로 heap으로 변환 - O(n) -&gt; 만약 heappush로 만든다면? O(nlog(n))하지만 heapify는 최소 힙만을 지원합니다.최소 힙이란 부모 노드의 키 값이 자식 노드의 키 값보다 작거나 같은 완전 이진 트리를 말합니다.최대 힙이란 각 노드의 키 값이 자식의 키 값보다 작지 않은 트리입니다.1927번 최소 힙https://www.acmicpc.net/problem/1927heapq 모듈을 이용하여 리스트에 숫자가 있을 때와 없을 때를 구분지어서 만들어줬습니다.import heapqhlist=[]for i in range(int(input())):    a=int(input())    if len(hlist)!=0:        if a!=0:            heapq.heappush(hlist,a)        else:            b=heapq.heappop(hlist)            print(b)    else:        if a!=0:            heapq.heappush(hlist,a)        else:            print(a)90012345678120102012345678003211279번 최대 힙https://www.acmicpc.net/problem/11279heapq 모듈을 이용하여 리스트에 숫자가 있을 때와 없을 때를 구분지어서 만들어줬습니다.heapq는 작은 숫자를 출력하는 heappop을 지원하기 때문에 들어온 값에 -를 붙여서 저장하면 큰 수부터 출력할 수 있습니다.import heapqh_list=[]for i in range(int(input())):    a=-int(input())    if len(h_list)==0:        if a==0:            print(-a)        else:            heapq.heappush(h_list,a)    else:        if a==0:            b=heapq.heappop(h_list)            print(-b)        else:            heapq.heappush(h_list,a)1300120201321030201000011286번 절대값 힙https://www.acmicpc.net/problem/11286절대값은 절대값-원래값 쌍으로 저장해주면 절대값 기준으로 첫번째 비교, 원래값으로 두번째 비교가 된다.import heapqfor i in range(int(input())):    a=int(input())    if len(h_list)==0:        if a==0:            print(-a)        else:            heapq.heappush(h_list,(abs(a),a))    else:        if a==0:            b=heapq.heappop(h_list)            print(b[1])        else:            heapq.heappush(h_list,(abs(a),a))181-10-1010011-1-12-20-10-101010-202001715번 카드 정렬하기https://www.acmicpc.net/problem/1715import heapqcard=[]for i in range(int(input())):     a=int(input())    heapq.heappush(card,a)result=0heapq.heapify(card) # 최소 힙 만들기while len(card)&gt;1: # 카드가 하나 이상일 때  - 적은 수끼리 계속 더하는 것이 가장 적게 걸리는 수        a=heapq.heappop(card) #제일 작은수        b=heapq.heappop(card) #그다음 작은 수        heapq.heappush(card,a+b) #두개의 합        result+=a+b print(result)31020401001766번 문제집https://www.acmicpc.net/problem/1766‘순서가 정해져 있는 작업’을 차례로 수행해야 할 때, 순서를 결정할 때 사용하는 알고리즘입니다.방향 그래프에 존재하는 각 정점들의 선행 순서를 위배하지 않으며 모든 정점을 나열하면 됩니다.하나의 방향 그래프에는 여러 개의 위상 정렬이 가능합니다.위상 정렬을 만들기 위해 problem_list를 이중 리스트로 표현하는 것이 여러개의 우선 순위를 저장하기 좋다고 합니다.3개의 리스트를 만들어서 우선순위 존재 여부 확인, 우선순위에 있는 숫자들을 분리, 결과값 저장에 각각 사용하는 것이 나중에 원하는 값을 추출하기 편했습니다.import heapqproblem, compare = map(int, input().split())problem_list = [[] for i in range(problem + 1)] # 위상정렬 표현 ,index가 헷갈리지 않게 problem+1pre = [0 for i in range(problem + 1)] #1: 우선순위가 있다.heap = [] # 먼저 풀 문제들을 저장result = [] # 결과값을 저장for i in range(compare):    a, b = map(int, input().split())    problem_list[a].append(b) # 위상 정렬 만들기    pre[b] += 1 #우선순위가 있다면 +1    print(problem_list)print(pre)4 24 23 1[[], [], [], [1], [2]][0, 1, 1, 0, 0]for i in range(1, problem + 1):    if pre[i] == 0: #우선순위가 없는 것 먼저 heap에 push        heapq.heappush(heap, i) while heap: # heap에 남은 것이 없을 때까지    temp = heapq.heappop(heap) # 우선순위중 가장 작은 것 먼저 추출    result.append(temp)     for j in problem_list[temp]: # 위상 정렬 만든 것에서  확인        pre[j] -= 1 # 우선순위가 1인 것을 0으로 만들어줌-&gt; 더이상 우선순위가 없다는 의미        if pre[j] == 0: #우선순위가 더이상 없는 제일 작은 수를 push해줌            heapq.heappush(heap, j)for i in result: # 결과값 출력    print(i, end=' ')3 1 4 2 ",
        "url": "/programming-baekjoon9"
    }
    ,
    
    "programming-kaggle4": {
        "title": "캐글 (4) 타이타닉 생존자 앙상블과 스태킹까지",
            "author": "keonju",
            "category": "",
            "content": "kaggle 관련 글    캐글 (1) Simple Matplotlib &amp; Visualization Tips 공부하기    캐글 (2) 타이타닉 튜토리얼 1,2 공부하기    캐글 (3) 타이타닉 생존자 EDA 부터 분류까지    캐글 (4) 타이타닉 생존자 앙상블과 스태킹까지    캐글 (5) 메타 데이터를 이용한 데이터 관찰 및 준비https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python타이타닉 생존자 데이터를 가지고 앙상블과 스태킹을 필사해보았습니다.import pandas as pdimport numpy as npimport reimport sklearnimport xgboost as xgbimport seaborn as snsimport matplotlib.pyplot as plt%matplotlib inlineimport plotlyimport plotly.offline as pypy.init_notebook_mode(connected=True)import plotly.graph_objs as goimport plotly.tools as tlsimport warningswarnings.filterwarnings('ignore')from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,                               GradientBoostingClassifier, ExtraTreesClassifier)from sklearn.svm import SVCfrom sklearn.model_selection import KFold특성 탐색, 특성 공학, 특성 정제데이터를 살펴보고 특성 공학을 통해 범주형 특징을 수치적으로 인코딩합니다.train=pd.read_csv('train.csv')test=pd.read_csv('test.csv')PassengerId=test['PassengerId']train.head()                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      7.2500      NaN      S              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      71.2833      C85      C              2      3      1      3      Heikkinen, Miss. Laina      female      26.0      0      0      STON/O2. 3101282      7.9250      NaN      S              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      35.0      1      0      113803      53.1000      C123      S              4      5      0      3      Allen, Mr. William Henry      male      35.0      0      0      373450      8.0500      NaN      S      full_data=[train, test]# 이름의 길이 구하기train['Name_length']=train['Name'].apply(len)test['Name_length']=test['Name'].apply(len)# cabin 데이터 0, 1로 분류하기train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)# 가족 데이터 구하기for dataset in full_data:    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1# 홀로 탄 사람들과 아닌 사람 1, 0 for dataset in full_data:    dataset['IsAlone'] = 0    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1# Embarked 에서 Null은 'S'로 지정하기for dataset in full_data:    dataset['Embarked'] = dataset['Embarked'].fillna('S')# Fare에서 Null은 평균값으로 채워넣기for dataset in full_data:    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())# Fare 데이터 나누기 (qcut은 동일한 개수로 나눠준다)train['CategoricalFare'] = pd.qcut(train['Fare'], 4)# Age 데이터 수정하기 (cut은 동일한 길이로 나눠준다)for dataset in full_data:    age_avg = dataset['Age'].mean() #평균    age_std = dataset['Age'].std() #표준편차    age_null_count = dataset['Age'].isnull().sum() # Null값 개수    # null값에 특정 값 넣어주기    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list    dataset['Age'] = dataset['Age'].astype(int)train['CategoricalAge'] = pd.cut(train['Age'], 5)# 이름 검색해서 Title로 반환해주기def get_title(name):    title_search = re.search(' ([A-Za-z]+)\\.', name)    if title_search:        return title_search.group(1)    return \"\"for dataset in full_data:    dataset['Title'] = dataset['Name'].apply(get_title)# 틀린 단어들 하나로 통일시키기for dataset in full_data:    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')# 데이터 숫자로 변환하기for dataset in full_data:    # Mapping Sex    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)    # Title 숫자로 바꾸기    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}    dataset['Title'] = dataset['Title'].map(title_mapping)    dataset['Title'] = dataset['Title'].fillna(0)    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)    dataset.loc[ dataset['Fare'] &lt;= 7.91, 'Fare']= 0    dataset.loc[(dataset['Fare'] &gt; 7.91) &amp; (dataset['Fare'] &lt;= 14.454), 'Fare'] = 1    dataset.loc[(dataset['Fare'] &gt; 14.454) &amp; (dataset['Fare'] &lt;= 31), 'Fare']   = 2    dataset.loc[ dataset['Fare'] &gt; 31, 'Fare'] = 3    dataset['Fare'] = dataset['Fare'].astype(int)    dataset.loc[ dataset['Age'] &lt;= 16, 'Age'] = 0    dataset.loc[(dataset['Age'] &gt; 16) &amp; (dataset['Age'] &lt;= 32), 'Age'] = 1    dataset.loc[(dataset['Age'] &gt; 32) &amp; (dataset['Age'] &lt;= 48), 'Age'] = 2    dataset.loc[(dataset['Age'] &gt; 48) &amp; (dataset['Age'] &lt;= 64), 'Age'] = 3    dataset.loc[ dataset['Age'] &gt; 64, 'Age'] = 4# 수정한 column 지우기drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']train = train.drop(drop_elements, axis = 1)train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)test  = test.drop(drop_elements, axis = 1)시각화train.head()                  Survived      Pclass      Sex      Age      Parch      Fare      Embarked      Name_length      Has_Cabin      FamilySize      IsAlone      Title                  0      0      3      1      1      0      0      0      23      0      2      0      1              1      1      1      0      2      0      3      1      51      1      2      0      3              2      1      3      0      1      0      1      0      22      0      1      1      2              3      1      1      0      2      0      3      0      44      1      2      0      3              4      0      3      1      2      0      1      0      24      0      1      1      1      # Heatmap을 통한 피어슨 상관계수 파악colormap = plt.cm.RdBuplt.figure(figsize=(14,12))plt.title('Pearson Correlation of Features', y=1.05, size=15)sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,             square=True, cmap=colormap, linecolor='white', annot=True)&lt;AxesSubplot:title={'center':'Pearson Correlation of Features'}&gt;상관관계가 높은 항들이 많이 있지 않은 것을 알 수 있다.이는 서로 중복되거나 불필요한 데이터가 많이 없다는 뜻이다.g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )g.set(xticklabels=[])&lt;seaborn.axisgrid.PairGrid at 0x2828c74e9a0&gt;앙상블SklearnHelper 클래스를 통해 학습에 필요한 클래스들을 만듭니다.‘init‘에 sklearn에서 제공하는 학습 방법을 넣을 수 있습니다.또한 우리가 알고싶은 매개변수를 더 추가할 수 있습니다.ntrain = train.shape[0]ntest = test.shape[0]SEED = 0NFOLDS = 5kf = KFold(n_splits= NFOLDS, shuffle=True ,random_state=SEED)class SklearnHelper(object):    def init(self, clf, seed=0, params=None):        params[‘random_state’] = seed        self.clf = clf(**params)def train(self, x_train, y_train):    self.clf.fit(x_train, y_train)def predict(self, x):    return self.clf.predict(x)def fit(self,x,y):    return self.clf.fit(x,y)def feature_importances(self,x,y):    print(self.clf.fit(x,y).feature_importances_)def get_oof(clf, x_train, y_train, x_test):        oof_train = np.zeros((ntrain,))    oof_test = np.zeros((ntest,))    oof_test_skf = np.empty((NFOLDS, ntest))    for i, (train_index, test_index) in enumerate(kf.split(train)):        x_tr = x_train[train_index]        y_tr = y_train[train_index]        x_te = x_train[test_index]        clf.train(x_tr, y_tr)        oof_train[test_index] = clf.predict(x_te)        oof_test_skf[i, :] = clf.predict(x_test)    oof_test[:] = oof_test_skf.mean(axis=0)    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)첫번째 레벨 모델 생성  Random Forest classifier  Extra Trees classifier  AdaBoost classifer  Gradient Boosting classifer  Support Vector Machineparametern_jobs: 훈련 과정의 코어 개수n_estimators: 학습 모델 안의 분류 트리 개수max_depth: 최대 깊이verbose: 학습 과정 중에 텍스트 출력 여부rf_params = {    'n_jobs' : -1,    'n_estimators' : 500,    'warm_start' : True,    'max_depth' : 6,    'min_samples_leaf' : 2,    'max_features' : 'sqrt',    'verbose' : 0}et_params = {    'n_jobs' : -1,    'n_estimators' : 500,    'max_depth' : 8,    'min_samples_leaf' : 2,    'verbose' : 0}ada_params = {    'n_estimators' : 500,    'learning_rate' : 0.75}gb_params = {    'n_estimators' : 500,    'max_depth' : 5,    'min_samples_leaf' : 2,    'verbose' : 0}svc_params = {    'kernel' : 'linear',    'C' : 0.025}rf = SklearnHelper(clf = RandomForestClassifier,seed = SEED,params = rf_params)et = SklearnHelper(clf = ExtraTreesClassifier,seed = SEED,params = et_params)ada = SklearnHelper(clf = AdaBoostClassifier,seed = SEED,params = ada_params)gb = SklearnHelper(clf = GradientBoostingClassifier,seed = SEED,params = gb_params)svc = SklearnHelper(clf = SVC,seed = SEED,params = svc_params)# Numpy 배열 만들기 (ravel은 평평하게 만들어주는 함수)y_train = train['Survived'].ravel()train = train.drop(['Survived'], axis=1)x_train = train.values x_test = test.valueset_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) print(\"Training is complete\")Training is completerf_feature = rf.feature_importances(x_train,y_train)et_feature = et.feature_importances(x_train, y_train)ada_feature = ada.feature_importances(x_train, y_train)gb_feature = gb.feature_importances(x_train,y_train)[0.10435218 0.2102886  0.03567688 0.01960763 0.04623086 0.02985483 0.13117994 0.04982096 0.07059698 0.01160042 0.29079073][0.11874701 0.38179518 0.0293706  0.01702077 0.05590766 0.02766201 0.0468396  0.08340196 0.04583293 0.02149851 0.17192377][0.03  0.01  0.016 0.062 0.04  0.01  0.692 0.012 0.056 0.002 0.07 ][0.08808198 0.0160359  0.04796194 0.01384621 0.05556242 0.0240539 0.17261786 0.03568649 0.11134606 0.00579069 0.42901655]rf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]et_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]ada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]gb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854  ,0.05965792 , 0.02774745,  0.07462718,  0.4593142 ,  0.01340093]cols = train.columns.valuesfeature_dataframe = pd.DataFrame( {'features': cols,     'Random Forest feature importances': rf_features,     'Extra Trees  feature importances': et_features,      'AdaBoost feature importances': ada_features,    'Gradient Boost feature importances': gb_features    })trace = go.Scatter(    y = feature_dataframe['Random Forest feature importances'].values,    x = feature_dataframe['features'].values,    mode='markers',    marker=dict(        sizemode = 'diameter',        sizeref = 1,        size = 25,        color = feature_dataframe['Random Forest feature importances'].values,        colorscale='Portland',        showscale=True    ),    text = feature_dataframe['features'].values)data = [trace]layout= go.Layout(    autosize= True,    title= 'Random Forest Feature Importance',    hovermode= 'closest',    yaxis=dict(        title= 'Feature Importance',        ticklen= 5,        gridwidth= 2    ),    showlegend= False)fig = go.Figure(data=data, layout=layout)py.iplot(fig,filename='scatter2010')trace = go.Scatter(    y = feature_dataframe['Extra Trees  feature importances'].values,    x = feature_dataframe['features'].values,    mode='markers',    marker=dict(        sizemode = 'diameter',        sizeref = 1,        size = 25,        color = feature_dataframe['Extra Trees  feature importances'].values,        colorscale='Portland',        showscale=True    ),    text = feature_dataframe['features'].values)data = [trace]layout= go.Layout(    autosize= True,    title= 'Extra Trees Feature Importance',    hovermode= 'closest',    yaxis=dict(        title= 'Feature Importance',        ticklen= 5,        gridwidth= 2    ),    showlegend= False)fig = go.Figure(data=data, layout=layout)py.iplot(fig,filename='scatter2010')trace = go.Scatter(    y = feature_dataframe['AdaBoost feature importances'].values,    x = feature_dataframe['features'].values,    mode='markers',    marker=dict(        sizemode = 'diameter',        sizeref = 1,        size = 25,        color = feature_dataframe['AdaBoost feature importances'].values,        colorscale='Portland',        showscale=True    ),    text = feature_dataframe['features'].values)data = [trace]layout= go.Layout(    autosize= True,    title= 'AdaBoost Feature Importance',    hovermode= 'closest',    yaxis=dict(        title= 'Feature Importance',        ticklen= 5,        gridwidth= 2    ),    showlegend= False)fig = go.Figure(data=data, layout=layout)py.iplot(fig,filename='scatter2010')trace = go.Scatter(    y = feature_dataframe['Gradient Boost feature importances'].values,    x = feature_dataframe['features'].values,    mode='markers',    marker=dict(        sizemode = 'diameter',        sizeref = 1,        size = 25,        color = feature_dataframe['Gradient Boost feature importances'].values,        colorscale='Portland',        showscale=True    ),    text = feature_dataframe['features'].values)data = [trace]layout= go.Layout(    autosize= True,    title= 'Gradient Boosting Feature Importance',    hovermode= 'closest',    yaxis=dict(        title= 'Feature Importance',        ticklen= 5,        gridwidth= 2    ),    showlegend= False)fig = go.Figure(data=data, layout=layout)py.iplot(fig,filename='scatter2010')두번째 예측 진행하기base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),     'ExtraTrees': et_oof_train.ravel(),     'AdaBoost': ada_oof_train.ravel(),      'GradientBoost': gb_oof_train.ravel()    })base_predictions_train.head()                  RandomForest      ExtraTrees      AdaBoost      GradientBoost                  0      0.0      0.0      0.0      0.0              1      1.0      1.0      1.0      1.0              2      1.0      0.0      1.0      1.0              3      1.0      1.0      1.0      1.0              4      0.0      0.0      0.0      0.0      data = [    go.Heatmap(        z= base_predictions_train.astype(float).corr().values ,        x=base_predictions_train.columns.values,        y= base_predictions_train.columns.values,          colorscale='Viridis',            showscale=True,            reversescale = True    )]py.iplot(data, filename='labelled-heatmap')x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)gbm = xgb.XGBClassifier(    #learning_rate = 0.02, n_estimators= 2000, max_depth= 4, min_child_weight= 2, #gamma=1, gamma=0.9,                         subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1).fit(x_train, y_train)predictions = gbm.predict(x_test)StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,                            'Survived': predictions })StackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)",
        "url": "/programming-kaggle4"
    }
    ,
    
    "study-ml6": {
        "title": "머신러닝 정리 (6) &lt;br&gt; 모델 평가와 성능 향상",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학    머신러닝 정리 (6)-모델 평가와 성능 향상  모델 평가와 성능 향상          교차 검증        그리드 서치  평가 지표와 측정          이진 분류의 평가 지표      머신러닝 정리 (6) 모델 평가와 성능 향상본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python모델 평가와 성능 향상분류에서 score 메소드를 통해 얼마나 정확히 분류된 샘플의 비율을 알 수 있습니다.이번 단원에서는 일반화 성능 측정 방법인 교차 검증과 score 메소드가 제공하는 정확도와 R^2값 이외에 분류와 회귀 성능을 측정하는 다른 방법 그리고 지도 학습 모델의 매개변수를 조정하는 데 유용한 그리드 서치에 관해서 학습하겠습니다.교차 검증교차 검증은 훈련 세트와 테스트 세트로 나누는 것보다 더 안정적인 평가 방법입니다.데이터를 여러번 반복해 나누고 여러 모델을 학습합니다.k-fold 교차검증은 k라는 fold를 지정해 거의 비슷한 크기를 가진 부분 집합 k개로 나눕니다.첫 번째 폴드를 테스트 세트로 사용하고 나머지 폴드를 훈련 세트로 사용하여 학습합니다.이렇게 k 번을 반복하여 분할마다 정확도를 측정하여 k개의 정확도를 알 수 있습니다.scikit-learn 교차 검증model_selection 모듈의 cross_val_score 함수로 구현됩니다.cross_val_score 함수의 매개변수는 평가하려는 모델과 훈련 데이터, 타깃 레이블입니다.from sklearn.model_selection import cross_val_scorefrom sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionscores = cross_val_score(logreg, iris.data, iris.target, cv=10)다음과 같은 방식으로 평가할 수 있습니다.cv 매개변수는 fold의 개수를 지정할 수 있습니다.정확도를 간단하게 나타내려면 평균을 사용하면 됩니다.모델이 폴드에 매우 의존적이거나 데이터셋이 작으면 정확도의 차이가 커집니다.cross_validate는 분할마다 훈련과 테스트에 걸린 시간을 담은 딕셔너리를 반환하고 테스트 점수와 훈련 점수도 얻을 수 있습니다.교차 검증의 장단점train_test_split은 상황에 따라 테스트 세트는 너무 쉬운 데이터만 들어갈 수 있지만 교차 검증을 사용하면 각 샘플에 정확하게 한 번씩 들어가기 때문에 한 번씩은 테스트 세트가 될 수 있습니다.또한 훈련 데이터에 얼마나 민감한지 알 수 있으며 train_data에 더 많은 데이터를 학습할 수 있습니다.하지만 모델을 k개 만들어서 학습하므로 연산 비용이 k배 늘어나게 됩니다.계층별 k-fold 교차 검증과 그 외 전략k-fold는 한 분할에 같은 데이터가 몰리게 된다면 분할에 따라 너무 높거나 너무 낮게 나올 수 있습니다.따라서 stratified k-fold를 통해 폴드 안의 클래스 비율이 전체 데이터 셋의 클래스 비율과 같도록 데이터를 나눌 수 있습니다.교차 검증 상세 옵션from sklearn.model_selection import KFoldkfold = KFold(n_splits=5)cross_val_score(logreg, iris.data, iris.target, cv=kfold)앞서 cv에 fold 개수를 지정하지만 scikit-learn에서는 KFold(n_splits=n)을 통해서 fold의 개수를 지정해줍니다.KFold의 shuffle 매개변수를 True로 만들어 데이터를 섞어서 샘플의 순서를 바꿀 수 있습니다.random_state를 통해 똑같으 작업을 재현할 수 있으며 사용하지 않는다면 매번 다른 결과가 나옵니다.LOOCV (Leave-One-Out cross-validation)폴드 하나에 샘플 하나만 들어 있는 교차 검증으로 생각할 수 있습니다.각 반복에서 하나의 데이터 포인트를 선택해 테스트 세트로 사용합니다.from sklearn.model_selection import LeaveOneOutloo = LeaveOneOut()scores = cross_val_score(logreg, iris.data, iris.target, cv=loo)임의 분할 교차 검증train_size만큼의 포인트로 훈련 세트를 만들고 훈련 세트와 중첩되지 않은 test_Size만큼의 포인트로 테스트 세트를 만들도록 분할합니다.n_splits 만큼 반복됩니다.따라서 훈련 세트나 테스트 세트의 크기와 독립적으로 조절할 때 유용합니다.또한 데이터의 일부만 사용하게 되기 때문에 부분 샘플링 하게 됩니다.계층별 버전으로 사용한다면 StratifiedShuffleSplit을 사용하면 됩니다.from sklearn.model_selection import ShuffleSplitshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)scores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)그룹별 교차 검증Groups를 통해 훈련 세트와 테스트 세트에서 분리되지 않아야할 그룹을 지정할 수 있습니다.from sklearn.model_selection import GroupKFoldgroups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]scores = cross_val_score(logreg, X, y, groups=groups, cv=GroupKFold(n_splits=3))반복 교차 검증RepeatedKFold, RepeatedStratifiedKFlold를 통해 반복 교차 검증을 할 수 있습니다.회귀에서는 RepeatedKFold, 분류에서는 RepeatedStratifiedKFold를 사용합니다.KFold와 StratifiedKFold를 통해 분할합니다.n_splits를 통해 분할 폴드 수를, n_repeats를 통해 반복 횟수를 지정합니다.from sklearn.model_selsection import RepeatedStratifiedKFoldrskfold=RepeatedStratifiedKFold(random_state=42)그리드 서치매개변수를 튜닝하여 일반화 성능을 개선합니다.관심있는 매개변수를 대상으로 가능한 모든 조합을 시도해보는 것입니다.매개변수를 조정하기 위해 사용한 테스트 세트가 아닌 새로운 테스트 세트를 만들어 모델을 평가해야 합니다.따라서 처음에 훈련 세트를 통해 모델 순련, 검증 세트를 통해 매개변수 선택, 테스트 세트를 통해 모델 평가를 하는 방법을 사용합니다.매개변수를 선택한 후 훈련 세트와 검증 세트를 합해 모델을 다시 만듭니다.그리드 서치에서도 교차 검증을 이용해 각 매개변수 조합의 성능을 평가할 수 있습니다.best_params_를 통해 선택한 매개변수를 알 수 있습니다.best_score_을 통해 최상의 교차 검증 정확도를 알 수 있습니다.best_estimator_을 통해서 속성을 얻을 수 있습니다.교차 검증의 결과는 cv_results_ 속성에 담겨 있습니다.from sklearn.model_selection import GridSearchCVfrom sklearn.svm import SVCgrid_search = GridSearchCV(SVC(), param_grid, cv=5, return_train_score=True)중첩 교차 검증원본 데이터를 교차 검증 분할 방식을 사용하여 나누는 것을 중첩 교차 검증이라고 합니다.바깥쪽 루프에서 데이터를 훈련 세트와 테스트 세트로 나눈 뒤 각 훈련 세트에 대해 그리드 서치를 실행합니다.그 다음 분할된 테스트 세트의 점수를 최적 매개변수 설정을 사용해 각각 측정합니다.따라서 테스트 점수 목록을 만들어줍니다.최적 매개변수가 모델을 얼마나 잘 일반화 시키는지 확인하기 위한 방법입니다.그리드 서치는 n_jobs를 통해 병렬화하여 연산을 빠르게 할 수 있습니다.평가 지표와 측정이진 분류의 평가 지표정확도가 높은 분류가 반드시 좋은 분류라고 할 수 없습니다.어떤 경우에는 1종오류가 없어야하는 경우도 있습니다. (암 통계에서의 거짓 음성)불균형 데이터셋한 클래스가 다른 것보다 훨씬 많은 데이터셋오차 행렬classfication_report를 통해서 여러가지 오차 행렬 결과 요약을 볼 수 있습니다.각 분석에 중요성에 따라 사용하는 성능 지표가 달라집니다.정밀도-재현율 곡선과 ROC 곡선문제를 더 잘 이해하기 위해 정밀도-재현율 곡선을 사용합니다.from sklearn.metrics import precision_recall_curveprecision, recall, thresholds = precision_recall_curve(    y_test, svc.decision_function(X_test))곡선이 오른쪽 위로 갈수록 좋은 분류기입니다.오른 쪽 위 지점은 한 임계값에서 정밀도와 재현율이 모두 높은 곳입니다.임계값이 높을수록 정밀도는 높아지는 쪽으로 이동하고 재현율은 낮아집니다.f1_score은 정밀도-재현율 곡선의 기본 임계값에 대한 점수입니다.ROC 곡선TPR과 FPR을 나타냅니다.TPR은 재현율을 나타냅니다.ROC 곡선은 왼쪽 위에 가까울 수록 이상적입니다.FPR이 낮게 유지되면서 재현율이 높은 분류기가 좋은 것입니다.곡선 아래 면적을 AUC라고 하며 roc_auc_score 함수로 계산합니다.다중 분류의 평가 지표클래스가 불균형할 때는 좋은 평가 방법이 되지 못합니다.다중 클래스용 f1-score이 있습니다.한 클래스를 양성으로 잡고 나머지 클래스는 음성으로 간주하고 f1-score을 계산합니다.‘macro’ 평균은 클래스별 f1-score에 가중치를 주지않습니다.‘weighted’ 평균은 클래스별 샘플 수로 가중치를 두어 f1-score 점수의 평균을 계싼합니다.‘micro’평균은 모든 클래스의 FP, FN, TP의 총 수를 파악한 뒤 정밀도, 재현율, f1-score로 이 수치를 계산합니다.회귀의 평가 지표회귀에서는 R^2로 평가하는 것이 가장 좋습니다.",
        "url": "/study-ML6"
    }
    ,
    
    "programming-baekjoon8": {
        "title": "백준 (8) &lt;br&gt; (10845,1158,1966,2164, &lt;br&gt; 11866,18258)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)    백준 (9) 우선순위 큐 알고리즘 (1927,11279,11286,1715,11766)큐에 관련된 6문제를 풀어보았다.https://www.acmicpc.net/problemset?sort=ac_desc&amp;algo=723190번 뱀을 풀지 못했다…큐란?큐는 선입선출의 개념이다.10845번 큐https://www.acmicpc.net/problem/10845이전에 풀었던 스택에서와 유사한 문제이다.먼저 명령어를 입력받았고 그 명령에 ‘push’가 있다면 띄어쓰기를 기준으로 나누어 que_list에 추가해주었다.제일 앞에 숫자를 추출하는 것은 list의 index를 사용하였다.que_list=[]for i in range(int(input())):    command=input()    if 'push' in command:        a,b=command.split()        que_list.append(b)    if 'front' in command:        if len(que_list)==0:            print('-1')        else:            print(que_list[0])    if 'back' in command:        if len(que_list)==0:            print('-1')        else:            print(que_list[-1])    if 'pop' in command:        if len(que_list)==0:            print('-1')        else:            a=que_list[0]            que_list.remove(a)            print(a)    if 'size' in command:        print(len(que_list))    if 'empty' in command:        if len(que_list)==0:            print('1')        else:            print('0')15push 1push 2front1back2size2empty0pop1pop2pop-1size0empty1pop-1push 3empty0front31158번 요세푸스 문제https://www.acmicpc.net/problem/1158먼저 N개의 숫자가 들어가있는 que 리스트를 만들고 que에서 빠질 숫자들을 저장하는 result 리스트를 만들었습니다.num을 통해 que의 인덱싱을 조절해줍니다.하나의 숫자가 que에서 빠지면 뒤에 숫자가 그 빠진 위치에 들어오기 때문에 num은 K-1 만큼 더해줬습니다.num이 que의 인덱스 범위에 벗어나게 되면 처음으로 돌아가야하고 그 숫자는 num/len(que)의 나머지가 됩니다.그렇게 반복문으로 나온 순서대로 result에 append 시켜주면 문제는 해결됩니다.마지막으로 ‘[]’가 아닌 ‘&lt;&gt;’로 묶여있기 때문에 리스트 전체를 문자열로 만들어서 ‘[]’를 ‘&lt;&gt;’로 replace를 통해 바꿔주었습니다.N,K=map(int,input().split())result=[]num=0que=[]for i in range(1,N+1):    que.append(i)for i in range(N):    num=num+K-1    if num &gt;=len(que):        num=num%len(que)    result.append(que.pop(num))result=str(result)result=result.replace('[','&lt;')result=result.replace(']','&gt;')print(result)7 3&lt;3, 6, 2, 7, 5, 1, 4&gt;1966번 프린터 큐https://www.acmicpc.net/problem/1966목표하는 index 값을 ‘target’으로 바꿔서 처리하기 수월했다.pop으로 숫자를 꺼내서 뒤에 append를 추가하였다.test_cases = int(input())for _ in range(test_cases):    n,m = list(map(int, input().split( )))    prior = list(map(int, input().split( )))    index = list(range(len(prior)))    index[m] = 'target'    order = 0        while True:        if prior[0]==max(prior):            order += 1            if index[0]=='target':                print(order)                break            else:                prior.pop(0)                index.pop(0)        else:            prior.append(prior.pop(0))            index.append(index.pop(0))31 0514 21 2 3 426 01 1 9 1 1 152164번 카드 2https://www.acmicpc.net/problem/2164deque를 이용해서 한번은 숫자를 없애고 한번은 뒤로 append해주고 남은 하나의 숫자를 출력하면 된다.import sysfrom collections import dequecard=deque()for i in range(int(input())):    card.append(i+1)    while len(card) != 1:    card.popleft()    a=card.popleft()    card.append(a)print(card[0])6411866번 요세푸스 문제 0https://www.acmicpc.net/problem/11866위에 문제와 같이 풀어도 됐다…N,K=map(int,input().split())result=[]num=0que=[]for i in range(1,N+1):    que.append(i)for i in range(N):    num=num+K-1    if num &gt;=len(que):        num=num%len(que)    result.append(que.pop(num))result=str(result)result=result.replace('[','&lt;')result=result.replace(']','&gt;')print(result)7 3&lt;3, 6, 2, 7, 5, 1, 4&gt;3190번 뱀https://www.acmicpc.net/problem/3190n=int(input())board=[[0]*n for i in range(n)]print(board)6[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]K=int(input())for i in range(K):    a,b=map(int,input().split())    board[a-1][b-1]=1print(board)33 42 55 3[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0]]18258번 큐 2https://www.acmicpc.net/problem/18258위에서 list와 인덱싱을 통해 처리했던 문제를 deque를 통해서 시간을 절약하는 문제이다.import sysfrom collections import dequeque_list=deque()for i in range(int(input())):    command=input()    if 'push' in command:        a,b=command.split()        que_list.append(b)    if 'front' in command:        if len(que_list)==0:            print('-1')        else:            print(que_list[0])    if 'back' in command:        if len(que_list)==0:            print('-1')        else:            print(que_list[-1])    if 'pop' in command:        if len(que_list)==0:            print('-1')        else:            print(que_list.popleft())    if 'size' in command:        print(len(que_list))    if 'empty' in command:        if len(que_list)==0:            print('1')        else:            print('0')15push 1push 2front1back2size2empty0pop1pop2pop-1size0empty1pop-1push 3empty0front3",
        "url": "/programming-baekjoon8"
    }
    ,
    
    "programming-kaggle3": {
        "title": "캐글 (3) &lt;br&gt; 타이타닉 생존자 EDA 부터 분류까지",
            "author": "keonju",
            "category": "",
            "content": "kaggle 관련 글    캐글 (1) Simple Matplotlib &amp; Visualization Tips 공부하기    캐글 (2) 타이타닉 튜토리얼 1,2 공부하기    캐글 (3) 타이타닉 생존자 EDA 부터 분류까지    캐글 (4) 타이타닉 생존자 앙상블과 스태킹까지    캐글 (5) 메타 데이터를 이용한 데이터 관찰 및 준비https://www.kaggle.com/ash316/eda-to-prediction-dietanic/notebook캐글의 타이타닉 데이터를 통한 EDA부터 예측까지 필사를 진행해보았습니다.데이터 탐색import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsplt.style.use('fivethirtyeight')import warningswarnings.filterwarnings('ignore')%matplotlib inlinedata=pd.read_csv('train.csv')data.head()                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      7.2500      NaN      S              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      71.2833      C85      C              2      3      1      3      Heikkinen, Miss. Laina      female      26.0      0      0      STON/O2. 3101282      7.9250      NaN      S              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      35.0      1      0      113803      53.1000      C123      S              4      5      0      3      Allen, Mr. William Henry      male      35.0      0      0      373450      8.0500      NaN      S      data.isnull().sum()PassengerId      0Survived         0Pclass           0Name             0Sex              0Age            177SibSp            0Parch            0Ticket           0Fare             0Cabin          687Embarked         2dtype: int64얼마나 많은 사람들이 살아남았나?f,ax=plt.subplots(1,2,figsize=(18,8))data['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)ax[0].set_title('Survived')ax[0].set_ylabel('')sns.countplot('Survived',data=data,ax=ax[1])ax[1].set_title('Survived')plt.show()train_set에서 891명의 승객 중, 350명이 살아남았습니다.  범주형 변수Sex, Embarked  순서형 변수PClass  연속형 변수Age특징 분석# Sex -&gt; Categorical Featuredata.groupby(['Sex','Survived'])['Survived'].count()Sex     Survivedfemale  0            81        1           233male    0           468        1           109Name: Survived, dtype: int64f,ax=plt.subplots(1,2,figsize=(18,8))data[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])ax[0].set_title('Survived vs Sex')sns.countplot('Sex',hue='Survived',data=data,ax=ax[1])ax[1].set_title('Sex:Survived vs Dead')plt.show()&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;배에 탄 남자의 수가 여자의 수보다 많지만 생존율은 여성이 훨씬 높습니다.# Pclass -&gt; Ordinal Featurepd.crosstab(data.Pclass,data.Survived,margins=True).style.background_gradient(cmap='summer_r')            Survived        0        1        All                Pclass                                                                    1                        80                        136                        216                                                2                        97                        87                        184                                                3                        372                        119                        491                                                All                        549                        342                        891                f,ax=plt.subplots(1,2,figsize=(18,8))data['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])ax[0].set_title('Number Of Passengers By Pclass')ax[0].set_ylabel('Count')sns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])ax[1].set_title('Pclass:Survived vs Dead')plt.show()Pclass=3인 승객 수가 훨씬 많았지만 생존율은 매우 낮은 모습을 보이고 있습니다.따라서 부유한 사람의 생존율이 더 높다고 할 수 있습니다.pd.crosstab([data.Sex,data.Survived],data.Pclass,margins=True).style.background_gradient(cmap='summer_r')                    Pclass        1        2        3        All                Sex        Survived                                                                            female                        0                        3                        6                        72                        81                                                        1                        91                        70                        72                        233                                                male                        0                        77                        91                        300                        468                                                        1                        45                        17                        47                        109                                                All                                                216                        184                        491                        891                sns.factorplot('Pclass','Survived',hue='Sex',data=data)plt.show()CrossTab과 FactorPlot을 통해 성별과 Pclass간의 관계를 볼 수 있습니다.# Age--&gt; Continous Featureprint('Oldest Passenger was of:',data['Age'].max(),'Years')print('Youngest Passenger was of:',data['Age'].min(),'Years')print('Average Age on the ship:',data['Age'].mean(),'Years')Oldest Passenger was of: 80.0 YearsYoungest Passenger was of: 0.42 YearsAverage Age on the ship: 29.69911764705882 Yearsf,ax=plt.subplots(1,2,figsize=(18,8))sns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[0])ax[0].set_title('Pclass and Age vs Survived')ax[0].set_yticks(range(0,110,10))sns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[1])ax[1].set_title('Sex and Age vs Survived')ax[1].set_yticks(range(0,110,10))plt.show()Pclass에 따라 연령대의 분포를 볼 수 있습니다.Sex와 Age의 관계를 본다면 연령층이 낮으면 성별과 상관없이 우수한 생존율을 볼 수 있습니다.# 정규식을 통한 Initial 추출data['Initial']=0for i in data:    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.')pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r')            Initial        Capt        Col        Countess        Don        Dr        Jonkheer        Lady        Major        Master        Miss        Mlle        Mme        Mr        Mrs        Ms        Rev        Sir                Sex                                                                                                                                                                                    female                        0                        0                        1                        0                        1                        0                        1                        0                        0                        182                        2                        1                        0                        125                        1                        0                        0                                                male                        1                        2                        0                        1                        6                        1                        0                        2                        40                        0                        0                        0                        517                        0                        0                        6                        1                Miss, Mr뿐만 아니라 다른 명칭으로 적힌 사람들이 많다.따라서 이에 대한 처리가 필요하다.data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)data.groupby('Initial')['Age'].mean()InitialMaster     4.574167Miss      21.860000Mr        32.739609Mrs       35.981818Other     45.888889Name: Age, dtype: float64data.loc[(data.Age.isnull())&amp;(data.Initial=='Mr'),'Age']=33data.loc[(data.Age.isnull())&amp;(data.Initial=='Mrs'),'Age']=36data.loc[(data.Age.isnull())&amp;(data.Initial=='Master'),'Age']=5data.loc[(data.Age.isnull())&amp;(data.Initial=='Miss'),'Age']=22data.loc[(data.Age.isnull())&amp;(data.Initial=='Other'),'Age']=46data.Age.isnull().any()Falsef,ax=plt.subplots(1,2,figsize=(20,10))data[data['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')ax[0].set_title('Survived= 0')x1=list(range(0,85,5))ax[0].set_xticks(x1)data[data['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')ax[1].set_title('Survived= 1')x2=list(range(0,85,5))ax[1].set_xticks(x2)plt.show()연령의 분포를 확인할 수 있으며 가장 많은 사망자를 가진 연령층도 파악할 수 있습니다.sns.factorplot('Pclass','Survived',col='Initial',data=data)plt.show()# Embarked--&gt; Categorical Valuepd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=True).style.background_gradient(cmap='summer_r')                    Sex        female        male        All                        Survived        0        1        0        1                        Embarked        Pclass                                                                                    C                        1                        1                        42                        25                        17                        85                                                        2                        0                        7                        8                        2                        17                                                        3                        8                        15                        33                        10                        66                                                Q                        1                        0                        1                        1                        0                        2                                                        2                        0                        2                        1                        0                        3                                                        3                        9                        24                        36                        3                        72                                                S                        1                        2                        46                        51                        28                        127                                                        2                        6                        61                        82                        15                        164                                                        3                        55                        33                        231                        34                        353                                                All                                                81                        231                        468                        109                        889                # Chances for Survival by Port Of Embarkation¶sns.factorplot('Embarked','Survived',data=data)fig=plt.gcf()fig.set_size_inches(5,3)plt.show()f,ax=plt.subplots(2,2,figsize=(20,15))sns.countplot('Embarked',data=data,ax=ax[0,0])ax[0,0].set_title('No. Of Passengers Boarded')sns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])ax[0,1].set_title('Male-Female Split for Embarked')sns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])ax[1,0].set_title('Embarked vs Survived')sns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])ax[1,1].set_title('Embarked vs Pclass')plt.subplots_adjust(wspace=0.2,hspace=0.5)plt.show()S에서 탑승한 사람이 가장 많습니다.그리고 이 사람들은 대부분 3 Class인 것을 확인할 수 있습니다.sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data)plt.show()PClass와 Embarked에 따른 생존률을 파악할 수 있습니다.data['Embarked'].fillna('S',inplace=True)data.Embarked.isnull().any()False# SibSip--&gt;Discrete Featurepd.crosstab([data.SibSp],data.Survived).style.background_gradient(cmap='summer_r')            Survived        0        1                SibSp                                                            0                        398                        210                                                1                        97                        112                                                2                        15                        13                                                3                        12                        4                                                4                        15                        3                                                5                        5                        0                                                8                        7                        0                sns.barplot('SibSp','Survived',data=data)&lt;AxesSubplot:xlabel='SibSp', ylabel='Survived'&gt;sns.factorplot('SibSp','Survived',data=data)&lt;seaborn.axisgrid.FacetGrid at 0x2b48e376fa0&gt;pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap='summer_r')            Pclass        1        2        3                SibSp                                                                    0                        137                        120                        351                                                1                        71                        55                        83                                                2                        5                        8                        15                                                3                        3                        1                        12                                                4                        0                        0                        18                                                5                        0                        0                        5                                                8                        0                        0                        7                가족 수가 많은 class는 3 class인 것을 알 수 있으며 가족 수가 적을수록 더 좋은 생존률을 볼 수 있습니다.Parchpd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r')            Pclass        1        2        3                Parch                                                                    0                        163                        134                        381                                                1                        31                        32                        55                                                2                        21                        16                        43                                                3                        0                        2                        3                                                4                        1                        0                        3                                                5                        0                        0                        5                                                6                        0                        0                        1                sns.barplot('Parch','Survived',data=data)&lt;AxesSubplot:xlabel='Parch', ylabel='Survived'&gt;sns.factorplot('Parch','Survived',data=data)&lt;seaborn.axisgrid.FacetGrid at 0x2b48ea62160&gt;# Fare--&gt; Continous Featureprint('Highest Fare was:',data['Fare'].max())print('Lowest Fare was:',data['Fare'].min())print('Average Fare was:',data['Fare'].mean())Highest Fare was: 512.3292Lowest Fare was: 0.0Average Fare was: 32.2042079685746f,ax=plt.subplots(1,3,figsize=(20,8))sns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])ax[0].set_title('Fares in Pclass 1')sns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])ax[1].set_title('Fares in Pclass 2')sns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])ax[2].set_title('Fares in Pclass 3')plt.show()Pclass에서는 1등석 승객이 되면 생존 확률이 높은 것을 알 수 있습니다.나이는 어리면 생존율이 높다는 것을 알 수 있습니다.가족이 있다면 적은 가족일 때 생존율이 높은 것을 알 수 있습니다.특징들간의 상관관계sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()--&gt;correlation matrixfig=plt.gcf()fig.set_size_inches(10,8)plt.show()히트맵에서 상관관계를 볼 수 있습니다.양의 상관 관계와 음의 상관 관계가 있습니다.상관 관계가 높다면 둘은 multicolinearity라고 불리는 거의 동일한 정보를 포함하고 있다는 것을 알 수 있습니다.여기서는 SibSp와 Parch가 가장 높지만 0.41이기 때문에 multicolinearity가 없다고 할 수 있습니다.데이터 클리닝데이터를 모두 중요하게 사용할 필요는 없기 때문에 관찰을 통해 새로운 데이터로 변경하는 것을 할 수 있습니다.data['Age_band']=0data.loc[data['Age']&lt;=16,'Age_band']=0data.loc[(data['Age']&gt;16)&amp;(data['Age']&lt;=32),'Age_band']=1data.loc[(data['Age']&gt;32)&amp;(data['Age']&lt;=48),'Age_band']=2data.loc[(data['Age']&gt;48)&amp;(data['Age']&lt;=64),'Age_band']=3data.loc[data['Age']&gt;64,'Age_band']=4data.head(2)                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked      Initial      Age_band                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      7.2500      NaN      S      Mr      1              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      71.2833      C85      C      Mrs      2      data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')                    Age_band                                            1                        382                                                2                        325                                                0                        104                                                3                        69                                                4                        11                sns.factorplot('Age_band','Survived',data=data,col='Pclass')plt.show()가족 수와 홀로 온 사람들 사이의 생존율을 볼 수 있습니다.data['Family_Size']=0data['Family_Size']=data['Parch']+data['SibSp']#family sizedata['Alone']=0data.loc[data.Family_Size==0,'Alone']=1sns.factorplot('Family_Size','Survived',data=data)&lt;seaborn.axisgrid.FacetGrid at 0x2b48f8cf790&gt;sns.factorplot('Alone','Survived',data=data)&lt;seaborn.axisgrid.FacetGrid at 0x2b48e966b80&gt;sns.factorplot('Alone','Survived',data=data,hue='Sex',col='Pclass')plt.show()Fare_Range 범주화data['Fare_Range']=pd.qcut(data['Fare'],4)data.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')                    Survived                Fare_Range                                                    (-0.001, 7.91]                        0.197309                                                (7.91, 14.454]                        0.303571                                                (14.454, 31.0]                        0.454955                                                (31.0, 512.329]                        0.581081                data['Fare_cat']=0data.loc[data['Fare']&lt;=7.91,'Fare_cat']=0data.loc[(data['Fare']&gt;7.91)&amp;(data['Fare']&lt;=14.454),'Fare_cat']=1data.loc[(data['Fare']&gt;14.454)&amp;(data['Fare']&lt;=31),'Fare_cat']=2data.loc[(data['Fare']&gt;31)&amp;(data['Fare']&lt;=513),'Fare_cat']=3sns.factorplot('Fare_cat','Survived',data=data,hue='Sex')plt.show()문자열을 숫자로 범주화하기data['Sex'].replace(['male','female'],[0,1],inplace=True)data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)data['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})fig=plt.gcf()fig.set_size_inches(18,15)plt.xticks(fontsize=14)plt.yticks(fontsize=14)plt.show()예측 모델 만들기from sklearn.linear_model import LogisticRegressionfrom sklearn import svmfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.naive_bayes import GaussianNB from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_splitfrom sklearn import metrics # 정확도 확인from sklearn.metrics import confusion_matrixtrain,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])train_X=train[train.columns[1:]]train_Y=train[train.columns[:1]]test_X=test[test.columns[1:]]test_Y=test[test.columns[:1]]X=data[data.columns[1:]]Y=data['Survived']SVMmodel=svm.SVC(kernel='rbf',C=1,gamma=0.1)model.fit(train_X,train_Y)prediction1=model.predict(test_X)print('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))Accuracy for rbf SVM is  0.835820895522388Linear Support Vector Machinemodel=svm.SVC(kernel='linear',C=0.1,gamma=0.1)model.fit(train_X,train_Y)prediction2=model.predict(test_X)print('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))Accuracy for linear SVM is 0.8171641791044776Logistic Regressionmodel = LogisticRegression()model.fit(train_X,train_Y)prediction3=model.predict(test_X)print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))The accuracy of the Logistic Regression is 0.8134328358208955Decision Treemodel=DecisionTreeClassifier()model.fit(train_X,train_Y)prediction4=model.predict(test_X)print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))The accuracy of the Decision Tree is 0.8022388059701493KNNmodel=KNeighborsClassifier() model.fit(train_X,train_Y)prediction5=model.predict(test_X)print('The accuracy of the KNN is',metrics.accuracy_score(prediction5,test_Y))The accuracy of the KNN is 0.832089552238806N의 개수에 따른 정확도 변화a_index=list(range(1,11))a=pd.Series()x=[0,1,2,3,4,5,6,7,8,9,10]for i in list(range(1,11)):    model=KNeighborsClassifier(n_neighbors=i)     model.fit(train_X,train_Y)    prediction=model.predict(test_X)    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))plt.plot(a_index, a)plt.xticks(x)fig=plt.gcf()fig.set_size_inches(12,6)plt.show()print('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())Accuracies for different values of n are: [0.75746269 0.79104478 0.80970149 0.80223881 0.83208955 0.81716418 0.82835821 0.83208955 0.8358209  0.83208955] with the max value as  0.835820895522388Gaussian Naive Bayesmodel=GaussianNB()model.fit(train_X,train_Y)prediction6=model.predict(test_X)print('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))The accuracy of the NaiveBayes is 0.8134328358208955Random Forestsmodel=RandomForestClassifier(n_estimators=100)model.fit(train_X,train_Y)prediction7=model.predict(test_X)print('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))The accuracy of the Random Forests is 0.8246268656716418Cross Validataion교차 검증을 통해서 확인을 해야합니다.K-Fold 교차분석을 사용합니다.shuffle=True를 넣어줘야합니다.from sklearn.model_selection import KFoldfrom sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import cross_val_predictkfold = KFold(n_splits=10, random_state=22, shuffle=True) xyz=[]accuracy=[]std=[]classifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']models=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]for i in models:    model = i    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")    cv_result=cv_result    xyz.append(cv_result.mean())    std.append(cv_result.std())    accuracy.append(cv_result)new_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       new_models_dataframe2                  CV Mean      Std                  Linear Svm      0.784607      0.057841              Radial Svm      0.828377      0.057096              Logistic Regression      0.799176      0.040154              KNN      0.808140      0.040287              Decision Tree      0.805868      0.045909              Naive Bayes      0.795843      0.054861              Random Forest      0.817104      0.041512      plt.subplots(figsize=(12,6))box=pd.DataFrame(accuracy,index=[classifiers])box.T.boxplot()&lt;AxesSubplot:&gt;new_models_dataframe2['CV Mean'].plot.barh(width=0.8)plt.title('Average CV Mean Accuracy')fig=plt.gcf()fig.set_size_inches(8,5)plt.show()heatmap을 통해 모델별 정확도 확인하기f,ax=plt.subplots(3,3,figsize=(12,10))y_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')ax[0,0].set_title('Matrix for rbf-SVM')y_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')ax[0,1].set_title('Matrix for Linear-SVM')y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')ax[0,2].set_title('Matrix for KNN')y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')ax[1,0].set_title('Matrix for Random-Forests')y_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')ax[1,1].set_title('Matrix for Logistic Regression')y_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')ax[1,2].set_title('Matrix for Decision Tree')y_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')ax[2,0].set_title('Matrix for Naive Bayes')plt.subplots_adjust(hspace=0.2,wspace=0.2)plt.show()하이퍼 파라미터 튜닝SVM과 Random Forest의 파라미터 튜닝하기SVMfrom sklearn.model_selection import GridSearchCVC=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]gamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]kernel=['rbf','linear']hyper={'kernel':kernel,'C':C,'gamma':gamma}gd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)gd.fit(X,Y)print(gd.best_score_)print(gd.best_estimator_)Fitting 5 folds for each of 240 candidates, totalling 1200 fits0.8282593685267716SVC(C=0.4, gamma=0.3)Random Forestn_estimators=range(100,1000,100)hyper={'n_estimators':n_estimators}gd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)gd.fit(X,Y)print(gd.best_score_)print(gd.best_estimator_)Fitting 5 folds for each of 9 candidates, totalling 45 fits0.819327098110602RandomForestClassifier(n_estimators=300, random_state=0)Ensembling1) Voting Classifier2) Bagging3) Boosting# Voting Classifierfrom sklearn.ensemble import VotingClassifierensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),                                              ('LR',LogisticRegression(C=0.05)),                                              ('DT',DecisionTreeClassifier(random_state=0)),                                              ('NB',GaussianNB()),                                              ('svm',svm.SVC(kernel='linear',probability=True))                                             ],                        voting='soft').fit(train_X,train_Y)print('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))cross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")print('The cross validated score is',cross.mean())The accuracy for ensembled model is: 0.8246268656716418The cross validated score is 0.8249188514357053# Bagging - KNNfrom sklearn.ensemble import BaggingClassifiermodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)model.fit(train_X,train_Y)prediction=model.predict(test_X)print('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')print('The cross validated score for bagged KNN is:',result.mean())The accuracy for bagged KNN is: 0.835820895522388The cross validated score for bagged KNN is: 0.8160424469413232# Bagging DecisionTreemodel=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)model.fit(train_X,train_Y)prediction=model.predict(test_X)print('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')print('The cross validated score for bagged Decision Tree is:',result.mean())The accuracy for bagged Decision Tree is: 0.8208955223880597The cross validated score for bagged Decision Tree is: 0.8171410736579275# AdaBoostfrom sklearn.ensemble import AdaBoostClassifierada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)result=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')print('The cross validated score for AdaBoost is:',result.mean())The cross validated score for AdaBoost is: 0.8249188514357055# Stochastic Gradient Boostingfrom sklearn.ensemble import GradientBoostingClassifiergrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)result=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')print('The cross validated score for Gradient Boosting is:',result.mean())The cross validated score for Gradient Boosting is: 0.8115230961298376# XGBoostimport xgboost as xgxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)result=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')print('The cross validated score for XGBoost is:',result.mean())[20:33:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.The cross validated score for XGBoost is: 0.8160299625468165# Hyper-Parameter Tuning for AdaBoostn_estimators=list(range(100,1100,100))learn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]hyper={'n_estimators':n_estimators,'learning_rate':learn_rate}gd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)gd.fit(X,Y)print(gd.best_score_)print(gd.best_estimator_)Fitting 5 folds for each of 120 candidates, totalling 600 fits0.8293892411022534AdaBoostClassifier(learning_rate=0.1, n_estimators=100)# Confusion Matrix for the Best Modelada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)result=cross_val_predict(ada,X,Y,cv=10)sns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')plt.show()Feature Importancef,ax=plt.subplots(2,2,figsize=(15,12))model=RandomForestClassifier(n_estimators=500,random_state=0)model.fit(X,Y)pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])ax[0,0].set_title('Feature Importance in Random Forests')model=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)model.fit(X,Y)pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')ax[0,1].set_title('Feature Importance in AdaBoost')model=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)model.fit(X,Y)pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')ax[1,0].set_title('Feature Importance in Gradient Boosting')model=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)model.fit(X,Y)pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')ax[1,1].set_title('Feature Importance in XgBoost')plt.show()[20:39:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.",
        "url": "/programming-kaggle3"
    }
    ,
    
    "study-ml5": {
        "title": "머신러닝 정리 (5) &lt;br&gt; 데이터 표현과 특성 공학",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학    머신러닝 정리 (6)-모델 평가와 성능 향상  범주형 변수          원-핫 인코딩        OneHotEncoder와 ColumnTransformer: scikit-learn으로 범주형 변수 다루기  make_column_transformer로 간편하게 ColumnTransformer 만들기  구간 분할, 이산화 그리고 선형 모델, 트리 모델  상호작용과 다항식  일변량 비선형 변환  특성 자동 선택  일변량 통계(ANOVA)  모델 기반 특성 선택  반복적 특성 선택  전문가 지식 활용머신러닝 정리 (5) 데이터 표현과 특성 공학본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python범주형 변수우리가 실제 사용하는 데이터의 대부분은 범주형 변수입니다.이 데이터에서는 근로자의 수입이 50000달러를 초과하는지, 이하인지 예측하려고 합니다.사용된 데이터셋은 다음과 같습니다.# pip install mglearnimport pandas as pdfrom preamble import *import os# 이 파일은 열 이름을 나타내는 헤더가 없으므로 header=None으로 지정하고# \"names\" 매개변수로 열 이름을 제공합니다data = pd.read_csv(    os.path.join(mglearn.datasets.DATA_PATH, \"adult.data\"), header=None, index_col=False,    names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',           'marital-status', 'occupation', 'relationship', 'race', 'gender',           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',           'income'])# 예제를 위해 몇개의 열만 선택합니다data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week',             'occupation', 'income']]# IPython.display 함수는 주피터 노트북을 위해 포맷팅된 출력을 만듭니다display(data.head())                  age      workclass      education      gender      hours-per-week      occupation      income                  0      39      State-gov      Bachelors      Male      40      Adm-clerical      &lt;=50K              1      50      Self-emp-not-inc      Bachelors      Male      13      Exec-managerial      &lt;=50K              2      38      Private      HS-grad      Male      40      Handlers-cleaners      &lt;=50K              3      53      Private      11th      Male      40      Handlers-cleaners      &lt;=50K              4      28      Private      Bachelors      Female      40      Prof-specialty      &lt;=50K      원-핫 인코딩범주형 변수를 0 또는 1 값을 가진 하나 이상의 새로운 특성으로 바꾼 것입니다.0과 1로 표현된 변수가 선형 이진 분류 공식에 적용할 수 있기 때문입니다.범주형 데이터 문자열 확인하기데이터셋을 읽은 뒤 범주형 데이터가 있는지 확인해보는 것이 좋습니다.print(data.gender.value_counts()) Male      21790 Female    10771Name: gender, dtype: int64get_dummies 함수를 사용해 쉽게 인코딩할 수 있습니다.data_dummies=pd.get_dummies(data)print(list(data_dummies.columns))['age', 'hours-per-week', 'workclass_ ?', 'workclass_ Federal-gov', 'workclass_ Local-gov', 'workclass_ Never-worked', 'workclass_ Private', 'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc', 'workclass_ State-gov', 'workclass_ Without-pay', 'education_ 10th', 'education_ 11th', 'education_ 12th', 'education_ 1st-4th', 'education_ 5th-6th', 'education_ 7th-8th', 'education_ 9th', 'education_ Assoc-acdm', 'education_ Assoc-voc', 'education_ Bachelors', 'education_ Doctorate', 'education_ HS-grad', 'education_ Masters', 'education_ Preschool', 'education_ Prof-school', 'education_ Some-college', 'gender_ Female', 'gender_ Male', 'occupation_ ?', 'occupation_ Adm-clerical', 'occupation_ Armed-Forces', 'occupation_ Craft-repair', 'occupation_ Exec-managerial', 'occupation_ Farming-fishing', 'occupation_ Handlers-cleaners', 'occupation_ Machine-op-inspct', 'occupation_ Other-service', 'occupation_ Priv-house-serv', 'occupation_ Prof-specialty', 'occupation_ Protective-serv', 'occupation_ Sales', 'occupation_ Tech-support', 'occupation_ Transport-moving', 'income_ &lt;=50K', 'income_ &gt;50K']data_dummies.head()                  age      hours-per-week      workclass_ ?      workclass_ Federal-gov      ...      occupation_ Tech-support      occupation_ Transport-moving      income_ &lt;=50K      income_ &gt;50K                  0      39      40      0      0      ...      0      0      1      0              1      50      13      0      0      ...      0      0      1      0              2      38      40      0      0      ...      0      0      1      0              3      53      40      0      0      ...      0      0      1      0              4      28      40      0      0      ...      0      0      1      0      5 rows × 46 columnsNumpy 배열로 바꾸고 타깃 값을 분리하여 학습 모델에 적용해야합니다.features = data_dummies.loc[:, 'age':'occupation_ Transport-moving']# NumPy 배열 추출X = features.valuesy = data_dummies['income_ &gt;50K'].values #타깃 값print(\"X.shape: {}  y.shape: {}\".format(X.shape, y.shape))X.shape: (32561, 44)  y.shape: (32561,)# 로지스틱 회귀 분석from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)logreg = LogisticRegression(max_iter=5000)logreg.fit(X_train, y_train)print(\"테스트 점수: {:.2f}\".format(logreg.score(X_test, y_test)))테스트 점수: 0.81숫자로 표현된 범주형 특성0부터 시작하는 연속된 자연수로 이루어진 데이터셋은 연속형인지 범주형인지 알기 힘듭니다.따라서 데이터셋이 의미하는 것이 무엇인지 확인해야 합니다.숫자로 표현된 범주형 데이터셋은 get_dummies를 사용하면 숫자 특성은 바뀌지 않습니다.따라서 columns 매개변수에 인코딩하고 싶은 열을 명시해야합니다.OneHotEncoder와 ColumnTransformer: scikit-learn으로 범주형 변수 다루기scikit-learn의 OneHotEncoder을 통해 모든 열에 인코딩을 수행할 수 있습니다.sparse=False는 희소 행렬이 아닌 넘파이 배열을 반환합니다.변환된 특성에 해당하는 원본 범주형 변수 이름을 알기 위해서는 get_feature_names 메소드를 사용합니다.ColumnTransformer의 fit, transform 메소드를 사용할 수 있습니다.from sklearn.preprocessing import OneHotEncoderfrom sklearn.compose import ColumnTransformerfrom sklearn.preprocessing import StandardScalerct = ColumnTransformer(    [(\"scaling\", StandardScaler(), ['age', 'hours-per-week']),     (\"onehot\", OneHotEncoder(sparse=False), ['workclass', 'education', 'gender', 'occupation'])])이렇게 변환된 모델을 LogisticRegression에 학습합니다.from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_split# income을 제외한 모든 열을 추출합니다data_features = data.drop(\"income\", axis=1)# 데이터프레임과 income을 분할합니다X_train, X_test, y_train, y_test = train_test_split(    data_features, data.income, random_state=0)ct.fit(X_train)X_train_trans = ct.transform(X_train)print(X_train_trans.shape)(24420, 44)44개의 특성이 만들어진 것은 위의 pd.get_dummies를 사용했을 때와 마찬가지이고 스케일 조정이 된 차이가 있습니다.logreg = LogisticRegression(max_iter=1000)logreg.fit(X_train_trans, y_train)X_test_trans = ct.transform(X_test)print(\"테스트 점수: {:.2f}\".format(logreg.score(X_test_trans, y_test)))테스트 점수: 0.81make_column_transformer로 간편하게 ColumnTransformer 만들기클래스 이름을 기반으로 자동으로 각 단계에 이름을 붙여주는 make_column_transformer 함수가 있습니다.변환된 데이터는 넘파이 배열이므로 열 이름을 가지고 있지 않습니다.from sklearn.compose import make_column_transformerct = make_column_transformer(    (StandardScaler(), ['age', 'hours-per-week']),    (OneHotEncoder(sparse=False), ['workclass', 'education', 'gender', 'occupation']))print(ct)ColumnTransformer(transformers=[('standardscaler', StandardScaler(),                                 ['age', 'hours-per-week']),                                ('onehotencoder', OneHotEncoder(sparse=False),                                 ['workclass', 'education', 'gender',                                  'occupation'])])구간 분할, 이산화 그리고 선형 모델, 트리 모델wave 데이터 셋을 통해선형 회귀 모델과 결정 트리 회귀를 비교해보았습니다.from sklearn.linear_model import LinearRegressionfrom sklearn.tree import DecisionTreeRegressorX, y = mglearn.datasets.make_wave(n_samples=120)line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)reg = DecisionTreeRegressor(min_samples_leaf=3).fit(X, y)plt.plot(line, reg.predict(line), label=\"Decision Tree\")reg = LinearRegression().fit(X, y)plt.plot(line, reg.predict(line), '--', label=\"linear regression\")plt.plot(X[:, 0], y, 'o', c='k')plt.ylabel(\"output\")plt.xlabel(\"input\")plt.legend(loc=\"best\")plt.show()선형 모델은 선형 관계로만 모델링하므로 특성이 하나일 땐 직선으로 나타납니다.구간 분할을 통해 한 특성을 여러 특성으로 나누면 강력한 선형 모델을 만들 수 있습니다.균일한 너비로 나누거나 데이터의 분위를 사용할 수도 있습니다.KBinsDiscretizer 클래스를 이용하면 됩니다.KBinsDiscretizer는 한 번에 여러 개의 특성에 적용할 수 있습니다.bin_edges_는 특성별로 경계값이 저장되어 있으며 길이가 1인 넘파이 배열이 출력됩니다.기본적으로 원-핫-인코딩을 적용하며 구간마다 새로운 특성이 생기는 희소 행렬을 만듭니다.n개의 구간을 지정하면 n개의 차원이 생성됩니다.from sklearn.preprocessing import KBinsDiscretizerkb = KBinsDiscretizer(n_bins=10, strategy='uniform')kb.fit(X)print(\"bin edges: \\n\", kb.bin_edges_)bin edges:  [array([-2.967, -2.378, -1.789, -1.2  , -0.612, -0.023,  0.566,  1.155,        1.744,  2.333,  2.921])]X_binned = kb.transform(X)X_binnedprint(X[:10])X_binned.toarray()[:10][[-0.753] [ 2.704] [ 1.392] [ 0.592] [-2.064] [-2.064] [-2.651] [ 2.197] [ 0.607] [ 1.248]]array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])원-핫-인코딩된 밀집 배열을 만들어보면 다음과 같습니다.kb = KBinsDiscretizer(n_bins=10, strategy='uniform', encode='onehot-dense')kb.fit(X)X_binned = kb.transform(X)  line_binned = kb.transform(line)reg = LinearRegression().fit(X_binned, y)plt.plot(line, reg.predict(line_binned), label='linear regression')reg = DecisionTreeRegressor(min_samples_split=3).fit(X_binned, y)plt.plot(line, reg.predict(line_binned), label='decision tree')plt.plot(X[:, 0], y, 'o', c='k')plt.vlines(kb.bin_edges_[0], -3, 3, linewidth=1, alpha=.2)plt.legend(loc=\"best\")plt.ylabel(\"output\")plt.xlabel(\"input\")plt.show(두 선이 완전히 겹치는 결과가 나왔습니다.선형 모델은 훨씬 유연해졌지만 결정 트리는 덜 유연해졌습니다.상호작용과 다항식상호작용과 다항식을 추가하면 특성을 더 풍부하게 만들 수 있습니다.X_combined = np.hstack([X, X_binned])print(X_combined.shape)  reg = LinearRegression().fit(X_combined, y)line_combined = np.hstack([line, line_binned])plt.plot(line, reg.predict(line_combined), label='Linear regression with original characteristics added')plt.vlines(kb.bin_edges_[0], -3, 3, linewidth=1, alpha=.2)plt.legend(loc=\"best\")plt.ylabel(\"Regression output\")plt.xlabel(\"Input characteristics\")plt.plot(X[:, 0], y, 'o', c='k')plt.show() (120, 11)기울기가 모두 같으므로 원본 특성을 곱한 값을 더해 20개의 특성을 만들어줍니다.X_product = np.hstack([X_binned, X * X_binned])print(X_product.shape)(120, 20)reg = LinearRegression().fit(X_product, y)line_product = np.hstack([line_binned, line * line_binned])plt.plot(line, reg.predict(line_product), label='Linear regression with original characteristics added')plt.vlines(kb.bin_edges_[0], -3, 3, linewidth=1, alpha=.2)plt.plot(X[:, 0], y, 'o', c='k')plt.ylabel(\"Regression output\")plt.xlabel(\"Input characteristics\")plt.legend(loc=\"best\")plt.show()preprocessiong 모듈의 PolynomialFeatures에 구현되어 있습니다.from sklearn.preprocessing import PolynomialFeatures# x ** 10까지 고차항을 추가합니다# 기본값인 \"include_bias=True\"는 절편에 해당하는 1인 특성을 추가합니다poly = PolynomialFeatures(degree=10, include_bias=False)poly.fit(X)X_poly = poly.transform(X)print(\"X_poly.shape:\", X_poly.shape)X_poly.shape: (120, 10)print(\"항 이름:\\n\", poly.get_feature_names())항 이름: ['x0', 'x0^2', 'x0^3', 'x0^4', 'x0^5', 'x0^6', 'x0^7', 'x0^8', 'x0^9', 'x0^10']이렇게 다항 회귀 모델을 만들 수 있습니다.다항식의 특성은 1차원 데이터셋에서도 매우 부드러운 곡선을 만듭니다.하지만 고차원 다항식은 데이터가 부족한 영역에서 너무 민감하게 동작합니다.reg = LinearRegression().fit(X_poly, y)line_poly = poly.transform(line)plt.plot(line, reg.predict(line_poly), label='Multinomial linear regression')plt.plot(X[:, 0], y, 'o', c='k')plt.ylabel(\"Regression output\")plt.xlabel(\"Input characteristics\")plt.legend(loc=\"best\")plt.show()비교를 위한 커널 SVM 모델from sklearn.svm import SVRfor gamma in [1, 10]:    svr = SVR(gamma=gamma).fit(X, y)    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))plt.plot(X[:, 0], y, 'o', c='k')plt.ylabel(\"Regression output\")plt.xlabel(\"Input characteristics\")plt.legend(loc=\"best\")plt.show()커널 SVM을 사용해 특성 데이터 변환없이 다항 회귀와 비슷한 복잡도를 가진 예측을 만들 수 있었습니다.보스턴 데이터셋을 통한 비교from sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import MinMaxScalerboston = load_boston()X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,                                                    random_state=0)# 데이터 스케일 조정scaler = MinMaxScaler()X_train_scaled = scaler.fit_transform(X_train)X_test_scaled = scaler.transform(X_test)poly = PolynomialFeatures(degree=2).fit(X_train_scaled)X_train_poly = poly.transform(X_train_scaled)X_test_poly = poly.transform(X_test_scaled)print(\"X_train.shape:\", X_train.shape)print(\"X_train_poly.shape:\", X_train_poly.shape)X_train.shape: (379, 13)X_train_poly.shape: (379, 105)from sklearn.linear_model import Ridgeridge = Ridge().fit(X_train_scaled, y_train)print(\"상호작용 특성이 없을 때 점수: {:.3f}\".format(ridge.score(X_test_scaled, y_test)))ridge = Ridge().fit(X_train_poly, y_train)print(\"상호작용 특성이 있을 때 점수: {:.3f}\".format(ridge.score(X_test_poly, y_test)))상호작용 특성이 없을 때 점수: 0.621상호작용 특성이 있을 때 점수: 0.753from sklearn.ensemble import RandomForestRegressorrf = RandomForestRegressor(n_estimators=100, random_state=0).fit(X_train_scaled, y_train)print(\"상호작용 특성이 없을 때 점수: {:.3f}\".format(rf.score(X_test_scaled, y_test)))rf = RandomForestRegressor(n_estimators=100, random_state=0).fit(X_train_poly, y_train)print(\"상호작용 특성이 있을 때 점수: {:.3f}\".format(rf.score(X_test_poly, y_test)))상호작용 특성이 없을 때 점수: 0.795상호작용 특성이 있을 때 점수: 0.775Ridge의 성능은 크게 높였지만 RandomForest는 상호작용 특성이 있는 Ridge만큼의 성능과 맞먹었습니다.일변량 다항식log, exp, sin과 같은 함수들도 특성 변환에 사용됩니다.log, exp 함수는 데이터 스케일을 변경하는 것에 사용되며, sin과 cos은 주기적인 패턴이 들어간 데이터를 변경하는 것에 유용하게 사용됩니다.일변량 비선형 변환을 하는 이유는 특성과 타깃 값 사이에 비선형성이 있다면 모델을 만들기 어렵기 때문입니다.또한 대부분의 모델은 특성의 분포가 정규분포와 비슷할 때 데이터 간 편차를 줄여 성능이 상승합니다.![그림1](https://user-images.githubusercontent.com/54880474/141170385-38f2a372-e76e-44f8-8f05-5ffcd01b743c.png)![그림2](https://user-images.githubusercontent.com/54880474/141170390-e416f90a-e5d0-433a-9c24-09f89bcad250.png)다음 분포처럼 적은 값 쪽에 분포가 몰리지만 값의 범위가 매우 넓을 때 log함수를 사용하게 된다면 log함수는 x가 커질수록 증가율이 작아지기 때문에 멀리 떨어진 값을 중간값과 가깝게 이동시켜 정규분포와 유사한 형태를 띄게 만들 수 있습니다.  트리 모델처럼 스스로 중요한 상호작용을 찾을 수 있는 모델은 괜찮지만 선형 모델과 같이 스케일과 분포에 민감한 모델은 비선형 변환을 통해 좋은 효과를 얻을 수 있습니다.  ## 특성 자동 선택특성이 많으면 모델은 복잡해지고 과대적합 가능성이 상승하는 것을 앞에서 배웠습니다.  따라서 불필요한 특성을 줄이는 것이 모델 학습에 좋은 영향을 미칩니다.  특성 선택을 사용할 때는 test 데이터셋에 영향이 가지않도록 train 데이터셋에만 적용해야하는 주의점이 있습니다.  ### 일변량 통계 (ANOVA)일변량 통계는 특성과 타깃 사이에 중요한 통계적 관계를 계산합니다.  각 특성을 독립적으로 평가하여 다른 특성과 깊게 연관된 특성은 선택하지 않습니다.  SelectKBest는 K개의 특성을, SelectPercentile은 지정한 비율만큼의 특성을 선택해줍니다.  또한 분류 모델에서는 f_classif, 회귀에서는 f_regression을 사용합니다.  일변량 통계는 Y값과 특성의 통계적 유의미를 분석합니다.   귀무가설을 기각하기 위해서는 p-value가 작아야합니다.  일변량 통계의 귀무가설은 '집단간 평균은 같다'입니다.  집단간 분산을 집단 내 분산으로 나눈 것을 F-value라고 합니다.  F-value가 작으면 p-value가 커지므로 이는 집단간 분산이 분모에 있으므로 집단간 분산이 작다는 의미와 같습니다.  이것은 집단간 차이가 적다는 의미이므로 타깃에 미치는 영향이 적다고 할 수 있습니다.  ![그림3](https://user-images.githubusercontent.com/54880474/141170392-caac02a7-62e7-4091-a621-6705b3eb68c9.png)따라서 타깃에 미치는 영향이 적은 특성을 제외시킬 수 있게 됩니다.  ### 모델 기반 특성 선택특성의 중요도를 측정하여 순서를 매깁니다.  결정 트리에서 중요도는 feature_importances_, 선형 모델에서는 계수의 절대값을 통해 중요도를 결정합니다.  임계치를 threshold를 통해 지정하는데 중앙값, 평균값, 1.2*평균값과 같이 지정해주면 됩니다.  ### 반복적 특성 선택  특성 수가 다른 일련의 모델을 생성하여 최선의 특성을 선택합니다.  여러 개의 모델을 만들어야하기 때문에 계산비용이 증가하게 됩니다.  1. 전진(후진) 선택법특성을 하나도 선택하지 않은 모델부터 회귀에서 결정계수(R^2)나 분류에서 정확도를 통해서 scoring 매개변수를 기준으로 종료조건을 충족시킬 때까지 하나씩 추가하는 방법입니다.  2. 재귀적 특성 제거  모든 특성을 가지고 시작해서 어떤 종료 조건이 될 때까지 특성을 하나씩 제거하는 방법이니다.  마찬가지로 중요도가 낮은 특성을 먼저 제거합니다.  제거한 다음 새로운 모델을 만들어서 정의한 특성 개수가 남을 때까지 계속합니다.  특성 자동 선택을 이용하면 특성 개수가 적어지기 때문에 모델의 예측 속도가 빨라지며 해석력이 좋아집니다.  ### 전문가 지식 활용  모델에 따라 외삽 문제가 발생할 수도 있고 그렇게 된다면 결정계수가 전혀 상관없는 예측을 할 수도 있습니다.  따라서 모델을 예측할 때는 서로간의 연관관계를 확인하고 특성을 추가, 제거하여 더 좋은 모델을 만들 수 있습니다.  ",
        "url": "/study-ML5"
    }
    ,
    
    "license-adsp6": {
        "title": "ADSP (6) 통계 분석(1)",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP 합격 후기ADSP를 준비하면서 공부한 내용을 정리한 글입니다.3과목 2장 통계 분석에 대한 부분을 정리한 글입니다.  통계의 이해          통계와 표본 조사      표본 추출 방법      측정과 척도      기술통계와 추리통계        확률과 확률분포          이산확률분포      연속확률분포      여러가지 통계값        추정과 가설검정          추정      가설검정      통계의 이해통계와 표본 조사통계분석하고자 하는 집단에 대해 조사하거나 실험을 통해 얻는 자료표본조사전수조사가 불가능하기 때문에 표본의 대표성을 신뢰할 수 있는 표본 조사를 진행신뢰수준신뢰수준 95%라는 말은 95% 신뢰할 수 있다는 말이 아니라 100번 조사했을 때 오차범위 내에서 동일한 결과가 95번 나온다는 말오차범위 오차범위는 결과값에 대한 오차범위로 오차범위 3%라면 n-3~n+3의 값을 갖는다는 의미표본 추출 방법단순 랜덤 추출법표본 추출 방법에서 N개의 모집단에서 n개의 데이터를 무작위로 추출하는 방법계통 추출법모집단의 원소에 차례대로 번호를 부여한 뒤 일정한 간격을 두고 데이터를 추출하는 방법.집락(군집) 추출법각각 군집으로 구분한 뒤 단순 랜덤 추출법에 의하여 선택된 군집의 데이터를 표본으로 사용한다.집락은 서로 동질적이지만, 집락 내 데이터는 서로 이질적이다.층화 추출법층화 추출법은 집락 추출법과 유사하나 각 집락은 서로 이질적이고 내부 데이터는 서로 독립적이다.  비례 층화 추출법비례 층화 추출법은 전체 데이터의 분포를 반영하여 각 군집별 데이터를 추출하는 방법이다.  불비례 층화 추출법불비례 층화 추출법은 전체 데이터의 분포를 반영하지 않고 각 군집에서 원하는 데이터의 개수를 추출한다.측정과 척도측정표본조사를 실시하는 경우 추출된 원소들이나 실험 단위로부터 주어진 목적에 적합하게 관측해 자료를 얻는 것척도관측 대상의 속성을 측정하여 그 값이 숫자로 나타나도록 일정한 규칙을 정하여 바꾸는 도구척도의 종류  질적 척도          명목 척도 측정 대상이 어느 집단에 속하는지 나타내는 자료 (성별, 지역 등)      순서 척도 측정 대상이 명목척도이면서 서열 관계를 갖는 자료 (선호도, 학년 등)        양적 척도          구간 척도 측정 대상이 가지고 있는 속성의 양을 측정할 수 있으며 두 구간 사이에 의미가 있는 자료  (온도, 지수 등)      비율 척도 측정 대상이 구간척도이면서도 절대적 기준 0이 존재하여 사칙연산이 가능한 척도  (신장, 무게, 점수, 등)      기술통계와 추리통계기술통계기술 통계는 표본 자체의 속성이나 특징을 파악하는 데 중점을 두고 자료를 요약하고 조직화, 단순화하는데 그 목적이 있다.추리통계추리통계는 수집한 데이터를 바탕으로 ‘추론 및 예측’하는 통계 기법이다.표본에서 얻은 통계치를 바탕으로 오차를 고려하면서 모수를 확률적으로 추정하는 통계 기법이다.모집단의 특성을 추정하는 데 초점을 두고 가설을 검증하거나 확률적인 가능성을 파악한다.통계 기초 개졈편차평균과의 차이분산 Var(X)평균으로부터의 분포분산이 크면 퍼짐의 정도가 크다.표준편차 sd(X)분산은 기존과 다른 단위를 갖게 되기 때문에 단위를 일치시키기 위해 분산에 루트를 씌워서 구하는 값확률과 확률분포확률발생 가능한 모든 사건들의 집합 표본공간에서 표본공간의 부분집합인 특정 사건 A가 발생할 수 있는 비율조건부 확률특정 사건 A가 발생했다는 사실 아래 또 다른 사건 B가 발생할 확률독립사건서로에게 영향을 주지 않는 두 개의 사건배반사건공통된 부분이 없는 사건확률변수무작위 실험을 했을 때 특정 확률로 발생하는 각각의 결과를 수치적 값으로 표현하는 변수확률분포확률변수의 모든 값과 그 대응하는 확률이 어떻게 분포하고 있는 지를 보여주는 분포이산확률분포의 확률함수를 ‘확률질량함수’연속확률분포의 확률함수를 ‘확률밀도함수’이산확률분포베르누이 분포확률변수 X가 취할 수 있는 값이 두 개인 경우로 일반적으로 한 번의 시행을 할 때 성공과 실패로 나눌 수 있는 성공할 확률이 p인 분포ex) 하나의 동전을 던져 앞면이 나올 확률이항 분포이항 분포는 n번의 베루누이 시행에서 k번 성공할 확률의 분포ex) 하나의 동전을 3번 던져 앞면이 2번 나올 확률기하 분포성공할 확률이 p인 베르누이 시행에서 처음으로 성공이 나올 때까지 k번 실패할 확률의 분포ex) 동전을 던져서 3번째에 앞면이 나올 확률다항 분포이항 분포를 확장한 개념으로, n번의 시행에서 각 시행이 3개 이상의 결과를 가질 수 있는 확률의 분포ex) 주사위를 n번 던졌을 때 Pn의 확률로 1이 x번, 2가 y번 3이 z번 나올 확률포아송 분포단위 시간 또는 단위 공간 내에서 발생할 수 있는 사건의 발생 횟수에 대한 확률분포ex) 8시간 동안 3명의 손님이 왔을 때 1시간 동안 1명의 손님이 올 확률이산확률변수확률변수가 취할 수 있는 실수 값의 수를 셀 수 있는 변수연속확률분포균일분포균일 분포는 연속형 확률변수인 X가 취할 수 있는 모든 값에 대하여 같은 확률을 갖고 있는 분포균일 분포 그래프 아래 면적의 넓이는 확률의 총합인 1정규분포정규분포는 평균이 μ, 표준편차가 σ인 분포표준정규분포는 평균이 0, 표준편차가 1인 정규분포t-분포자유도가 n인 t분포는 평균이 0이고 종 모양이지만 정규분포보다 두꺼운 꼬리를 갖는 분포표준정규분포를 활용하여 모수를 추정하기 위해서는 모표준편차를 사전에 알고있어야 하지만, 현실적으로 모르기 때문에 t분포를 이용하여 모평균 검정 또는 두 집단의 평균이 동일한지 계산하기 위한 검정통계량으로 활용된다.자유도가 커질수록 t분포는 표준정규분포와 가깝다.자유도는 표본자료들이 모집단에 정보를 주는 독립적인 자료의 개수ex) 의자 4개와 사람 4명이 있을 때 마지막 사람에겐 선택권이 없으므로 자유도는 3카이제곱 분포표준정규분포를 따르는 확률변수 Z1,Z2,···,Zn의 제곱의 합 X는 자유도가 n인 카이제곱 분포를 따른다.모평균과 모분산을 모르는 두 개 이상의 집단 간 동질성 검정 또는 모분산 검정을 위해 활용된다.  동질성 검정두 집단의 내부 구성비를 비교하는 것F분포서로 독립인 두 카이제곱 분포를 따르는 확률변수 V1~x2(k1),V2~x2(k2)를 각각의 자유도로 나누었을 때 서로의 비율 X는 자유도가 k1, k2인 F분포를 따른다.F분포는 등분산 검정 및 분산분석을 위해 활용된다.  등분산 검정가설 검정을 수행하는 환경에 따라 두 모집단에 대한 평균을 비교할 때 사용연속확률변수확률변수가 취할 수 있는 실수 값이 어떤 특정 구간 전체에 해당하여 그 수를 셀 수 없는 변수를 연속확률번수확률밀도함수의 아래 면적이 확률여러가지 통계값기댓값특정 사건이 시행되었을 때 확률변수 X가 취할 수 있는 값의 평균 값분산분산데이터들이 중심에서 얼마나 떨어져 있는지를 알아보기 위한 측도확률변수의 분산확률변수가 취할 수 있는 값들이 모평균에서 얼마나 떨어져 있는지를 측정하는 측도표준편차자료의 산포도를 나타내는 수치첨도확률분포의 뾰족한 정도를 나타내는 측도로 값이 3에 가까울수록 정규분포 모양왜도확률분포의 비대칭 정도를 나타내는 측도0일 경우 평균, 중앙값, 최빈값이 같음공분산확률변수 X, Y의 상관 정도를 나타내는 값하나의 확률변수가 증가할 때 다른 확률변수의 증감소 여부를 확인양수면 X가 증가할 때 Y도 증가, 음수면 X가 증가할 때 Y는 감소공분산 값이 100이면 두 확률 분포가 어느 정도 선형성을 갖는지 알 수 없음상관계수공분산의 문제를 해결한 값으로 -1과 1 사이의 값 중에서 공분산을 X의 표준편차와 Y의 표준편차로 모두 나눈 값추정과 가설검정추정모수의 추정우리가 궁극적으로 알고싶은 값은 모수지만, 전수조사를 해야 알 수 있기 때문에 모수를 추정한다.점추정모집단의 모수, 특히 모평균을 추정할 때 모평균을 하나의 특정한 값이라고 예측한 것불편추정량이란 모수를 추정할 때 추정하는 값과 실제 모수 값의 차이의 기댓값이 0이므로 어느 한쪽으로 편향되지 않아 모수를 추정하기에 이상적인 값구간추정구간추정은 모수가 특정한 구간 안에 존재할 것이라 예상하는 것가설검정통계적 가설검정은 모집단의 특성에 대한 주장 또는 가설을 세우고 표본에서 얻은 정보를 이용해 가설이 옳은지 판정하는 과정으로 귀무가설과 대립가설로 구분귀무가설 (H0)모집단이 어떤 특징을 지닐것으로 여겨지는 가설대립가설 (H1)귀무가설이 틀렸다고 판단될 경우 채택되는 가설연구를 통해 증명하고자 하는 새로운 가설제1종 오류와 제2종 오류제 1종 오류귀무가설이 사실인데 귀무가설이 틀렸다고 결정하는 오류제 2종 오류귀무가설이 틀렸음에도 귀무가설이 옳다고 결정하는 오류검정통계량귀무가설의 옳고 그름을 판단할 수 있는 값기각역귀무가설을 기각하게 될 검정통계량의 영역검정통계량이 기각역 내에 있으면 귀무가설을 기각한다.기각역의 경계값을 임계값이라고 한다.유의수준 (α)귀무가설이 참인데도 이를 잘못 기각하는 오류를 범할 확률의 최대 허용 한계제 1종 오류를 줄이기위해 사용된다.유의확률 (p-value)귀무가설을 지지하는 정도를 나타낸 확률p-value &lt; α귀무가설을 기각할 수 있다.p-value &gt; α귀무가설을 기각할 수 없다.  기존의 귀무가설을 수립하고 이를 기각하는 증거를 찾는 대립가설을 채택하는 것이 가설 검증의 횟수를 줄일 수 있기 때문에 귀무가설을 검증한다.모수검정모수검정은 표본이 정규성을 갖는다는 모수적 특성을 이용등간척도, 비율척도평균피어슨 상관계수one sample t-test, two sample t-test, paired t-test, one way anova비모수검정비모수검정은 정규성 검정에서 정규분포를 따르지 않는다고 증명되거나 소규모 실험에서와 같이 정규분포를 가정할 수 없는 경우에 사용명목척도, 서열척도중앙값스피어만 상관계수부호검정, Wilcoxon 부호순위검정, Mann-Whitney 검정",
        "url": "/license-adsp6"
    }
    ,
    
    "license-adsp5": {
        "title": "ADSP (5) R 기초와 데이터 마트",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP 합격 후기ADSP를 준비하면서 공부한 내용을 정리한 글입니다.3과목 1장 R 기초와 데이터 마트에 대한 부분을 정리한 글입니다.  R 기초          데이터 타입      연산자      R 데이터 구조      R 내장 함수      R 데이터 핸들링      제어문      통계분석에 사용되는 R 함수        데이터 마트          데이터 전처리      R 패키지 활용        데이터 탐색          탐색적 데이터 분석      결측값      이상값      R 기초데이터 타입문자형 타입””, ‘‘안에 입력하여 사용한다.class('abc')class(\"abc\")class('1')class(\"True\")‘character’‘character’‘character’‘character’숫자형 타입numeric, double(실수), integer(정수), complex(복소수)와 같은 타입이 있다.double은 숫자로만 표현이 가능하다.Inf는 Infinite의 약자로 무한대를 의미한다.class(Inf)class(1)class(-3)‘numeric’‘numeric’‘numeric’논리형 타입참, 거짓을 의미한다.class(TRUE)class(FALSE)‘logical’‘logical’NaN. NA, NULLNaN은 ‘Not of Number’의 약자로 음수의 제곱근을 구하려고 시도하는 것과 같은 경우에 오류와 함께 숫자가 아님을 반환한다.‘Not Available’의 약자인 Na와 NULL은 결측값을 의미한다.NA는 공간을 차지하는 방면, NULL은 공간을 차지하지 않는 존재하지 않는 값을 의미한다.sqrt(-3)class(NA)class(NULL)Warning message in sqrt(-3):\"NaN이 생성되었습니다\"NaN‘logical’‘NULL’연산자대입 연산자&lt;-,«-,= : 오른쪽 값을 왼쪽에 대입-&gt;,-» : 왼쪽 값을 오른쪽에 대입string1 &lt;- 'abc''data'-&gt;string2number1&lt;&lt;-15Inf-&gt;&gt;number2logical=NAstring1string2number1number2logical‘abc’‘data’15Inf&lt;NA&gt;비교 연산자== : 두 값이 같은지 비교&lt; ,&gt; : 초과, 미만을 비교&lt;= , =&gt; : 이상, 이하를 비교is.character : 문자형인지 아닌지를 비교is.numeric : 숫자형인지 아닌지를 비교is.logical : 논리형인지 아닌지를 비교is.na : NA인지 아닌지를 비교is.null : NULL인지 아닌지를 비교NA는 비교할 값이 존재하지 않으므로 어떤 것과 비교를 하더라도 NA를 반환한다.string1=='abc'string2&gt;'DATA'number1&lt;=15is.na(logical)is.null(NULL)TRUEFALSETRUETRUETRUE산술 연산자두 숫자형 타입의 계산을 위한 연산자로서 다양한 연산이 가능하다.+, -, *, / : 두 숫자간의 사칙연산%/% : 두 숫자의 나눗셈의 몫%% : 두 숫자의 나눗셈의 나머지^, ** : 거듭제곱exp() : 자연상수의 거듭제곱기타 연산자논리값을 계싼하기 위한 연산자로는 부정연산자 (!), AND 연산자 (&amp;), OR 연산자 (|)가 있다.부정 연산자는 현재의 논리값에 반대되는 값, AND 연산자는 두 값이 모두 참일 때만 참, OR 연산자는 두 값 중 하나의 값만 참이여도 참이다.!TRUETRUE&amp;TRUETRUE&amp;FALSE!(TRUE)TRUE | FALSEFALSETRUEFALSEFALSETRUER 데이터 구조벡터벡터는 타입이 같은 여러 데이터를 하나의 행으로 저장하는 1차원 데이터 구조다.‘연결한다’라는 의미의 ‘concatenate’의 c를 써서 데이터를 묶을 수 있다.v4&lt;-c(3, TRUE,FALSE)v4v5&lt;-c('a',1,TRUE)v5&lt;ol class=list-inline&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;0&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;‘a’&lt;/li&gt;\t&lt;li&gt;‘1’&lt;/li&gt;\t&lt;li&gt;‘TRUE’&lt;/li&gt;&lt;/ol&gt;벡터를 생성할 때 c 안에 콤마를 구분자로 써서 성분을 직접 입력할 수 있지만 콜론:을 사용하여 시작과 끝값을 지정해 벡터를 생성할 수도 있다.v1&lt;-c(1:6)v1&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;6&lt;/li&gt;&lt;/ol&gt;행렬행렬은 2차원 구조를 가진 벡터다.벡터의 성질을 가지고 있으므로 행렬에 저장된 모든 데이터는 같은 타입이어야 한다.matrix를 사용하여 행렬을 만들 경우 nrow를 사용하여 행의 수를 결정하거나 ncol을 사용하여 열의 수를 결정할 수 있다.m1&lt;-matrix(c(1:6),nrow=2)m1m2&lt;-matrix(c(1:6),ncol=2)m2\t135\t246\t14\t25\t36matrix를 사용하여 행렬을 만들 경우 행렬의 값들이 열로 저장된다.  byrow=T를 지정하면 값들이 열이 아닌 행으로 나온다.m3&lt;-matrix(c(1:6),nrow=2, byrow=T)m3\t123\t456벡터에 차원을 주는 방법도 있다.dim함수는 벡터를 행렬로 변환할 뿐만 아니라 주어진 행렬이 몇 개의 행과 열로 구성되어 있는지 행렬의 크기를 나타내기도 한다.v1&lt;-c(1:6)v1dim(v1)&lt;-c(2,3)v1&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;6&lt;/li&gt;&lt;/ol&gt;\t135\t246배열3차원 이상의 구조를 갖는 벡터이다.array를 사용하여 만들 수 있으나 몇 차원의 구조를 갖는지 dim 옵션에 명시해야 한다.주피터 노트북 R을 사용하면 해당 기능이 작동하지 않는다.a1&lt;-array(c(1:12), dim=c(2,3,2))a1a2&lt;-c(1:12)dim(a2)&lt;-c(2,3,2)a2&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;6&lt;/li&gt;\t&lt;li&gt;7&lt;/li&gt;\t&lt;li&gt;8&lt;/li&gt;\t&lt;li&gt;9&lt;/li&gt;\t&lt;li&gt;10&lt;/li&gt;\t&lt;li&gt;11&lt;/li&gt;\t&lt;li&gt;12&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;6&lt;/li&gt;\t&lt;li&gt;7&lt;/li&gt;\t&lt;li&gt;8&lt;/li&gt;\t&lt;li&gt;9&lt;/li&gt;\t&lt;li&gt;10&lt;/li&gt;\t&lt;li&gt;11&lt;/li&gt;\t&lt;li&gt;12&lt;/li&gt;&lt;/ol&gt;리스트리스트는 성분들이 서로 이질적이다.L&lt;-list()L[[1]]&lt;-5L[[2]]&lt;-c(1:6)L[[3]]&lt;-matrix(c(1:6),nrow=2)L[[4]]&lt;-array(c(1:12),dim=c(2,3,2))L\t5\t&lt;ol class=list-inline&gt;\t1\t2\t3\t4\t5\t6&lt;/ol&gt;\t\t135\t246\t&lt;ol class=list-inline&gt;\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12&lt;/ol&gt;데이터프레임2차원 구조를 갖는 관계형 데이터 구조행렬과 같은 모양이지만 각 열은 서로 다른 타입의 데이터를 가질 수 있다.R의 벡터는 시작 인덱스 값을 1로 갖는다.v1&lt;-c(1,2,3)v2&lt;-c('a','b','c')df1&lt;-data.frame(v1,v2)df1v1v2\t1a\t2b\t3cR의 벡터는 시작 인덱스 값을 1로 갖는다.df1[0]df1[1]v1\t1\t2\t3R 내장 함수기본 함수help() or ? : 함수의 도움말paste() : 문자열 이어 붙이기seq() : 시작값, 끝값, 간격으로 수열을 생성rep() : 주어진 데이터를 일정 횟수만큼 반복rm() : 대입 연산자에 의해 생성된 변수 삭제ls() : 현재 생성된 변수들의 리스트 표현print() : 값을 콘솔창에 출력# help와 ?는 R스튜디오에서는 도움창이 있지만 주피터 노트북에서는 하단에 창으로 나온다.help(paste)?paste paste('This is','a pen')seq(1,10,by=2)rep(1,5)a&lt;-1arm(a)# a : errorls()print(10)‘This is a pen’&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;7&lt;/li&gt;\t&lt;li&gt;9&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;1&lt;/li&gt;&lt;/ol&gt;1&lt;ol class=list-inline&gt;\t&lt;li&gt;‘a1’&lt;/li&gt;\t&lt;li&gt;‘a2’&lt;/li&gt;\t&lt;li&gt;‘df1’&lt;/li&gt;\t&lt;li&gt;‘L’&lt;/li&gt;\t&lt;li&gt;‘logical’&lt;/li&gt;\t&lt;li&gt;‘m1’&lt;/li&gt;\t&lt;li&gt;‘m2’&lt;/li&gt;\t&lt;li&gt;‘m3’&lt;/li&gt;\t&lt;li&gt;‘number1’&lt;/li&gt;\t&lt;li&gt;‘number2’&lt;/li&gt;\t&lt;li&gt;‘string1’&lt;/li&gt;\t&lt;li&gt;‘string2’&lt;/li&gt;\t&lt;li&gt;‘strint1’&lt;/li&gt;\t&lt;li&gt;‘v1’&lt;/li&gt;\t&lt;li&gt;‘v2’&lt;/li&gt;\t&lt;li&gt;‘v4’&lt;/li&gt;\t&lt;li&gt;‘v5’&lt;/li&gt;&lt;/ol&gt;[1] 10통계 함수sum : 입력된 값의 합mean : 입력된 값의 평균median : 입력된 값의 중앙값var : 입력된 값의 표본 분산sd : 입력된 값의 표본 표준편차max : 입력된 값의 최댓값min : 입력된 값의 최솟값range : 입력된 값의 최댓값과 최솟값summary : 입력된 값의 요약값NA가 있는 경우에 na.rm=T 사용하면 NA 제외하고 계산v1&lt;-c(1:9)sum(v1)mean(v1)median(v1)var(v1)sd(v1)max(v1)min(v1)range(v1)summary(v1)45557.52.7386127875258391&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;9&lt;/li&gt;&lt;/ol&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.       1       3       5       5       7       9 R 데이터 핸들링데이터 이름 변경2차원 이상의 데이터 구조는 colnames와 rownames 함수를 사용하여 행과 열의 이름을 표현m1&lt;-matrix(c(1:6),nrow=2)colnames(m1)&lt;-c('c1','c2','c3')rownames(m1)&lt;-c('r1','r2')m1colnames(m1)rownames(m1)df&lt;-data.frame(x=c(1,2,3),y=c(4,5,6))colnames(df1)&lt;-c('c1','c2')rownames(df1)&lt;-c('r1','r2','r3')df1colnames(df1)rownames(df1)c1c2c3\tr1135\tr2246&lt;ol class=list-inline&gt;\t&lt;li&gt;‘c1’&lt;/li&gt;\t&lt;li&gt;‘c2’&lt;/li&gt;\t&lt;li&gt;‘c3’&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;‘r1’&lt;/li&gt;\t&lt;li&gt;‘r2’&lt;/li&gt;&lt;/ol&gt;c1c2\tr11a\tr22b\tr33c&lt;ol class=list-inline&gt;\t&lt;li&gt;‘c1’&lt;/li&gt;\t&lt;li&gt;‘c2’&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;‘r1’&lt;/li&gt;\t&lt;li&gt;‘r2’&lt;/li&gt;\t&lt;li&gt;‘r3’&lt;/li&gt;&lt;/ol&gt;데이터 추출대괄호([])를 이용하여 원하는 위치의 데이터를 얻을 수 있으며 행과 열의 이름으로 데이터를 얻을 수 있다.v1&lt;-c(3,6,9,12)v1[2]m1&lt;-matrix(c(1:6),nrow=3)m1m1[2,2]colnames(m1)&lt;-c('c1','c2')m1[,'c1']rownames(m1)&lt;-c('r1','r2','r3')m1['r3','c2']6\t14\t25\t365&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;&lt;/ol&gt;6데이터프레임에서는 $ 기호를 사용하여 원하는 열의 데이터를 구할 수 있으며, $와 []의 혼용이 가능하다.v1&lt;-c(1:6)v2&lt;-c(7:12)df1&lt;-data.frame(v1,v2)df1$v1df1$v2[3]&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;6&lt;/li&gt;&lt;/ol&gt;9데이터 결합행으로 결합하는 rbind와 열로 결합하는 cbind가 대표적이다.v1&lt;-c(1,2,3)v2&lt;-c(4,5,6)rbind(v1,v2)cbind(v1,v2)\tv1123\tv2456v1v2\t14\t25\t36행의 수 혹은 열의 수가 같아야 결합이 가능하다.하지만 벡터와 벡터의 결합에서는 부족한 데이터를 앞에서부터 다시 재활용하여 사용한 뒤, 오류와 함께 결과를 반환한다.v1&lt;-c(1,2,3)v2&lt;-c(4,5,6,7,8)rbind(v1,v2)Warning message in rbind(v1, v2):\"number of columns of result is not a multiple of vector length (arg 1)\"\tv112312\tv245678제어문반복문for과 while이 있다.for(i in 1:3){    print(i)}data&lt;-c('a','b','c')for (i in data){    print(i)}i&lt;-0while(i&lt;5){    print(i)    i&lt;-i+1}[1] 1[1] 2[1] 3[1] \"a\"[1] \"b\"[1] \"c\"[1] 0[1] 1[1] 2[1] 3[1] 4조건문참과 거짓에 따라 특정 코드가 수행될지 혹은 수행되지 않을지를 결정한다.number&lt;-5if(number&lt;5){    print('number은 5보다 작다')}else if (number&gt;5){    print('number은 5보다 크다')}else{    print('number은 5와 같다.')}number&lt;-3if(number&lt;5){    print('number은 5보다 작다')}else if (number&gt;5){    print('number은 5보다 크다')}else{    print('number은 5와 같다.')}number&lt;-7if(number&lt;5){    print('number은 5보다 작다')}else if (number&gt;5){    print('number은 5보다 크다')}else{    print('number은 5와 같다.')}[1] \"number은 5와 같다.\"[1] \"number은 5보다 작다\"[1] \"number은 5보다 크다\"사용자 정의 함수사용자가 원하는 변수로 정의 함수를 만들 수 있다.comparedTo5&lt;-function(number){    if(number&lt;5){    print('number은 5보다 작다')    }else if (number&gt;5){    print('number은 5보다 크다')    }else{    print('number은 5와 같다.')    }}comparedTo5(10)comparedTo5(3)comparedTo5(5)[1] \"number은 5보다 크다\"[1] \"number은 5보다 작다\"[1] \"number은 5와 같다.\"주석주석은 #를 사용하여 표시한다.# 1+1 계산 방법1+12통계분석에 사용되는 R 함수숫자 연산sqrt : 제곱근abs : 절댓값exp : e의 제곱수log : 밑이 e인 로그 값log10 : 밑이 10인 로그 값pi : 원주율round : 반올림 값ceiling : 올림floor : 내림문자 연산tolower : 소문자 변환toupper : 대문자 변환nchar : 문자열의 길이substr : 문자 일부분 추출strsplit : 구분자로 나누어 분할grepl : 문자열에 존재 확인gsub : 일부 문자를 대체data&lt;-'This is a pen'tolower(data)toupper(data)nchar(data)substr(data,9,13)strsplit(data,'is')grepl('pen',data)gsub('pen','banana',data)‘this is a pen’‘THIS IS A PEN’13‘a pen’\t&lt;ol class=list-inline&gt;\t'Th'\t' '\t' a pen'&lt;/ol&gt;TRUE‘This is a banana’벡터 연산length : 벡터의 길이paste : 구분자를 기준으로 결합cov : 공분산cor : 상관계수table : 데이터의 개수order : 벡터의 순서행렬 연산t : 전치행렬diag : 대각행렬%*% : 행렬의 곱데이터 탐색head : 데이터 앞의 일부분tail : 데이터 뒤의 일부분quantile : 4분위수x&lt;-c(1:12)head(x,5)tail(x)quantile(x)&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;7&lt;/li&gt;\t&lt;li&gt;8&lt;/li&gt;\t&lt;li&gt;9&lt;/li&gt;\t&lt;li&gt;10&lt;/li&gt;\t&lt;li&gt;11&lt;/li&gt;\t&lt;li&gt;12&lt;/li&gt;&lt;/ol&gt;&lt;dl class=dl-horizontal&gt;\t&lt;dt&gt;0%&lt;/dt&gt;\t\t&lt;dd&gt;1&lt;/dd&gt;\t&lt;dt&gt;25%&lt;/dt&gt;\t\t&lt;dd&gt;3.75&lt;/dd&gt;\t&lt;dt&gt;50%&lt;/dt&gt;\t\t&lt;dd&gt;6.5&lt;/dd&gt;\t&lt;dt&gt;75%&lt;/dt&gt;\t\t&lt;dd&gt;9.25&lt;/dd&gt;\t&lt;dt&gt;100%&lt;/dt&gt;\t\t&lt;dd&gt;12&lt;/dd&gt;&lt;/dl&gt;데이터 전처리subset : 조건식에 맞는 데이터 추출merge : 특정 공통된 열을 기준으로 병합apply : 열 또는 행별로 주어진 함수 적용df1&lt;-data.frame(x=c(1,1,1,2,2),y=c(2,3,4,3,3))df2&lt;-data.frame(x=c(1,2,3,4),z=c(5,6,7,8))subset(df1, x==1)merge(df1,df2,by=c('x'))apply(df1,1,sum) # 각 행에 함수 적용apply(df1,2,sum) # 각 열에 함수 적용xy\t12\t13\t14xyz\t125\t135\t145\t236\t236&lt;ol class=list-inline&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;&lt;/ol&gt;&lt;dl class=dl-horizontal&gt;\t&lt;dt&gt;x&lt;/dt&gt;\t\t&lt;dd&gt;7&lt;/dd&gt;\t&lt;dt&gt;y&lt;/dt&gt;\t\t&lt;dd&gt;15&lt;/dd&gt;&lt;/dl&gt;정규분포기본값은 표준 정규분포로 mean=0,sd=1이다.dnorm : 주어진 값에서 함수 값rnorm : 주어진 개수만큼 표본 추출pnorm : 주어진 값보다 작은 확률 값qnorm : 주어진 넓이 값을 갖는 x표본추출runif : 균일 분포에서 주어진 개수만큼 표본 추출sample : 주어진 데이터에서 주어진 개수만큼 표본을 추출날짜Sys.Date : 연, 월, 일 출력Sys.time : 연, 월, 일, 시간 출력as.Date : 주어진 값을 날짜 형식으로 변환format : 원하는 날짜 형식으로 변경as.POSIXct : 타임스탬프를 날짜 및 시간으로 변환타임 스탬프란 1970/1/1 UTC부터 날짜가 몇 초가 흘렀는지 나타내는 값Sys.Date()Sys.time()as.Date(\"2020-01-01\")format(Sys.Date(),'%Y/%m/%d')format(Sys.Date(),'%A') #A는 요일#시간 데이터의 unclass 값이 타임스탬프  unclass(Sys.time())as.POSIXct(1577804401,origin='1970-01-01')2021-11-04[1] \"2021-11-04 01:01:04 KST\"2020-01-01‘2021/11/04’‘목요일’1635955264.19852[1] \"2020-01-01 00:00:01 KST\"산점도plot : 산점도  type : p는 점, l은 직선, b는 점과 직선, n은 아무것도 표시하지 않음xlim : x축의 범위, ylim : y축의 범위xlab : x축의 이름, ylab : y축의 이름main : 산점도의 이름abline : 추가 직선  v : 수직선, h : 수평선col : 매개변수의 색상x&lt;-c(1:10)y&lt;-rnorm(10)plot(x,y,type='b',xlim=c(-2,12),ylim=c(-3,3),xlab='X axis',ylab='Y axis',main='Test plot')abline(v=c(1,10),col='blue')파일 읽기 쓰기read.csv : CSV 파일 불러오기write.csv : CSV 파일로 저장saveRDS : 분석 모델 및 R 파일 저장readRDS : 분석 모델 및 R 파일 읽기기타install.packages : 패키지 설치library : 패키지 호출getwd : 작업 디렉토리 확인setwd : 작업 디렉토리 설정데이터 마트데이터 마트란 데이터 웨어하우스로부터 특정 사용가가 관심을 갖는 데이터들을 주제별, 부서별로 추출하여 모은 비교적 작은 데이터 웨어하우스데이터 전처리데이터 정제와 분석 변수 처리 과정데이터 정제 : 결측값과 이상값을 처리분석 변수 처리 : 변수 선택, 차원 축소, 파생변수 생성, 변수 변환, 클래스 불균형파생변수 : 목적과 조건에 따라 생성한 변수요약변수 : 기본적인 통계 자료를 추출한 변수R 패키지 활용reshape 패키지https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/reshapemelt : 특정 변수를 기준으로 나머지 변수에 대한 세분화된 데이터 생성install.packages ('reshape')package 'reshape' successfully unpacked and MD5 sums checkedThe downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packagesstudent_number&lt;-c(1,2,1,2)semester&lt;-c(1,1,2,2)math_score&lt;-c(60,90,70,90)english_score&lt;-c(80,70,40,60)score&lt;-data.frame(student_number,semester,math_score,english_score)rownames(score)&lt;-c(1,2,3,4)scorelibrary(reshape)melt(score,id=c('student_number','semester'))student_numbersemestermath_scoreenglish_score\t1 1 6080\t2 1 9070\t1 2 7040\t2 2 9060Warning message:\"package 'reshape' was built under R version 3.6.3\"student_numbersemestervariablevalue\t1            1            math_score   60           \t2            1            math_score   90           \t1            2            math_score   70           \t2            2            math_score   90           \t1            1            english_score80           \t2            1            english_score70           \t1            2            english_score40           \t2            2            english_score60           cast : melt로 변환한 데이터를 요약을 위해 새롭게 가공melted_score&lt;-melt(score,id=c('student_number','semester'))# 학생의 과목별 평균 구하기cast(melted_score,student_number~variable,mean)# 학생의 학기별 평균 점수cast(melted_score,student_number~semester,mean)# 학생의 과목별 최대 점수cast(melted_score,student_number~variable,max)student_numbermath_scoreenglish_score\t1 6560\t2 9065student_number12\t1 7055\t2 8075student_numbermath_scoreenglish_score\t1 7080\t2 9070sqldf 패키지sqldf는 표준 SQL 문장을 활용하여 R에서 데이터프레임을 다루게 한다.install.packages('sqldf')also installing the dependencies 'ellipsis', 'fastmap', 'bit', 'vctrs', 'rlang', 'cachem', 'bit64', 'blob', 'DBI', 'memoise', 'Rcpp', 'gsubfn', 'proto', 'RSQLite', 'chron'  There are binary versions available but the source versions are later:        binary source needs_compilationrlang   0.4.11 0.4.12              TRUEcachem   1.0.4  1.0.6              TRUEblob     1.2.1  1.2.2             FALSERcpp     1.0.6  1.0.7              TRUERSQLite  2.2.7  2.2.8              TRUE  Binaries will be installedpackage 'ellipsis' successfully unpacked and MD5 sums checkedpackage 'fastmap' successfully unpacked and MD5 sums checkedpackage 'bit' successfully unpacked and MD5 sums checkedpackage 'vctrs' successfully unpacked and MD5 sums checkedpackage 'rlang' successfully unpacked and MD5 sums checkedpackage 'cachem' successfully unpacked and MD5 sums checkedpackage 'bit64' successfully unpacked and MD5 sums checkedpackage 'DBI' successfully unpacked and MD5 sums checkedpackage 'memoise' successfully unpacked and MD5 sums checkedpackage 'Rcpp' successfully unpacked and MD5 sums checkedWarning message:\"cannot remove prior installation of package 'Rcpp'\"Warning message in file.copy(savedcopy, lib, recursive = TRUE):\"C:\\Users\\keonj\\anaconda3\\Lib\\R\\library\\00LOCK\\Rcpp\\libs\\x64\\Rcpp.dll를 C:\\Users\\keonj\\anaconda3\\Lib\\R\\library\\Rcpp\\libs\\x64\\Rcpp.dll로 복사하는데 문제가 발생했습니다: Permission denied\"Warning message:\"restored 'Rcpp'\"package 'gsubfn' successfully unpacked and MD5 sums checkedpackage 'proto' successfully unpacked and MD5 sums checkedpackage 'RSQLite' successfully unpacked and MD5 sums checkedpackage 'chron' successfully unpacked and MD5 sums checkedpackage 'sqldf' successfully unpacked and MD5 sums checkedThe downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packagesinstalling the source package 'blob'library(sqldf)sqldf('select * from score')sqldf('select * from score where student_number=1')sqldf('select avg(math_score),avg(english_score) from score group by student_number')student_numbersemestermath_scoreenglish_score\t1 1 6080\t2 1 9070\t1 2 7040\t2 2 9060student_numbersemestermath_scoreenglish_score\t1 1 6080\t1 2 7040avg(math_score)avg(english_score)\t6560\t9065plyr 패키지apply함수를 기반으로 데이터를 분리하고 다시 결합하는 기능을 제공한다.https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html입력 데이터 구조, 출력 데이터 구조, plyex) ddply-데이터프레임, 데이터프레임, plyinstall.packages('plyr')package 'plyr' successfully unpacked and MD5 sums checkedWarning message:\"cannot remove prior installation of package 'plyr'\"Warning message in file.copy(savedcopy, lib, recursive = TRUE):\"C:\\Users\\keonj\\anaconda3\\Lib\\R\\library\\00LOCK\\plyr\\libs\\x64\\plyr.dll를 C:\\Users\\keonj\\anaconda3\\Lib\\R\\library\\plyr\\libs\\x64\\plyr.dll로 복사하는데 문제가 발생했습니다: Permission denied\"Warning message:\"restored 'plyr'\"The downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packagesclass&lt;-c('A','A','B','B')math&lt;-c(50,70,60,90)english&lt;-c(70,80,60,80)score&lt;-data.frame(class,math,english)scorelibrary(plyr)#summarise는 데이터 요약ddply(score,'class',summarise,math_avg=mean(math),eng_avg=mean(english))#transform은 데이터 추가ddply(score,'class',transform,math_avg=mean(math),eng_avg=mean(english))classmathenglish\tA 5070\tA 7080\tB 6060\tB 9080Attaching package: 'plyr'The following objects are masked from 'package:reshape':    rename, round_anyclassmath_avgeng_avg\tA 6075\tB 7570classmathenglishmath_avgeng_avg\tA 50706075\tA 70806075\tB 60607570\tB 90807570year&lt;-c(2012,2012,2012,2012,2013,2013,2013,2013)month&lt;-c(1,1,2,2,1,1,2,2)value&lt;-c(3,5,7,9,1,5,4,6)data&lt;-data.frame(year,month,value)dataddply(data,c('year','month'),summarise,value_avg=mean(value))ddply(data,c('year','month'),function(x){    value_avg=mean(x$value)    value_sd=sd(x$value)    data.frame(avg_sd=value_avg/value_sd)})yearmonthvalue\t20121   3   \t20121   5   \t20122   7   \t20122   9   \t20131   1   \t20131   5   \t20132   4   \t20132   6   yearmonthvalue_avg\t20121   4   \t20122   8   \t20131   3   \t20132   5   yearmonthavg_sd\t2012    1       2.828427\t2012    2       5.656854\t2013    1       1.060660\t2013    2       3.535534data.table 패키지특정 칼럼별 주소값을 갖는 인덱스를 생성하여 연산과 검색을 빠르게 수행할 수 있다.install.packages('data.table')  There is a binary version available but the source version is later:           binary source needs_compilationdata.table 1.14.0 1.14.2              TRUE  Binaries will be installedpackage 'data.table' successfully unpacked and MD5 sums checkedThe downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packageslibrary('data.table')year&lt;-rep(c(2012:2015),each=12000000)month&lt;-rep(rep(c(1:12),each=1000000),4)value&lt;-runif(48000000)# 데이터 프레임과 데이터 테이블 생성DataFrame&lt;-data.frame(year,month,value)DataTable&lt;-as.data.table(DataFrame)# 데이터 프레임의 검색 시간 측정system.time(DataFrame[DataFrame$year==2012,])# 데이터 테이블의 검색 시간 측정system.time(DataTable[DataTable$year==2012,])# 칼럼이 키 값으로 설정될 경우 자동 오름차순 정렬  setkey(DataTable,year)# 키 값으로 설정된 칼럼과 J 표현식을 사용한 검색시간 측정 system.time(DataTable[J(2012)])Warning message:\"package 'data.table' was built under R version 3.6.3\"Attaching package: 'data.table'The following object is masked from 'package:reshape':    melt   user  system elapsed    0.87    0.24    1.11    user  system elapsed    0.47    0.21    0.29    user  system elapsed    0.32    0.06    0.12 데이터 탐색탐색적 데이터 분석데이터를 이해하고 의미있는 관계를 찾기 위해 데이터의 통계값과 분포 등을 시각화하고 분석하는 것head(iris,3)summary(iris)str(iris)Sepal.LengthSepal.WidthPetal.LengthPetal.WidthSpecies\t5.1   3.5   1.4   0.2   setosa\t4.9   3.0   1.4   0.2   setosa\t4.7   3.2   1.3   0.2   setosa  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width    Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100   1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300   Median :5.800   Median :3.000   Median :4.350   Median :1.300   Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199   3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800   Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500         Species   setosa    :50   versicolor:50   virginica :50                                                  'data.frame':\t150 obs. of  5 variables: $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...결측값결측값은 존재하지 않는 데이터를 의미하며 NA로 표현하지만 null, 공백, -1과 같은 표현을 할 때도 있다.결측치를 처리하는 것은 가장 중요한 과정이다.Amelia와 DMwR2가 대표적이다.Ameliamissmap 함수로 결측값을 시각화하기 좋다.install.packages('Amelia')also installing the dependency 'RcppArmadillo'  There are binary versions available but the source versions are later:                  binary     source needs_compilationRcppArmadillo 0.10.4.0.0 0.10.7.0.0              TRUEAmelia             1.7.6      1.8.0              TRUE  Binaries will be installedpackage 'RcppArmadillo' successfully unpacked and MD5 sums checkedpackage 'Amelia' successfully unpacked and MD5 sums checkedThe downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packagescopy_iris&lt;-iriscopy_iris[sample(1:150,30),1]&lt;-NA #결측값 생성 library(Amelia)missmap(copy_iris)Warning message:\"package 'Amelia' was built under R version 3.6.3\"Loading required package: Rcpp## ## Amelia II: Multiple Imputation## (Version 1.7.6, built: 2019-11-24)## Copyright (C) 2005-2021 James Honaker, Gary King and Matthew Blackwell## Refer to http://gking.harvard.edu/amelia/ for more information## 결측값 대치 방법단순 대치법데이터를 삭제하는 방법이다.대량의 데이터 손실이 발생할 수 있다.complete.cases 함수가 있다.하나의 열에 결측값이 존재하면 FALSE, 존재하지 않으면 TRUE를 반환한다.copy_iris&lt;-irisdim(copy_iris) # 기존 데이터copy_iris[sample(1:150,30),1]&lt;-NA # 결측값 생성copy_iris&lt;-copy_iris[complete.cases(copy_iris),] # 단순 대치법dim(copy_iris) # 결측값 처리 후&lt;ol class=list-inline&gt;\t&lt;li&gt;150&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;120&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;&lt;/ol&gt;평균 대치법평균 혹은 중앙값으로 결측값을 대치하여 완전한 자료로 만드는 방법이다.비조건부 평균 대치법 : 데이터의 평균값으로 결측값을 대치하는 방법조건부 평균 대치법 : 실제 값들을 분석하여 회귀분석을 사용하는 방법DMwR2 패키지의 central Imputation 함수가 있다.install.packages('DMwR2')package 'DMwR2' successfully unpacked and MD5 sums checkedThe downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packagescopy_iris&lt;-iriscopy_iris[sample(1:150,30),1]&lt;-NA # 결측값 생성meanValue&lt;-mean(copy_iris$Sepal.Length,na.rm=T) #결측값을 제외한 평균  copy_iris$Sepal.Length[is.na(copy_iris$Sepal.Length)] &lt;-meanValue # 평균 대치  # central Imputation 사용library(DMwR2)copy_iris[sample(1:150,30),1]&lt;-NA # 결측값 생성copy_iris&lt;-centralImputation(copy_iris)단순 확률 대치법평균 대치법에서 추정량 표준 오차의 과소 추정 문제를 보완하고자 고안된 방법으로 KNN 방법이 있다.KNN 방법 : KNN 알고리즘으로 주변 K개의 데이터 중 가장 많은 데이터로 대치하는 방법copy_iris&lt;-iriscopy_iris[sample(1:150,30),1]&lt;-NA # 결측값 생성copy_iris&lt;-knnImputation(copy_iris,k=10)다중 대치법여러 번의 대치를 통해 n개의 임의 완전자료를 만드는 방법으로 대치, 분석, 결합의 세 단계로 구성된다.copy_iris&lt;-iriscopy_iris[sample(1:150,30),1]&lt;-NA # 결측값 생성iris_imp&lt;-amelia(copy_iris,m=3,cs='Species') #cs는 cross-sectional로 분석에 포함될 정보copy_iris$Sepal.Length&lt;-iris_imp$imputation[[3]]$Sepal.Length-- Imputation 1 --  1  2  3-- Imputation 2 --  1  2  3-- Imputation 3 --  1  2  3이상값이상값이란 값이 존재하지 않는 결측값과 달리 다른 데이터와 비교했을 때 극단적으로 크거나 작은값ESD(Extreme Studentized Deviation)ESD는 평균으로부터 ‘표준편차 3’만큼 떨어진 값들을 이상값으로 인식하는 방법이다.전체 데이터의 약 0.3퍼센트를 이상값으로 구분한다.사분위수사분위수를 이용하여 25%(Q1), 75%(Q3)에 해당하는 값을 활용하여 이상치를 판단한다.IQR이란 Q1~Q3 사이를 의미하며 사분범위라고 한다.사분범위에서 1.5분위수 벗어나는 경우를 이상치로 판단한다.Q1-1.5IQR 미만이거나 Q3+1.5IQR 초과면 이상값으로 판단한다.",
        "url": "/license-adsp5"
    }
    ,
    
    "license-adsp4": {
        "title": "ADSP (4) 분석 마스터플랜",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP 합격 후기ADSP를 준비하면서 공부한 내용을 정리한 글입니다.2과목 2장 분석 마스터플랜에 대한 부분을 정리한 글입니다.  마스터플랜 수립          수행 과제 도출 및 우선순위 평가      로드맵 수립        분석 거버넌스 체계 수립  데이터 거버넌스 체계 수립마스터플랜 수립분석 마스터플랜이란 어떤 하나의 분석 프로젝트를 위한 전체 설계도와 같다.분석 마스터플랜 수립 프레임워크수행 과제 도출 및 우선순위 평가전략적 중요도  전략적 필요성  시급성실행 용이성  투자 용이성  기술 용이성ROI 요소투자비용 요소 (Investment)Volume, Variety, Velocity비즈니스 효과 (Return)Value우선 평가 기준전략적 중요도에 따른 시급성을 먼저 판단 - Value난이도는 데이터 분석의 적합성 여부를 따져본다. - Volume, Variety, Velocity포트폴리오 사분면 분석을 활용한 우선순위 평가 기준난이도와 시급성에 따라 포트폴리오 분석을 진행한다.로드맵 수립분석 체계 도입 - 분석 유효성 검증 - 분석 확산 및 고도화세부 이행계획 수립분석 데이터 수집/확보 - 분석 데이터 준비 - 모델링 및 평가분석 거버넌스 체계 수립분석 거버넌스 체계 구성요소조직, 과제 기획 및 운영 프로세스, 분석 관련 시스템, 데이터, 분석 관련 교육 및 마인드 육성 체계분석 준비도분석 업무 파악, 분석 인력 및 조직, 분석 기법, 분석 데이터, 분석 문화, IT 인프라분석 성숙도  도입분석 시작, 환경과 시스템 구축  활용분석 결과를 업무에 적용  확산전사 차원에서 분석 관리, 공유  최적화분석을 진화시켜 혁신 및 성과 향상에 기여분석 수준 진단 결과데이터 거버넌스 체계 수립데이터 거버넌스 구성 요소원칙, 조직, 프로세스데이터 거버넌스 체계  데이터 표준화표준 용어 설정, 명명 규칙, 메타데이터 구축, 데이터 사전 구축  데이터 관리 체계 메타데이터와 데이터 사전의 관리 원칙 수립  데이터 저장소 관리전사 차원의 저장소 구성  표준화 활동표준 준수 여부를 주기적으로 점검하고 모니터링 실시데이터 분석 조직 유형  집중형 조직 구조조직 내 별도의 독립적인 분석 전담 조직 구성  기능 중심의 조직 구조일반적으로 분석을 수행하는 형태로 별도 조직을 구성하지 않고 해당 업무 부서에서 직접 분석  분산형 조직 구조분석 조직의 인력을 현업 부서에 배치해 분석 업무를 수행분석 과제 관리 프로세스과제 발굴  분석 idea 발굴  분석 과제 후보 제안  분석 과제 확정과제 수행  팀 구성  분석 과제 실행  분석 과제 진행 관리  결과 공유/개선",
        "url": "/license-adsp4"
    }
    ,
    
    "license-adsp3": {
        "title": "ADSP (3) 데이터 분석 기회의 이해",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP 합격 후기ADSP를 준비하면서 공부한 내용을 정리한 글입니다.2과목 1장 데이터 분석 기회의 이해에 대한 부분을 정리한 글입니다.  분석 기획  분석 방법론 개요          전통적인 분석 방법론      빅데이터 분석 방법론        분석 과제 발굴  분석 프로세스 관리 방안분석 기획분석 대상이 무엇인지 알고 있고 그 분석 방법도 알고 있다면 ‘최적화’분석 대상이 무엇인지 알고 있지만, 그 분석 방법을 모른다면 ‘솔루션’분석 대상이 무엇인지 모르고 그 분석 방법도 모른다면 ‘발견’분석 대상이 무엇인지는 모르지만, 그 분석 방법은 알고 있다면 ‘통찰’과제 중심적인 접근 방식빠르게 해결해야 하는 경우, 빠른 수행과 문제 해결이 목적장기적인 마스터플랜 방식지속적인 분석 내재화를 위한 경우, 정확도와 무엇이 문제인가에 대한 문제 정의가 목적분석 기획 시 고려사항가용 데이터 고려적절한 활용 방안과 유스케이스의 탐색장애 요소에 대한 사전 계획 수립분석 방법론 개요합리적 의사결정 방해요소고정 관념, 편향된 생각, 프레이밍 효과분석 방법론의 생성 과정방법론 -(내재화)-암묵지-(형식화)-형식지-(체계화)-방법론분석 방법론이 적용되는 업무 특성에 따른 모델      폭포수 모델단계적으로 진행되는 방식으로 하향식 방향        프로토타입 모델사용자 중심의 개발 방법        나선형 모델반복을 통해 점증적으로 개발하지만 위험 요소를 사전에 제거        계층적 프로세스 모델최상의 계층인 몇 개의 단계로 구성되어 있고 하나의 단계는 여러 개의 태스크로 구성되고 하나의 태스크는 여러개의 스텝으로 구성된다.단계: 프로세스 그룹을 통해 완성된 단계별 산출물 생성태스크: 단계를 구성하는 단위 활동스텝: WBS의 워크패이지에 해당  전통적인 분석 방법론      KDD 분석 방법론데이터로부터 통계적 패턴이나 지식을 찾기 위해 체계적으로 정리한 데이터 마이닝 프로세스데이터셋 선택 - 데이터 전처리 - 데이터 변환 - 데이터 마이닝 - 해석과 평가        CRISP-DM 분석 방법론KDD 분석 방법론보다 더 세분화업무 이해 - 데이터 이해 - 데이터 준비 - 모델링 - 평가 - 전개  빅데이터 분석 방법론분석 기획, 데이터 준비, 데이터 분석, 시스템 구현, 평가 및 전개의 5개 단계와 각각의 태스크와 스텝이 순차적으로 진행5단계 빅데이터 분석 방법론 플로우단계별분석 기획 - 데이터 준비 - 데이터 분석 - 시스템 구현 - 평가 및 전개단계별 수행 태스크분석 기획      비즈니스 이해 및 범위 설정비즈니스 이해 - 프로젝트 범위 설정        프로젝트 정의 및 계획 수립데이터 분석 프로젝트 정의 - 프로젝트 수행 계획 수립        프로젝트 위험계획 수립데이터 분석 위험 식별 - 위험 대응 계획 수립  데이터 준비      필요 데이터 정의데이터 정의 - 데이터 획득 방안 수립        데이터 스토어 설계정형 데이터 스토어 설계 - 비정형 데이터 스토어 설계        데이터 수집 및 정합성 검정데이터 수집 및 저장 - 데이터 정합성 점검  데이터 분석      분석용 데이터 준비비즈니스 룰 확인 - 분석용 데이터셋 준비        텍스트 분석텍스트 데이터 확인 및 추출 - 텍스트 데이터 분석        탐색적 분석탐색적 데이터 분석 - 데이터 시각화        모델링데이터 분할 - 데이터 모델링 - 모델 적용 및 운영 방안        모델 평가 및 검증모델 평가 - 모델 검증  시스템 구현      설계 및 구현시스템 분석 및 설계 - 시스템 구현        시스템 테스트 및 운영시스템 테스트 - 시스템 운영 계획  평가 및 전개      모델 발전 계획 수립모델 발전 계획        프로젝트 평가 및 보고프로젝트 성과 평가 - 프로젝트 종료  분석 과제 발굴하향식 접근법분석 대상을 알고 있을 때 문제 탐색 단계 - 문제 정의 단계 - 해결방안 탐색 단계 - 타당성 컴토 단계문제 탐색 단계      비즈니스 모델 탐색 기법업무, 제품, 고객, 규제와 감사, 지원 인프라        분석 기회 발굴 범위의 확장거시적 관점, 경쟁자 확대 관점, 시장의 니즈 탐색, 역량의 재해석        외부 참조 모델 기반 문제 탐색유사 동종 업계에서의 문제 탐색        분석 유스케이스유사 및 동종 사례 탐색  문제 정의 단계해결 방안 탐색 단계타당성 검토 단계  경제적 타당성  데이터 및 기술적 타당성상향식 접근법분석 대상을 모르고 있을 때분석 후 가치를 찾음지도, 비지도 학습 - 프로토타입지도학습정답이 있는 데이터를 활용하여 분석 모델 학습ex) 머신 러닝, DT, 인공신경망 모형, 분류, 회귀비지도학습정답이 없는 데이터를 활용하여 학습연관성, 유사성, 결합을 중심으로 데이터의 상태를 표현장바구니 분석, 기술통계, 프로파일링, 군집 분석, 주성분 분석, 다차원 척도프로토타이핑 접근법먼저 분석을 시도하고 결과를 확인하면서 개선가설의 생성 - 디자인에 대한 실험 - 실제 환경에서의 테스트 - 결과로부터 인사이트 도출 및 가설 확인분석 프로세스 관리 방안데이터의 양, 데이터의 복잡도, 분석의 속도, 분석 복잡도, 정확도와 정밀도",
        "url": "/license-adsp3"
    }
    ,
    
    "license-adsp2": {
        "title": "ADSP (2) 데이터의 가치와 미래",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP 합격 후기ADSP를 준비하면서 공부한 내용을 정리한 글입니다.1과목 2장 데이터의 가치와 미래에 대한 부분을 정리한 글입니다.  빅데이터의 이해          빅데이터의 특징      빅데이터의 출현 배경      빅데이터의 기능과 변화        데이터의 가치와 미래          빅데이터의 가치      빅데이터 활용 기술      빅데이터의 위기 요인과 통제 방안        가치창조를 위한 데이터 사이언스와 전략 인사이트          빅데이터 분석과 전략 인사이트      데이터 사이언스에 대한 이해      빅데이터의 이해빅데이터란 큰 용량과 복잡성으로 기존 애플리케이션이나 툴로는 다루기 어려운 데이터셋의 집합빅데이터의 특징3VVolume데이터 양의 증가Variety데이터 유형 증가Velocity데이터 수집 및 처리 속도의 증가4V4V는 3V에 추가된 특징이다.Value데이터 가치의 중요성Veracity예측 분석 결과에 대한 신뢰성의 중요성밑에 두개는 의견이 갈린다Visualization데이터의 시각화Variability데이터의 가변성Validility데이터의 정확성Volarility데이터의 휘발성빅데이터 출현 배경  데이터의 양적 증가과학기술의 발달로 인한 데이터의 양적 증가  산업계의 변화정보의 축적과 기술이 만나 새로운 가치를 창출할 수 있는 변화의 상태  학계의 변화다양한 분야에서의 데이터 이용으로 필요한 기술 아키텍처 및 통계 도구의 발전  관련 기술의 발전디지털화, 저장 기술의 발전과 가격 하락, 인터넷의 발전과 클라우드 컴퓨팅와 같은 빅데이터와 연관된 기술의 발전빅데이터의 기능과 변화빅데이터는 석탄, 철, 원유, 렌즈, 플랫폼과 같은 역할을 한다.빅데이터로 인한 변화사전처리 -&gt; 사후처리표본조사 -&gt; 전수조사질 -&gt; 양인과관계 -&gt; 상관관계데이터 처리, 저장, 분석, 아키텍처, 클라우드 컴퓨팅과 같은 기술 변화데이터의 양, 유형, 수집 및 처리 기술과 같은 데이터의 변화데이터 사이언티스트, 데이터 중심 조직과 같은 인재 조직 변화데이터의 가치와 미래빅데이터의 가치빅데이터의 가치는 어떻게 활용할 것인지에 달렸다.데이터의 활용 방식, 가치 창출 방식, 분석 기술의 발전과 같은 이유로 가치 산정은 어렵다.빅데이터 활용 기술      연관규칙 학습 (Association rule learning)변인간의 상관 관계를 찾는 방법        유형분석 (Classification tree analysis)새로운 사건이 속할 범주를 찾는 방법        유전 알고리즘 (Genetic algorithms)최적화가 필요한 문제의 해결책의 진화 방법        기계학습 (Machine learning)훈련 데이터로부터 학습한 알려진 특성을 활용해 예측하는 방법        회귀분석 (Regression analysis)독립변수를 조작하면서 종속변수가 어떻게 변하는지 보며 관계를 파악하는 방법        감정분석 (Sentiment analysis)특정 주제에 대한 말이나 글의 감정을 분석하는 방법        소셜 네트워크 분석 (Social network analysis)사회 관계망 분석으로 사람 사이의 관계를 분석하는 방법  빅데이터의 위기 요인과 통제 방안위기 요인      사생활 침해개인의 사생활 침해 및 정보의 오용 위험        책임 원칙 훼손알고리즘으로 인한 피해 발생 위험        데이터 오용데이터 과신 및 잘못된 지표 사용으로 피해 발생 위험  통제 방안      사생활 침해의 통제 방안제공자의 ‘동의’에서 사용자의 ‘책임’으로개인정보 비식별 기술 (데이터 마스킹, 가명 처리, 총계 처리, 값 삭제, 범주화)        책임 원칙 훼손의 통제 방안결과 기반 책임 원칙 고수        알고리즘 접근 허용알고리즘으로 인한 피해 발생 시 알고리즘 접근을 허용하여 피해자 구제  가치창조를 위한 데이터 사이언스와 전략 인사이트빅데이터 분석과 전략 인사이트빅데이터에서 중요한 것은 ‘크기’가 아니라 ‘인사이트’이다.데이터 분석을 많이 사용하는 것이 아닌 전략적으로 사용해야 효과적인 운영이 가능하다.일차원적 분석에서 시작하여 전략 도출을 위한 가치 기반 분석까지 확장되어야한다.데이터 사이언스에 대한 이해데이터 사이언스는 데이터로부터 의미 있는 정보를 추출해내는 학문데이터 마이닝은 분석에 포커스를 둔다면 데이터 사이언스는 분석뿐 아니라 효과적으로 구현하고 전달하는 과정까지 포괄하는 개념수학, 확률 모델, 머신러닝, 분석학, 패턴 인식과 같은 Analytics,프로그래밍, 데이터 엔지니어링, 데이터 웨어하우징과 같은 Data Management,커뮤니케이션, 시각화, 프레젠테이션, 스토리텔링과 같은 비즈니스 분석으로 구성된다.기술적 능력으로 이루어진 하드 스킬과 분석, 전달, 협력으로 이루어진 소프트 스킬이 합쳐져야한다.따라서 인문학적 사고 특성도 길러야한다.",
        "url": "/license-adsp2"
    }
    ,
    
    "license-adsp1": {
        "title": "ADSP (1) 데이터의 이해",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP 합격 후기ADSP를 준비하면서 공부한 내용을 정리한 글입니다.1과목 1장 데이터 이해에 대한 부분을 정리한 글입니다.  데이터의 정의          데이터의 유형        데이터와 정보          DIKW 피라미드      데이터의 단위        데이터베이스 개요          데이터베이스의 특징      데이터베이스의 활용      데이터베이스의 종류      SQL의 이해      데이터베이스 구성요소      데이터의 정의데이터란?기술적이고 사실적인 의미의 자료. 객관적 사실정보는 데이터로 부터 얻은 것으로 가공된 자료존재적 특성있는 그대로의 객관적 사실당위적 특성데이터는 추론, 예측, 전망, 추정을 위한 근거데이터의 유형정성적 데이터 (언어, 문자)집합으로 표현할 수 없는 기준이 명확하지 않은 데이터정량적 데이터 (수치, 모형, 기호)집합으로 표현할 수 있는 기준이 명확한 데이터정형 데이터 (CSV, 엑셀)고정된 틀을 가지고 있으며 연산이 가능한 데이터로 관계형 DB에 저장하며 수집과 관리가 용이비정형 데이터 (소셜 데이터, 댓글, 음성, 영상)고정된 틀이 존재하지 않고 연산이 불가능관계형 DB가 아닌 NoSQL DB에 저장반정형 데이터 (XML, JSON, 센서 데이터)고정된 형태는 있지만 연산이 불가능테이블 형태보다는 파일 형태로 저장하여 가공을 거쳐 정형 데이터로 변환 가능암묵지와 형식지암묵지학습과 체험을 통해 개인에게 습득되어 있지만 겉으로 드러나지 않는 상태의 지식형식지암묵지를 여러 사람이 공유할 수 있게 형상화된 지식개인에 내면화된 암묵지가 출화하고 이를 개인의 지식으로 연결화 되는 과정을 거치면 조직의 지식으로 공통화되어 형식지가 된다.데이터와 정보DIKW 피라미드데이터 (Data)개별 데이터 자체는 의미가 중요하지 않은 객관적 사실정보 (Information)데이터의 가공, 처리와 데이터 간 연관 관계 속에서 의미 도출정보가 내포하는 의미는 유용하지 않을 수 있음지식 (Knowledge)데이터를 통해 얻은 정보를 구조화하여 유의미한 정보를 분류하고 경험과 결합해 고유의 지식으로 내재화지혜 (Wisdom)지식의 축적과 아이디어가 결합된 창의적 산물데이터 단위비트‘0’과 ‘1’의 두 가지 값으로 신호를 나타내는 최소단위바이트8개의 비트로 구성된 데이터의 양을 나타내는 단위숫자와 영어는 1바이트, 한글은 2바이트킬로-메가-기가-테라-페타-엑사-제타-요타 (각 단위는 1024배)데이터베이스 개요DB체계적으로 수집, 축적하여 다양한 용도와 방법으로 이용할 수 있게 정리한 정보의 집합체DBMS이용자가 쉽게 데이터베이스를 구축, 유지할 수 있게 하는 관리 소프트웨어데이터베이스의 특징통합된 데이터동일한 내용의 데이터가 중복되어 있지 않다.저장된 데이터컴퓨터가 매체에 접근할 수 있는 저장 매체에 저장되어 있다.공용 데이터여러 사용자가 공유할 수 있다.변화하는 데이터삽입, 수정, 삭제를 통해 항상 현재의 정확한 데이터를 유지해야 한다.정보의 축적 및 전달 측면기계의 가독성대량의 정보를 일정한 형식에 따라 정보처리기기가 읽고 쓸 수 있다.검색 가능성다양한 방법으로 필요한 정보를 검색할 수 있다.원격 조작성정보통신망을 통해 원거리에서도 즉시 온라인으로 이용 가능하다.트랜잭션 특성트랜잭션이란 데이터 베이스에서 명령을 수행하는 하나의 논리적 기능 단위원자성데이터베이스에 모두 적용되거나 모두 적용되지 않아야 한다일관성트랜잭션의 결과는 항상 일관성을 띠어야 한다고립성하나의 트랜잭션이 다른 트랜잭션에 영향을 주지 않아야 한다지속성트랜잭션이 성공적으로 수행된 경우 그 결과는 영구적이어야 한다데이터베이스 활용기업 내부의 데이터베이스 활용인하우스 DB, OLTP, OLAP, CRM, SCM, ERP, BI, RTE 등이 있다.사회 기반 구조 데이터베이스물류 부문CALS, PORT-MIS, KROIS지리부문GIS, LBS, SIM교통부문ITS의료부문PACS, U-Health교육부문NEIS데이터베이스의 종류관계형 데이터베이스데이터를 테이블에 저장되고 하나의 열은 하나의 속성을 나타내고 같은 속성 값만 가진다. 정형 데이터를 다루는 데 좋다.Oracle, MySQL, MS-SQL, SQLiteNoSQL비관계형을 의미하며 대용량의 데이터 분석 및 분산 처리에 용이하다.MongoDB, Dynamo, BigtableSQL의 이해SQL은 DBMS에서 데이터베이스에 내리는 명령이다.DB마다 문법이 다르지만 기본적인 데이터 추출과 분석에 사용되는 문법은 거의 동일하다.데이터 정의 언어 (DDL)CREATE, ALERT, RENAME, DROP데이터 조작 언어 (DML)SELECT, INSERT, UPDATE, DELETE데이터 제어 언어 (DCL)GRANT, REVOKE트랜잭션 제어 언어 (TCL)COMMIT, SAVEPOINT, ROLLBACK데이터베이스 구성요소인스턴스하나의 객체를 의미 (홍길동, 남자,000-0000-0000)속성객체를 표현하기 위해 사용되는 값 (이름, 성별, 주민번호)엔터티데이터의 집합, 테이블과 달리 개념적인 존재메타데이터데이터를 설명하는 데이터인덱스데이터베이스에 저장할 때 지정되는 데이터의 이름",
        "url": "/license-adsp1"
    }
    ,
    
    "study-ml4": {
        "title": "머신러닝 정리 (4) &lt;br&gt; 비지도학습 (2)",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학    머신러닝 정리 (6)-모델 평가와 성능 향상머신러닝 정리 (4) - 비지도학습 (2)본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python비지도학습 (2)  군집          계층적 군집분석      비계층적 군집분석        K-평균 군집  병합 군집  DBSCAN  군집 알고리즘의 비교와 평가          타깃 값으로 군집 평가하기      타깃 값 없이 군집 평가하기      군집군집(clusterling)은 데이터셋을 클러스터라는 그룹으로 나누는 작업입니다.기본적으로 거리에 관련된 측정 방법을 활용합니다.한 클러스터 안의 데이터 포인트끼리는 매우 비슷하고 다른 클러스터의 데이터 포인트와는 구분되도록 데이터를 나누는 것이 목표입니다.유사성을 측정하는 것에는 군집 간 분산이 최대화 되거나, 군집 내 분산을 최소화하게 됩니다.종류계층적 군집분석각 요소들로부터 시작한 클러스터들이 계층 구조를 이루도록 군집분석을 수행합니다.이 때 만들어진 계층구조를 덴드로그램이라고 합니다.비계층적 군집분석각 클러스터의 계층을 고려하지 않고 평면적으로 군집분석을 수행합니다.K-평균 군집k-평균(k-means) 군집은 데이터의 어떤 영역을 대표하는 클러스터 중심을 찾습니다.알고리즘은 먼저 데이터 포인트를 가장 가까운 클러스터 중심에 할당하고 클러스터에 할당된 데이터 포인트의 평균으로 클러스터 중심을 다시 지정합니다.클러스터에 할당되는 데이터 포인트에 변화가 없을 때 알고리즘이 종료됩니다.  데이터 포인트를 무작위로 초기화합니다.  각 데이터 포인트를 가장 가까운 클러스터 중심에 할당합니다.  할당한 포인트의 평균값으로 클러스터 중심을 갱신합니다.  더이상 포인트에 변화가 없다면 알고리즘이 멈춥니다.K 값을 설정하는 방법으로는 elbow method, silhouette method와 같은 방법이 있습니다.K-means 사용 방법from sklearn.datasets import make_blobsfrom sklearn.cluster import KMeans# 인위적으로 2차원 데이터를 생성합니다X, y = make_blobs(random_state=1)# 군집 모델을 만듭니다kmeans = KMeans(n_clusters=3)kmeans.fit(X)# 라벨을 확인합니다print(kmeans.labels_)k-means는 지정해준 cluster 값만큼 데이터를 분류해줍니다.또한 cluster 개수만큼 label은 0부터 값을 순서대로 가집니다.하지만 cluster 값을 잘 지정해준다고 하더라도 분류가 실패하는 경우가 생깁니다.‘모든 클러스터의 반경이 똑같다’, ‘클러스터에서 모든 방향이 똑같이 중요하다’ 라는 두 가지 가정을 가지고 있기 때문에 중심에서 멀리 떨어진 경우에 데이터를 잘 처리하지 못합니다.즉, 서로 원형으로 잘 모여있는 데이터에 대해서는 잘 구분하지만 모양이 복잡할수록 더 성능이 나빠집니다.K-means는 클러스터 중심, 하나의 성분으로 표현된다고 볼 수 있습니다.하나의 성분으로 분해되는 관점으로 보는 것을 벡터 양자화라고 합니다.벡터 양자화는 입력 데이터의 차원보다 더 많은 클러스터를 사용해 데이터를 인코딩할 수 있습니다.즉, 2차원 데이터에서도 10개의 클러스터를 사용해 10개의 특성을 가지는 모델을 만들 수 있습니다.장점  이해하기 쉽다.구현하기 쉽다.비교적 빠르다.유연하고 효율적이다.단점  난수 초깃값에 따라 달라진다.활용 범위가 제한적이다.클러스터의 개수를 지정해야한다.최적의 군집을 찾기 어렵다.군집 개수 파악에 대한 합리적 추측이 필요하다.이상치나 노이즈에 민감하다.병합 군집병합 군집은 알고리즘은 시작할 때 각 포인트를 하나의 클러스터로 지정하고 어떤 종료 조건을 만족할 때 까지 가장 비슷한 두 클러스터를 합쳐나갑니다.종료 조건은 클러스터 개수로 하여 지정된 개수의 클러스터가 남을 때까지 비슷한 클러스터를 합칩니다.wardward 연결은 모든 클러스터 내의 분산을 가장 작게 증가시키는 두 클러스터를 합칩니다.따라서 크기가 비슷한 클러스터가 만들어집니다.averageaverage 연결은 클러스터 포인트 사이의 평균 거리가 가장 짧은 두 클러스터를 합칩니다.completecomplete 연결은 클러스터 포인트 사이의 최대 거리가 가장 짧은 두 클러스터를 합칩니다.singlesigle 연결은 클러스터 포인트 사이의 최소 거리가 가장 짧은 두 클러스터를 합칩니다.클러스터는 다음과 같은 모습으로 합쳐집니다.클러스터의 사용 방법은 다음과 같습니다.from sklearn.cluster import AgglomerativeClusteringX, y = make_blobs(random_state=1)agg = AgglomerativeClustering(n_clusters=3)assignment = agg.fit_predict(X)mglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)plt.legend([\"클러스터 0\", \"클러스터 1\", \"클러스터 2\"], loc=\"best\")plt.xlabel(\"특성 0\")plt.ylabel(\"특성 1\")병합 군집은 계층적 군집을 만듭니다.계층적 군집은 Scipy에서 덴드로그램을 통해 다차원 데이터셋을 처리하여 시각화 할 수 있습니다.from scipy.cluster.hierarchy import dendrogram, wardX, y = make_blobs(random_state=0, n_samples=12)# 데이터 배열 X 에 ward 함수를 적용합니다# SciPy의 ward 함수는 병합 군집을 수행할 때 생성된# 거리 정보가 담긴 배열을 리턴합니다linkage_array = ward(X)# 클러스터 간의 거리 정보가 담긴 linkage_array를 사용해 덴드로그램을 그립니다dendrogram(linkage_array)# 두 개와 세 개의 클러스터를 구분하는 커트라인을 표시합니다ax = plt.gca()bounds = ax.get_xbound()ax.plot(bounds, [7.25, 7.25], '--', c='k')ax.plot(bounds, [4, 4], '--', c='k')ax.text(bounds[1], 7.25, ' 두 개 클러스터', va='center', fontdict={'size': 15})ax.text(bounds[1], 4, ' 세 개 클러스터', va='center', fontdict={'size': 15})plt.xlabel(\"샘플 번호\")plt.ylabel(\"클러스터 거리\")DBSCANDBSCAN은 클러스터의 개수를 미리 지정할 필요가 없습니다.복잡한 형상도 찾을 수 있으며 어떤 클래스에 속하지 않는 포인트도 구분할 수 있습니다.병합 군집이나 k-means보다 다소 느리지만 큰 데이터 셋에도 적용할 수 있습니다.DBSCAN은 특성 공간에서 가까이 있는 데이터가 많아 붐비는 지역을 포인트로 찾습니다.밀집 지역이 한 클러스터를 구성하며 비어있는 지역을 경계로 다른 클러스터와 구분된다는 것입니다.밀도: 자기를 중심으로 반지름 안에 있는 다른 좌표점의 개수최소 거리: 이웃을 정의하기 위한 거리, 밀도 측정 반지름최소 데이터 개수: 밀집 지역을 정의하기 위해 필요한 이웃의 개수, 반지름 내에 있는 최소 데이터의 개수  무작위 포인트를 선택합니다.  포인트에서 eps 거리 안의 모든 포인트를 찾습니다.  거리 안의 포인트 수가 min_samples보다 적다면 노이즈로 레이블 합니다.  거리 안의 min_samples보다 포인트 수가 많다면 그 포인트는 핵심 샘플로 레이블하고 새로운 클러스터 레이블을 할당합니다.  포인트의 eps 거리 안의 모든 이웃을 확인하여 어떤 클러스터에도 할당되지 않았다면 방금 만든 클러스터 레이블을 할당합니다.  이 과정을 반복하여 클러스터는 eps 거리 안에 더 이상 핵심 샘플이 없을 때까지 커집니다.  아직 선택되지 못한 포인트를 기준으로 위 과정을 다시 반복합니다.DBSCAN은 다음과 같이 사용합니다.from sklearn.cluster import DBSCANX, y = make_blobs(random_state=0, n_samples=12)dbscan = DBSCAN()clusters = dbscan.fit_predict(X)print(\"클러스터 레이블:\\n\", clusters)다음 그림에서 보는 것과 같이 min_samples와 eps을 통해 모양이 많이 달라집니다.eps를 증가시키면 하나의 클러스터에 더 많은 포인트가 포함되고, min_samples를 키우면 노이즈가 증가합니다.장점  K-means와 다르게 군집의 수를 설정할 필요가 없습니다.다양한 모양의 군집이 형성될 수 있으며, 군집끼리 겹치는 경우가 없습니다.노이즈 개념 덕분에 이상치에 대응할 수 있습니다.eps, min_samples를 잘 설정하면 좋은 성능을 낼 수 있습니다.단점  한 데이터는 하나의 군집에 속하게 되므로 시작점에 따라 다른 모양의 군집이 형성됩니다.eps 값에 따라 성능이 크게 좌우됩니다.군집별로 밀도가 다른 경우 군집화가 제대로 이루어지지 않습니다.군집 알고리즘의 비교와 평가타깃 값으로 군집 평가하기1(최적일 때)과 0(무작위로 분류) 사이의 값을 제공하는 ARI, NMI가 가장 널리 사용하는 지표입니다.adjusted_rand_score과 normalized_mutual_info_score과 같은 군집용 측정 도구가 따로 존재하므로 accuracy_score을 사용하지 않아야 합니다.하지만 ARI와 NMI와 같이 정확한 클러스터를 알고 있어야 평가가 가능하다면 지도 학습 모델을 만드는 곳에만 사용되고 실제 애플리케이션 성능 평가는 사용할 수 없습니다.타깃 값 없이 군집 평가하기따라서 타깃 값 없이 실루엣 계수라는 것을 이용하여 군집용 지표가 존재합니다.실루엣 점수는 클러스터의 밀집 정도를 계산하는 것으로 높을수록 좋으며 최대 점수는 1입니다.하지만 K-평균에서 나왔던 문제와 비슷하게 모양이 복잡할 때는 밀집도를 활용한 평가는 좋지 않습니다.",
        "url": "/study-ML4"
    }
    ,
    
    "programming-baekjoon7": {
        "title": "백준 (7) &lt;br&gt; (10828,10773,1874,10799, &lt;br&gt; 4949,1406,2493)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)    백준 (9) 우선순위 큐 알고리즘 (1927,11279,11286,1715,11766)백준 7문제를 풀어보았다.중복되는 문제도 있습니다스택에 관련된 7문제를 풀어보았다.https://www.acmicpc.net/problemset?sort=ac_desc&amp;algo=7110828번 스택https://www.acmicpc.net/problem/10828input()을 이용하면 시간 초과가 발생한다. (sys.stdin.readline() 사용)입력받은 값에 각 명령어들이 있으면 명령어에 따라 작동하도록 해주면 된다.push는 뒤에 숫자가 따라오기 때문에 split() 함수를 통해 뒤에 숫자를 분리해줘서 스택에 넣어준다.pop은 스택의 길이가 0 보다 클 때와 작을 때를 구분하여 pop 함수를 실행하거나 -1을 출력해주면 된다. size는 스택의 길이와 같다.empty는 스택의 길이가 0 인가 아닌가를 판별해주면 된다.top은 pop과 유사하지만 pop 함수를 사용하면 가장 위에 숫자가 스택에서 사라지기 때문에 stack[-1]을 출력해주면 된다.# 처음 문제만 읽고 풀었을 때 작성한 코드import sysstack=[]for i in range(int(input())):    command=input()    if 'push' in command:        a,b=command.split()        b=int(b)        stack.append(b)    if 'top' in command:        if len(stack)&gt;0:            print(stack[-1])        else:            print('-1')    if 'size' in command:        print(len(stack))    if 'empty' in command:        if len(stack)==0:            print('1')        else: print('0')    if 'pop' in command:        if len(stack)&gt;0:            print(stack.pop())        else:            print('-1')14push 1push 2top2size2empty0pop2pop1pop-1size0empty1pop-1push 3empty0top3# 클래스와 함수를 이용하는 것이 훨씬 보기 좋을 것 같아서 새로 작성한 코드# 클래스와 함수에 익숙하지 않아서 생각보다 오래 걸렸다.# 마찬가지로 input 대신 sys.stdin.readline()를 사용하여 제출해야한다.class stack:    def __init__(self):         self.stack_list = []    def push(self, num):        self.stack_list.append(num)    def pop(self):        if len(self.stack_list) == 0:            print(\"-1\")        else:            print(self.stack_list.pop())    def size(self):        print(len(self.stack_list))    def empty(self):        if len(self.stack_list) == 0:            print(\"1\")        else :            print(\"0\")    def top(self):        if len(self.stack_list) == 0:            print(\"-1\")        else :            print(self.stack_list[-1])import sysnumber = int(input())stack=stack()for _ in range(number):    command = input().split()    if command[0] == \"push\":        stack.push(command[1])    if 'pop' in command:        stack.pop()    if 'size' in command:        stack.size()    if 'empty' in command:        stack.empty()    if 'top' in command:        stack.top()14push 1push 2top2size2empty0pop2pop1pop-1size0empty1pop-1push 3empty0top310773번 제로https://www.acmicpc.net/problem/10773스택에서 pop과 append를 이용하여 해결할 수 있는 간단한 문제였다.재현이가 0 을 외치면 pop을 이용해서 최근의 숫자를 지워주면 된다.반대로 0이 아닌 숫자를 외치면 스택에 append 해주면 된다.출력하는 값은 stack에 남아있는 숫자이기 때문에 sum을 이용해서 출력해준다.import sysstack=[]for i in range(int(input())):    money=int(input())    if money==0:        stack.pop()    else:        stack.append(money)print(sum(stack))4304001874번 스택 수열https://www.acmicpc.net/problem/1874스택과 푸쉬, 팝 이해하기push를 세 번 하면 [1,2,3] 스택이 쌓이게 되고 여기서 pop을 하면 3이 출력된다.n을 통해 입력할 숫자의 갯수를 입력받고 num을 통해 숫자를 입력받는다.count는 입력받을 숫자가 stack에 입력되도록 해준다. 0으로 두면 0부터 시작이다.따라서 1로 한다. result를 통해 +와 -를 입력받고 stack에는 count에 생긴 숫자들을 쌓아둔다.while문을 통해 stack을 완성하고 if문을 통해 해당 숫자가 나오면 -를 입력한 뒤 pop해서 숫자를 제거한다.n=int(input())count=1result=[]stack=[]temp=Truefor i in range(n):    num=int(input())    while count&lt;=num:        stack.append(count)        result.append('+')        count=count+1    if stack[-1]==num:        stack.pop()        result.append('-')    else:        temp=False        if temp==False:    print('NO')else:    for j in result:        print(j)843687521++++--++-++-----10799번 쇠막대기https://www.acmicpc.net/problem/10799()가 쌍을 이룰 때는 레이저이므로 L로 대체했다.stack=[]을 이용하여 막대가 몇 개 있는지 저장하는 용도로 사용하였다.(가 되면 막대가 계속 쌓이는 데, 레이저를 만나면 레이저 왼쪽으로 막대의 개수만큼 잘린다.그 다음 )를 만나면 막대가 끝난 경우이므로 또 잘린 막대가 생긴다.따라서 if 문으로 (가 나오면 막대 개수를 추가해주고L은 막대의 개수만큼 답을 더해주고 )는 잘린 막대 1개를 추가해준다.word=input()()(((()())(())()))(())word=word.replace('()','L')stack=[]answer=0for i in word:    if i=='(':        stack.append(0)    elif i==')':        stack.pop()        answer+=1    else:        answer+=len(stack)print(answer)        L 0( 0( 0( 0L 3L 6) 7( 7L 10) 11L 13) 14) 15( 15L 16) 17174949번 균형잡힌 세상https://www.acmicpc.net/problem/4949하나는 replace와 re.sub을 이용하여 괄호들을 제외한 모든 문자를 지워주는 문제로 풀었다.하지만 주제가 스택이었으므로 스택으로 다시 한 번 풀었다.# replace와 sub을 이용한 풀이import reword='a'while word != '.':    word=input()    if word=='.':        break    word=word.replace('.','')    word=re.sub('[a-zA-Z]','',word)    word=re.sub(' ','',word)    while ('[]' in word) or ('()' in word):        word=word.replace('[]','')        word=word.replace('()','')    if word =='':        print('yes')    else:        print('no')A rope may form )( a trail in a maze.noHelp( I[m being held prisoner in a fortune cookie factory)].noSo when I die (the [first] I will see in (heaven) is a score list).yes[ first in ] ( first out ).yes([ (([( [ ] ) ( ) (( ))] )) ]).yes .yes.# stack을 이용한 풀이while True: # 계속 입력받기 위한 while    word=input()    stack=[] # 괄호 저장을 위한 stack    temp=True # 나중에 stack에 있는지 없는지 판단을 위한 temp    if word=='.': # .만 입력하면 while이 끝난다.          break            for i in word:        if i =='[' or i=='(': # 괄호 여는 것은 모두 스택에 append한다.            stack.append(i)        elif i==']': # 닫힌 괄호가 나왔을 때,            if len(stack) ==0 or stack[-1]=='(': # 스택에 아무것도 없거나 소괄호만 있다면 no를 출력한다.                                print('no')                temp=False #temp를 false로 두어서 나중에 stack에 짝이 맞아서 아무것도 없을 때를 대비한다.                break            else:                stack.pop() # 괄호의 짝이 맞게 있다면 '['를 없애서 스택에서 비워준다.        elif i==')':             if len(stack) ==0 or stack[-1]=='[':                print('no')                temp=False                break            else:                stack.pop()    if temp==True: #괄호의 짝이 모두 맞았다면 stack은 0이 될 것이다.        if len(stack)==0:            print('yes')        else:            print('no')So when I die (the [first] I will see in (heaven) is a score list).yes[ first in ] ( first out ).yesHalf Moon tonight (At least it is better than no Moon at all].noA rope may form )( a trail in a maze.noHelp( I[m being held prisoner in a fortune cookie factory)].no([ (([( [ ] ) ( ) (( ))] )) ]).yes .yes.1406번 에디터https://www.acmicpc.net/problem/1406처음에 짠 코드는 답은 맞았지만 시간초과가 발생한다.찾아보니까 insert와 del은 시간 복잡도가 O(n)이라고 한다.따라서 O(1)인 pop과 append를 사용해서 문제를 해결해야 한다.pop과 append를 이용하여 빈 스택에 ‘L’,’D’로 변화된 커서에 대한 정보를 저장한다.B와 P는 처음 푼 것과 다르게 커서 변경없이 pop과 append로 값만 추가, 삭제해주면 된다.word_list에는 새로 입력받은 값이 저장될 것고 새로 만든 스택에 커서 변경된 정보가 순서대로 저장될 것이다. 실제 입력받은 문자에서의 변화와 반대로 저장되기 때문에 [::-1]로 새로운 스택은 합쳐줘야한다.당연히 sys.stin.readline()으로 입력받아야한다.# 시간 초과된 insert와 defword_list=list(input())point=int(input())j=len(word_list)for i in range(point):    command=input()    if command[0]=='L' and j!=0:        j=j-1             elif command[0]=='D' and j!=len(word_list):        j=j+1        elif command[0]=='B' and j != 0:        del word_list[j-1]        j=j-1            elif command[0]=='P':        if j==len(word_list):            word_list.insert(len(word_list)+1,command[2])            j=j+1            print(j)        elif j==0:            word_list.insert(j,command[2])            print(j)        else:            word_list.insert(j,command[2])            print(''.join(word_list))dmih11BBP x3LBBBP y0DDP z3yxzword_list=list(input())point=int(input())j=len(word_list)stack=[]for i in range(point):    command=input()    if command[0]=='L' and len(word_list) !=0:        stack.append(word_list.pop())    elif command[0]=='D' and len(stack) !=0:        word_list.append(stack.pop())    elif command[0]=='B'and len(word_list) !=0:        word_list.pop()    elif command[0]=='P':        word_list.append(command[2])print(word_list)print(stack)word_list.extend(stack[::-1])    print(''.join(word_list))abc9LLLLLP xLBP y['y']['c', 'b', 'a', 'x']yxabc2493번 탑https://www.acmicpc.net/problem/2493answer은 정답을 기록하는 용도로, stack은 비교하는 탑과 인덱스를 저장한다.즉, for문으로 레이저가 출발하는 송전탑을 고르고, stack에는 이 전에 지나온 송전탑들이 저장된다.i값은 인덱스보다 1 작기 때문에 답에는 1을 추가해줘야한다.n = int(input()) #5top = list(map(int, input().split())) #[6 9 5 7]stack = [] #[]answer = [0 for i in range(n)] #[0 0 0 0 4] for i in range(n):    while stack:        if stack[-1][1] &gt; top[i]:            answer[i] = stack[-1][0] + 1            break        else:            stack.pop()    stack.append([i, top[i]]) print(*answer)56 9 5 7 40 0 2 2 4a=[1,2,3]print(*a)1 2 3",
        "url": "/programming-baekjoon7"
    }
    ,
    
    "study-ml3": {
        "title": "머신러닝 정리 (3) &lt;br&gt; 비지도학습 (1)",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학    머신러닝 정리 (6)-모델 평가와 성능 향상머신러닝 정리 (3) - 비지도학습 (1)본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python비지도학습 (1)  비지도 학습의 종류  비지도 학습의 도전과제  데이터 전처리와 스케일 조정  여러 가지 전처리 방법          StandardScaler      RobustScaler      MinMaxScaler      Nomarlizer        데이터 변환 적용하기  Quantile Transformer 와 Power Transformer          Quantile Transformer      Power Transformer        훈련 데이터와 테스트 데이터의 스케일을 같은 방법으로 조정하기  지도 학습에서 데이터 전처리 효과  차원 축소, 특성 추출, 매니폴드 학습          주성분 분석(PCA)                  유방암 데이터 셋 시각화          고유얼굴 특성 추출                    비지도 학습의 종류책에서는 두 가지 비지도 학습을 공부합니다.비지도 변환과 군집입니다.비지도 변환은 데이터를 새롭게 표현하여 사람이나 다른 머신러닝 알고리즘이 원래 데이터보다 쉽게 해석할 수 있도록 만드는 알고리즘입니다.특히 고차원 데이터를 특성의 수를 줄이면서 꼭 필요한 특징을 포함한 데이터로 표현하는 방법인 차원축소가 대표적인 예입니다.비지도 변환으로 데이터를 구성하는 단위나 성분을 찾기도 합니다.텍스트 문서에서 주제를 추출하는 것이 예입니다.소셜 미디어에서 선거, 총기 규제, 팝스타 같은 주제로 일어나는 토론을 추적할 때 사용한다고 합니다.군집 알고리즘은 데이터를 서로 비슷하게 그룹으로 묶는 것 입니다.같은 사람이 찍힌 사진을 같은 그룹으로 묶게 추천해주는 것을 생각하면 이해하기 쉽습니다.비지도 학습의 도전과제비지도 학습에서 가장 어려운 일은 알고리즘이 유용한 가를 평가하는 것 입니다.비지도 학습은 보통 레이블이 없어서 어떤 것이 올바른 것인지 모릅니다.그래서 비지도 학습의 결과를 평가하기 위해서는 직접 확인하는 것이 유일한 방법일 때도 있다고 합니다.비지도 학습 알고리즘은 데이터를 잘 이해하고 싶을 때 탐색적 분석 단계에서도 많이 사용합니다.전처리 단계에서도 사용되는데 비지도 학습 결과를 사용하여 학습하면 지도 학습의 정확도가 좋아지기도 하고 메모리나 시간 절약도 할 수 있습니다.데이터 전처리와 스케일 조정신경망이나 SVM과 같은 스케일에 민감한 알고리즘은 데이터 특성 값을 조정해야합니다.여러 가지 전처리 방법StandardSclaer각 특성의 평균을 0, 분산을 1로 변경하여 모든 특성이 같은 크기를 가지게 합니다.특성의 최솟값과 최댓값의 크기를 제한하지는 않습니다.\\[z= \\frac {x−μ} σ ( z점수= \\frac{자료값-평균} {표준편차})\\]RobustScaler특성이 같은 스케일을 갖게되지만 평균과 분산 대신 중간 값과 사분위 값을 사용합니다.따라서 이상치에 영향을 받지 않습니다.MinMaxScaler모든 특성이 정확하게 0과 1 사이에 위치하도록 변경합니다.2차원 데이터셋일 경우에는 모든 데이터가 x 축의 0과 1, y축의 0과 1 사이의 사각 영역에 담기게 됩니다.(q는 각 사분위값을 뜻합니다.)\\[\\frac {x-q~2} {q~3-q~1}\\]Nomarlizer특성 벡터의 유클리디안 길이가 1이 되도록 데이터 포인트를 조정합니다.다른 말로 하면 지름이 1인 원(3차원에서는 구)에 들어옵니다.각 데이터 포인트가 다른 비율로 조정됩니다.  특성 벡터의 길이는 상관 없고 데이터의 방향이 중요할 때 많이 사용합니다.데이터 변환 적용하기from sklearn.preprocessing import MinMaxScalerscaler = MinMaxScaler()scaler.fit(X_train)X_train_scaled = scaler.transform(X_train)print(\"변환된 후 크기:\", X_train_scaled.shape)print(\"스케일 조정 전 특성별 최소값:\\n\", X_train.min(axis=0))print(\"스케일 조정 전 특성별 최대값:\\n\", X_train.max(axis=0))print(\"스케일 조정 후 특성별 최소값:\\n\", X_train_scaled.min(axis=0))print(\"스케일 조정 후 특성별 최대값:\\n\", X_train_scaled.max(axis=0))변환된 후 크기: (426, 30)스케일 조정 전 특성별 최소값:[  6.981   9.71   43.79  143.5     0.053   0.019   0.      0.      0.1060.05    0.115   0.36    0.757   6.802   0.002   0.002   0.      0.0.01    0.001   7.93   12.02   50.41  185.2     0.071   0.027   0.0.      0.157   0.055]스케일 조정 전 특성별 최대값:[  28.11    39.28   188.5   2501.       0.163    0.287    0.427    0.201    0.304    0.096    2.873    4.885   21.98   542.2      0.031    0.135    0.396    0.053    0.061    0.03    36.04    49.54   251.2   4254.    0.223    0.938    1.17     0.291    0.577    0.149]스케일 조정 후 특성별 최소값:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.0. 0. 0. 0. 0. 0.]스케일 조정 후 특성별 최대값:[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.1. 1. 1. 1. 1. 1.]변환 된 값들이 모두 0과 1 사이가 된 것을 알 수 있습니다.X_test_scaled = scaler.transform(X_test)print(\"스케일 조정 후 특성별 최소값:\\n\", X_test_scaled.min(axis=0))print(\"스케일 조정 후 특성별 최대값:\\n\", X_test_scaled.max(axis=0))스케일 조정 후 특성별 최소값:[ 0.034  0.023  0.031  0.011  0.141  0.044  0.     0.     0.154 -0.006-0.001  0.006  0.004  0.001  0.039  0.011  0.     0.    -0.032  0.0070.027  0.058  0.02   0.009  0.109  0.026  0.     0.    -0.    -0.002]스케일 조정 후 특성별 최대값:[0.958 0.815 0.956 0.894 0.811 1.22  0.88  0.933 0.932 1.037 0.427 0.4980.441 0.284 0.487 0.739 0.767 0.629 1.337 0.391 0.896 0.793 0.849 0.7450.915 1.132 1.07  0.924 1.205 1.631]테스트 세트의 최솟값과 최댓값은 0과 1이 아닐 수 있다.테스트 세트의 최솟값과 범위를 사용하지 않고 훈련 세트의 최솟값을 빼고 훈련 세트의 범위로 나누기 때문이다.MinMaxScaler을 사용하려면 항상 테스트와 훈련 세트 모두 같은 변환을 해야한다.Quantile Transformer 와 Power TransformerQuantile TransformerQuantile Transformer은 1000개의 분위를 사용하여 데이터를 균등하게 분포시킵니다.RobustScaler과 비슷하게 이상치에 민감하지 않으며 전체 데이터를 0과 1 사이로 압축합니다.분위 수는 n_quantiles 매개 변수로 설정할 수 있으며 속성의 크기는 (n_quantiles,n_features) 입니다.output_distribution 매개변수를 통해 균등 분포에서 정규 분포로 출력을 바꿀 수도 있습니다.Power TransformerPower Transformer는 method 매개변수에 ‘yeo-johnson’과 ‘box-cox’ 알고리즘을 지정할 수 있습니다.어떤 변환이 정규 분포에 가깝게 변환할지 사전에 알기 힘들기 때문에 히스토그램으로 확인해보는 것이 좋습니다.훈련 데이터와 테스트 데이터의 스케일을 같은 방법으로 조정하기지도 학습에서는 훈련 세트와 테스트 세트에 같은 변환을 적용하는 것이 중요합니다.잘 조정된 데이터는 같은 비율로 데이터를 바꿔 원본 데이터와 비율만 다른 그래프를 보여주지만 잘못 조정된 데이터는 배열이 엉망이 되어서 결과에 영향을 미칩니다.지도 학습에서 데이터 전처리 효과from sklearn.svm import SVCX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)svm = SVC(gamma='auto')svm.fit(X_train, y_train)print(\"테스트 세트 정확도: {:.2f}\".format(svm.score(X_test, y_test)))테스트 세트 정확도: 0.63# 0~1 사이로 스케일 조정scaler = MinMaxScaler()scaler.fit(X_train)X_train_scaled = scaler.transform(X_train)X_test_scaled = scaler.transform(X_test)svm.fit(X_train_scaled, y_train)print(\"스케일 조정된 테스트 세트의 정확도: {:.2f}\".format(svm.score(X_test_scaled, y_test)))스케일 조정된 테스트 세트의 정확도: 0.95# 평균 0, 분산 1을 갖도록 스케일 조정from sklearn.preprocessing import StandardScalerscaler = StandardScaler()scaler.fit(X_train)X_train_scaled = scaler.transform(X_train)X_test_scaled = scaler.transform(X_test)svm.fit(X_train_scaled, y_train)print(\"SVM 테스트 정확도: {:.2f}\".format(svm.score(X_test_scaled, y_test)))SVM 테스트 정확도: 0.97차원 축소, 특성 추출, 매니폴드 학습주성분 분석은 가장 많이 사용되는 데이터 변환 방법입니다.특성 추출에서는 비음수 행렬 분해가 많이 사용됩니다.2차원 산점도를 이용한 시각화 용도로 사용되는 t-SNE 알고리즘도 있습니다.차원 축소  차원 축소의 필요성관측 치의 수는 한정되어 있습니다.차원이 커질 수록 한정된 자료는 커진 차원의 패턴을 잘 설명하지 못하고 복잡도가 기하급수적으로 늘어나게 됩니다.상관계수가 높은 변수 중 일부만 분석하게 된다면 정보의 손실이 발생하게 됩니다.  차원 축소의 여러 방법    &gt; principal component    &gt; 변수 선택법    &gt; penalty 기반 regression    &gt; convolutional neural network    &gt; drop out &amp; bagging  차원 축소의 활용    &gt; 차원 축소를 통해 데이터를 잘 설명할 수 있는 잠재적 요소 추출    &gt; 이미지 분류, 시맨틱, 토픽 분류 등주성분 분석(PCA)공분산 행렬 개념은 다음과 같습니다.Principal Components의 개념은 차원을 줄이면서 정보 손실을 최소화하는 방법입니다.더 적은 개수로 데이터를 충분히 잘 설명할 수 있는 새로운 축을 찾아냅니다.공분산이 데이터의 형태를 변형시키는 방향의 축과 그것에 직교하는 축을 찾습니다.2차원의 경우 공분산이 나타내는 타원의 장축과 단축입니다.PC score은 새로 찾아낸 축에서의 좌표값을 의미하는데 기존 값을 새로운 축에 내린 정사영입니다.즉, PCA는 데이터의 분산을 최대한 보존하면서 서로 직교하는 새 기저(축)을 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법입니다.데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있고, 이렇게 분해된 고유벡터를 이용해 입력데이터를 선형변환하는 것이 PCA입니다.PCA의 수학적개념_Singular Value Decomposition(SVD)SVD와 eigen value, eigen vector의 연관성주성분 분석은 특성들이 통계적으로 상관관계가 없도록 데이터셋을 회전시키는 기술입니다.회전한 뒤에 데이터를 설명하는 데 중요한 새로운 특성 중 일부만 선택합니다.주성분 찾기  분산이 가장 큰 가장 많은 정보를 가지고 있는 방향을 찾는다.  첫 번째 방향과 직각인 방향 중에서 가장 많은 정보를 가지고 있는 방향을 찾는다.두 번째 그래프는 주성분 1과 2를 x 축과 y 축에 나란히 회전한 것으로 변환된 데이터의 상관관계 행렬이 대각선 방향을 제외하고 0이 됩니다.세 번째 그래프는 차원 축소 용도로 사용될 수 있습니다.  첫 번째 주성분만 유지하므로 2차원 데이터 셋이 1차원 데이터 셋으로 차원 감소 합니다.단순히 원본 특성 중 하나만 남기는 것이 아닌 가장 유용한 방향을 찾아 그 성분을 유지하는 것입니다.마지막 그래프는 데이터에 다시 평균을 더하고 반대로 회전시킨 그래프 입니다.원래 특성 공간에 있지만 첫 번째 주성분의 정보만 가지고 있습니다.보통 노이즈 제거나 주성분에서 유지되는 정보의 시각화를 위해 사용됩니다.유방암 데이터 셋 시각화유방암 데이터는 특성이 30개를 가지고 있기 때문에 너무 많은 산점도를 요구합니다.따라서 히스토그램을 그리는 방법이 있지만 히스토그램은 특성 간의 상호작용이나 클래스와의 곤계를 알려주지 못합니다.따라서 PCA를 통해 데이터를 회전시키고 차원을 축소합니다.그러면 다음과 같이 2차원 공간에서도 잘 구분됩니다.분류를 할 때 좋은 결과를 얻을 수 있을 것 같은 그래프입니다.하지만 두 축을 해석하기 어렵다는 단점을 가지고 있습니다.따라서 pca.components_를 통해서 중요도를 확인할 수 있습니다.고유얼굴 특성 추출PCA는 특성 추출에도 이용됩니다.RGB 강도로 기록된 픽셀들을 분류하는데 유용합니다.픽셀을 사용해서 두 이미지를 비교할 때, 각 픽셀의 회색톤 값을 다른 이미지에서 동일한 위치에 있는 픽셀 값과 비교합니다.하지만 이는 사람이 얼굴을 인식하는 것과 많이 다르고 특징을 잡기 어렵습니다.따라서 주성분으로 변환하여 거리를 계산하면 정확도가 높아집니다.PCA의 화이트닝 옵션을 사용하면 주성분의 스케일이 같습니다.화이트닝 옵션은 StandardScaler과 동일한 방식입니다.PCA 모델은 픽셀을 기반으로 하므로 얼굴의 배치와 조명이 비슷한 이미지를 판단하는 데 큰 영향을 줍니다.따라서 테스트 포인트를 주성분의 가중치 합으로 나타내는 것에 PCA 변환을 사용하는 방법도 해석 중 한 가지 방법입니다.원본 데이터를 재구성하는 방법도 PCA 모델의 해석 방법 중 한가지입니다.",
        "url": "/study-ML3"
    }
    ,
    
    "programming-kaggle2": {
        "title": "캐글 (2) &lt;br&gt; 타이타닉 튜토리얼 1,2 공부하기",
            "author": "keonju",
            "category": "",
            "content": "kaggle 관련 글    캐글 (1) Simple Matplotlib &amp; Visualization Tips 공부하기    캐글 (2) 타이타닉 튜토리얼 1,2 공부하기    캐글 (3) 타이타닉 생존자 EDA 부터 분류까지    캐글 (4) 타이타닉 생존자 앙상블과 스태킹까지    캐글 (5) 메타 데이터를 이용한 데이터 관찰 및 준비타이타닉 튜토리얼 1,2 공부하기kaggle 타이타닉 튜토리얼을 필사하였다.해당 유튜브를 따라서 필사하였고 블로그에 자세한 설명도 나와있었다.https://www.youtube.com/watch?v=_iqz7tFhox0&amp;list=PLC_wC_PMBL5MnqmgTLqDgu4tO8mrQakuFhttps://kaggle-kr.tistory.com/17?category=868316https://kaggle-kr.tistory.com/18?category=868316# 분석에 필요한 패키지import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns# plt의 스타일 지정plt.style.use('seaborn')sns.set(font_scale=2.5) # 결측치를 알기 쉽게 하는 패키지import missingno as msno# warning무시import warningswarnings.filterwarnings('ignore')# notebook에서 바로 그림 확인하는 코드%matplotlib inline# 데이터 불러오기df_train=pd.read_csv('train.csv')df_test=pd.read_csv('test.csv')df_test.head()                  PassengerId      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked                  0      892      3      Kelly, Mr. James      male      34.5      0      0      330911      7.8292      NaN      Q              1      893      3      Wilkes, Mrs. James (Ellen Needs)      female      47.0      1      0      363272      7.0000      NaN      S              2      894      2      Myles, Mr. Thomas Francis      male      62.0      0      0      240276      9.6875      NaN      Q              3      895      3      Wirz, Mr. Albert      male      27.0      0      0      315154      8.6625      NaN      S              4      896      3      Hirvonen, Mrs. Alexander (Helga E Lindqvist)      female      22.0      1      1      3101298      12.2875      NaN      S      # 데이터 확인df_train.head()                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      7.2500      NaN      S              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      71.2833      C85      C              2      3      1      3      Heikkinen, Miss. Laina      female      26.0      0      0      STON/O2. 3101282      7.9250      NaN      S              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      35.0      1      0      113803      53.1000      C123      S              4      5      0      3      Allen, Mr. William Henry      male      35.0      0      0      373450      8.0500      NaN      S      # 데이터 행렬 확인df_train.shape(891, 12)# 데이터 셋 특징 확인df_train.describe()                  PassengerId      Survived      Pclass      Age      SibSp      Parch      Fare                  count      891.000000      891.000000      891.000000      714.000000      891.000000      891.000000      891.000000              mean      446.000000      0.383838      2.308642      29.699118      0.523008      0.381594      32.204208              std      257.353842      0.486592      0.836071      14.526497      1.102743      0.806057      49.693429              min      1.000000      0.000000      1.000000      0.420000      0.000000      0.000000      0.000000              25%      223.500000      0.000000      2.000000      20.125000      0.000000      0.000000      7.910400              50%      446.000000      0.000000      3.000000      28.000000      0.000000      0.000000      14.454200              75%      668.500000      1.000000      3.000000      38.000000      1.000000      0.000000      31.000000              max      891.000000      1.000000      3.000000      80.000000      8.000000      6.000000      512.329200      # max 값만 확인df_train.max()PassengerId                            891Survived                                 1Pclass                                   3Name           van Melkebeke, Mr. PhilemonSex                                   maleAge                                     80SibSp                                    8Parch                                    6Ticket                           WE/P 5735Fare                               512.329dtype: objectdf_test.describe()                  PassengerId      Pclass      Age      SibSp      Parch      Fare                  count      418.000000      418.000000      332.000000      418.000000      418.000000      417.000000              mean      1100.500000      2.265550      30.272590      0.447368      0.392344      35.627188              std      120.810458      0.841838      14.181209      0.896760      0.981429      55.907576              min      892.000000      1.000000      0.170000      0.000000      0.000000      0.000000              25%      996.250000      1.000000      21.000000      0.000000      0.000000      7.895800              50%      1100.500000      3.000000      27.000000      0.000000      0.000000      14.454200              75%      1204.750000      3.000000      39.000000      1.000000      0.000000      31.500000              max      1309.000000      3.000000      76.000000      8.000000      9.000000      512.329200      # column값 확인df_train.columnsIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],      dtype='object')# 각 column의 null 데이터 비율 확인 {:&gt;10}:오른쪽 정렬for col in df_train.columns:    msg='column: {:&gt;10}\\t Percent of NaN value: {:.2f}%'.format(col,100*(df_train[col].isnull().sum()/df_train[col].shape[0]))    print(msg)column: PassengerId\t Percent of NaN value: 0.00%column:   Survived\t Percent of NaN value: 0.00%column:     Pclass\t Percent of NaN value: 0.00%column:       Name\t Percent of NaN value: 0.00%column:        Sex\t Percent of NaN value: 0.00%column:        Age\t Percent of NaN value: 19.87%column:      SibSp\t Percent of NaN value: 0.00%column:      Parch\t Percent of NaN value: 0.00%column:     Ticket\t Percent of NaN value: 0.00%column:       Fare\t Percent of NaN value: 0.00%column:      Cabin\t Percent of NaN value: 77.10%column:   Embarked\t Percent of NaN value: 0.22%# 값 확인df_train[col]0      S1      C2      S3      S4      S      ..886    S887    S888    S889    C890    QName: Embarked, Length: 891, dtype: object# null 값 확인df_train[col].isnull()0      False1      False2      False3      False4      False       ...  886    False887    False888    False889    False890    FalseName: Embarked, Length: 891, dtype: bool# null 값의 합df_train[col].isnull().sum()2# shape를 통해 총 데이터 갯수 확인하기df_train[col].isnull().sum()/df_train[col].shape[0]0.002244668911335578for col in df_test.columns:    msg='column: {:&gt;10}\\t Percent of NaN value: {:.2f}%'.format(col,100*(df_test[col].isnull().sum()/df_test[col].shape[0]))    print(msg)column: PassengerId\t Percent of NaN value: 0.00%column:     Pclass\t Percent of NaN value: 0.00%column:       Name\t Percent of NaN value: 0.00%column:        Sex\t Percent of NaN value: 0.00%column:        Age\t Percent of NaN value: 20.57%column:      SibSp\t Percent of NaN value: 0.00%column:      Parch\t Percent of NaN value: 0.00%column:     Ticket\t Percent of NaN value: 0.00%column:       Fare\t Percent of NaN value: 0.24%column:      Cabin\t Percent of NaN value: 78.23%column:   Embarked\t Percent of NaN value: 0.00%# missingno를 통해 확인하기msno.matrix(df=df_train.iloc[:,:],figsize=(8,8),color=(0.8,0.5,0.2))&lt;AxesSubplot:&gt;# iloc으로 가져오고 싶은 위치 찾기df_train.iloc[:,-1]0      S1      C2      S3      S4      S      ..886    S887    S888    S889    C890    QName: Embarked, Length: 891, dtype: objectmsno.bar(df=df_train.iloc[:,:],figsize=(8,8),color=(0.8,0.5,0.2))&lt;AxesSubplot:&gt;# pie plot과 count-plot 그래프 그리기# 도화지를 준비하는 과정 (1,2): 행렬 f, ax=plt.subplots(1,2,figsize=(18,8)) #'Survived'에 있는 값 count하기, 떨어뜨리기, 글자 규칙, 그리는 위치, 그림자df_train['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)ax[0].set_title('Pie plot-Survived') # 제목ax[0].set_ylabel('')sns.countplot('Survived',data=df_train,ax=ax[1]) #countplot을 [1] 위치에 그리기ax[1].set_title('Count plot-Survived')plt.show()2.1 PClass# class 별 생존자 수 count는 객체가 몇명인가df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=True).count()                  Survived              Pclass                        1      216              2      184              3      491      # sum은 숫자 자체의 데이터의 갯수 [0,1]에서 1을 다 더한 값df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=True).sum()                  Survived              Pclass                        1      136              2      87              3      119      # crosstab을 통해 비교 (margin은 All 표현,style.background_gradient를 통해 색상 조절)pd.crosstab(df_train['Pclass'],df_train['Survived'],margins=True).style.background_gradient(cmap='summer_r')            Survived        0        1        All                Pclass                                                                    1                        80                        136                        216                                                2                        97                        87                        184                                                3                        372                        119                        491                                                All                        549                        342                        891                # 평균 알아보기 (as_index를 통해 그래프 그리기 설정, sort_values를 통한 오름차순, ascending=False는 내림차순)df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=True).mean().sort_values(by='Survived',ascending=False)                  Survived              Pclass                        1      0.629630              2      0.472826              3      0.242363      # 그래프 기리기df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=True).mean().sort_values(by='Survived',ascending=False).plot()&lt;AxesSubplot:xlabel='Pclass'&gt;# as_index=False일때는 Pclass도 같이 그린다df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived',ascending=False).plot()&lt;AxesSubplot:&gt;# 막대그래프 그리기df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=True).mean().sort_values(by='Survived',ascending=False).plot.bar()&lt;AxesSubplot:xlabel='Pclass'&gt;y_position=1.02f,ax=plt.subplots(1,2,figsize=(18,8))# Class별 탑승자 수df_train['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])ax[0].set_title('Number of passenger By Pclass',y=y_position)ax[0].set_ylabel('Count')# Class별 Survived와 Dead 구분 (hue를 통해 색깔 구분)sns.countplot('Pclass',hue='Survived',data=df_train,ax=ax[1])ax[1].set_title('Pclass:Survived vs Dead',y=y_position)plt.show()2.2 Sexf,ax=plt.subplots(1,2,figsize=(18,8))df_train[['Sex','Survived']].groupby(['Sex'],as_index=True).mean().plot.bar(ax=ax[0])ax[0].set_title('Survived vs Sex')sns.countplot('Sex',hue='Survived',data=df_train,ax=ax[1])ax[1].set_title('Sex:Survived vs Dead')plt.show()df_train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean()                  Sex      Survived                  0      female      0.742038              1      male      0.188908      pd.crosstab(df_train['Sex'],df_train['Survived'],margins=True).style.background_gradient(cmap='summer_r')            Survived        0        1        All                Sex                                                                    female                        81                        233                        314                                                male                        468                        109                        577                                                All                        549                        342                        891                2.2 Both Sex and Pclass# factorplot 그래프 그리기# 선은 error barsns.factorplot('Pclass','Survived',hue='Sex',data=df_train,size=6,aspect=1.5)&lt;seaborn.axisgrid.FacetGrid at 0x2248b7c1c10&gt;# 축과 보는 방향을 바꾼 것sns.factorplot(x='Sex',y='Survived',col='Pclass',data=df_train,saturation=.5,size=9,aspect=1)&lt;seaborn.axisgrid.FacetGrid at 0x2248b8ad310&gt;sns.factorplot(x='Sex',y='Survived',hue='Pclass',data=df_train,saturation=.5,size=9,aspect=1)&lt;seaborn.axisgrid.FacetGrid at 0x2248ba293a0&gt;Ageprint('제일 나이 많은 탑승객: {:.1f} years'.format(df_train['Age'].max()))print('제일 나이 어린 탑승객: {:.1f} years'.format(df_train['Age'].min()))print('탑승객 평균 나이: {:.1f} years'.format(df_train['Age'].mean()))제일 나이 많은 탑승객: 80.0 years제일 나이 어린 탑승객: 0.4 years탑승객 평균 나이: 29.7 years# kdeplot(커널 밀도 함수) 그리기 (히스토그램과 유사)fig,ax=plt.subplots(1,1,figsize=(9,5))sns.kdeplot(df_train[df_train['Survived']==1]['Age'],ax=ax)sns.kdeplot(df_train[df_train['Survived']==0]['Age'],ax=ax)plt.legend(['Survived'==1,'Survived'==0])plt.show()# 히스토그램df_train[df_train['Survived']==1]['Age'].hist()&lt;AxesSubplot:&gt;그래프 그리는 다양한 방법f=plt.figure(figsize=(10,10))a=np.arange(100)b=np.sin(a)plt.plot(b)[&lt;matplotlib.lines.Line2D at 0x2248d260070&gt;]f,ax=plt.subplots(1,1,figsize=(10,10))a=np.arange(100)b=np.sin(a)plt.plot(b)[&lt;matplotlib.lines.Line2D at 0x2248d2c4040&gt;]plt.figure(figsize=(10,10))a=np.arange(100)b=np.sin(a)plt.plot(b)[&lt;matplotlib.lines.Line2D at 0x2248bd55280&gt;]# 탑승객의 연령별 분포plt.figure(figsize=(8,6))df_train['Age'][df_train['Pclass']==1].plot(kind='kde')df_train['Age'][df_train['Pclass']==2].plot(kind='kde')df_train['Age'][df_train['Pclass']==3].plot(kind='kde')plt.xlabel('Age')plt.title('Age Distribution within classes')plt.legend(['1st Class','2nd Class','3rd Class'])&lt;matplotlib.legend.Legend at 0x2248d2f8a00&gt;# 히스토그램은 겹치면 보이지 않음plt.figure(figsize=(8,6))df_train['Age'][df_train['Pclass']==1].plot(kind='hist')df_train['Age'][df_train['Pclass']==2].plot(kind='hist')df_train['Age'][df_train['Pclass']==3].plot(kind='hist')plt.xlabel('Age')plt.title('Age Distribution within classes')plt.legend(['1st Class','2nd Class','3rd Class'])&lt;matplotlib.legend.Legend at 0x2248b74fd30&gt;fig,ax=plt.subplots(1,1,figsize=(9,5))sns.kdeplot(df_train[(df_train['Survived']==0)&amp;(df_train['Pclass']==1)]['Age'],ax=ax)sns.kdeplot(df_train[(df_train['Survived']==1)&amp;(df_train['Pclass']==1)]['Age'],ax=ax)plt.legend(['Survived==1','Survived==0'])plt.title('1st class')plt.show()# 히스토그램은 겹치면 보이지 않음plt.figure(figsize=(8,6))df_train['Age'][(df_train['Pclass']==1)&amp;(df_train['Survived']==0)].plot(kind='hist')df_train['Age'][(df_train['Pclass']==1)&amp;(df_train['Survived']==1)].plot(kind='hist')plt.xlabel('Age')plt.title('Age Distribution within classes')Text(0.5, 1.0, 'Age Distribution within classes')fig,ax=plt.subplots(1,1,figsize=(9,5))sns.kdeplot(df_train[(df_train['Survived']==0)&amp;(df_train['Pclass']==2)]['Age'],ax=ax)sns.kdeplot(df_train[(df_train['Survived']==1)&amp;(df_train['Pclass']==2)]['Age'],ax=ax)plt.legend(['Survived==1','Survived==0'])plt.title('2nd class')plt.show()# 히스토그램은 겹치면 보이지 않음plt.figure(figsize=(8,6))df_train['Age'][(df_train['Pclass']==2)&amp;(df_train['Survived']==0)].plot(kind='hist')df_train['Age'][(df_train['Pclass']==2)&amp;(df_train['Survived']==1)].plot(kind='hist')plt.xlabel('Age')plt.title('Age Distribution within classes')Text(0.5, 1.0, 'Age Distribution within classes')fig,ax=plt.subplots(1,1,figsize=(9,5))sns.kdeplot(df_train[(df_train['Survived']==0)&amp;(df_train['Pclass']==3)]['Age'],ax=ax)sns.kdeplot(df_train[(df_train['Survived']==1)&amp;(df_train['Pclass']==3)]['Age'],ax=ax)plt.legend(['Survived==1','Survived==0'])plt.title('3rd class')plt.show()# 히스토그램은 겹치면 보이지 않음plt.figure(figsize=(8,6))df_train['Age'][(df_train['Pclass']==3)&amp;(df_train['Survived']==0)].plot(kind='hist')df_train['Age'][(df_train['Pclass']==3)&amp;(df_train['Survived']==1)].plot(kind='hist')plt.xlabel('Age')plt.title('Age Distribution within classes')Text(0.5, 1.0, 'Age Distribution within classes')change_age_range_survival_ratio=[]for i in range(1,80):    change_age_range_survival_ratio.append(df_train[df_train['Age']&lt;i]['Survived'].sum()/len(df_train[df_train['Age']&lt;i]['Survived']))    plt.figure(figsize=(7,7))plt.plot(change_age_range_survival_ratio)plt.title('Survial rate change depending on range of Age',y=1.02)plt.ylabel=('Survival rate')plt.xlabel('Range of Age(0~x)')plt.show()i=10df_train[df_train['Age']&lt;i]['Survived'].sum() / len(df_train[df_train['Age']&lt;i]['Survived'])0.6129032258064516Pclass, Sex, Agef,ax=plt.subplots(1,2,figsize=(18,8))sns.violinplot('Pclass','Age',hue='Survived',data=df_train,scale='count',split=True,ax=ax[0])ax[0].set_title('Pclass and Age vs Survived')ax[0].set_yticks(range(0,110,10))sns.violinplot('Sex','Age',hue='Survived',data=df_train,scale='count',split=True,ax=ax[1])ax[1].set_title('Sex and Age vs Survived')ax[1].set_yticks(range(0,110,10))plt.show()# split=Falsef,ax=plt.subplots(1,2,figsize=(18,8))sns.violinplot('Pclass','Age',hue='Survived',data=df_train,scale='count',split=False,ax=ax[0])ax[0].set_title('Pclass and Age vs Survived')ax[0].set_yticks(range(0,110,10))sns.violinplot('Sex','Age',hue='Survived',data=df_train,scale='count',split=False,ax=ax[1])ax[1].set_title('Sex and Age vs Survived')ax[1].set_yticks(range(0,110,10))plt.show()# scale 차이 같은 면적이기 때문에 count보다 숫자의 개념이 보기 힘듬f,ax=plt.subplots(1,2,figsize=(18,8))sns.violinplot('Pclass','Age',hue='Survived',data=df_train,scale='area',split=False,ax=ax[0])ax[0].set_title('Pclass and Age vs Survived')ax[0].set_yticks(range(0,110,10))sns.violinplot('Sex','Age',hue='Survived',data=df_train,scale='area',split=False,ax=ax[1])ax[1].set_title('Sex and Age vs Survived')ax[1].set_yticks(range(0,110,10))plt.show()Embarked# Embarked 비율f, ax= plt.subplots(1,1, figsize=(7,7))df_train[['Embarked','Survived']].groupby(['Embarked'],as_index=True).mean().sort_values(by='Survived',ascending=False).plot.bar(ax=ax)&lt;AxesSubplot:xlabel='Embarked'&gt;# sort_valuesdf_train[['Embarked','Survived']].groupby(['Embarked'],as_index=True).mean().sort_values(by='Survived')                  Survived              Embarked                        S      0.336957              Q      0.389610              C      0.553571      # 내림차순df_train[['Embarked','Survived']].groupby(['Embarked'],as_index=True).mean().sort_values(by='Survived',ascending=False)                  Survived              Embarked                        C      0.553571              Q      0.389610              S      0.336957      # sort_indexdf_train[['Embarked','Survived']].groupby(['Embarked'],as_index=True).mean().sort_index()                  Survived              Embarked                        C      0.553571              Q      0.389610              S      0.336957      f, ax=plt.subplots(2,2,figsize=(20,15))sns.countplot('Embarked',data=df_train,ax=ax[0,0])ax[0,0].set_title('(1) No. Of Passengers Boarded')sns.countplot('Embarked', hue='Sex',data=df_train,ax=ax[0,1])ax[0,1].set_title('(2) Male-Feamle split for embarked')sns.countplot('Embarked', hue='Survived',data=df_train,ax=ax[1,0])ax[1,0].set_title('(3) Embarked vs Survived')sns.countplot('Embarked', hue='Pclass',data=df_train,ax=ax[1,1])ax[1,1].set_title('(4) Embarked vs Pclass')# 좌우간격, 상하간격 맞추기plt.subplots_adjust(wspace=0.2,hspace=0.5)plt.show()Family - Sibsp + Parchdf_train['FamilySize']=df_train['SibSp']+df_train['Parch']+1print('Maximum size of Family:',df_train['FamilySize'].max())print('Minimum size of Family:',df_train['FamilySize'].min())Maximum size of Family: 11Minimum size of Family: 1f, ax=plt.subplots(1,3,figsize=(40,10))sns.countplot('FamilySize',data=df_train,ax=ax[0])ax[0].set_title('(1) No. Of Passenger Boarded',y=1.02)sns.countplot('FamilySize',hue='Survived',data=df_train,ax=ax[1])ax[1].set_title('(2) Survived countplot depending on FamilSize',y=1.02)df_train[['FamilySize','Survived']].groupby(['FamilySize'],as_index=True).mean().sort_values(by='Survived',ascending=False).plot.bar(ax=ax[2])ax[2].set_title('(3) Survived rate depending on FamilySize',y=1.02)plt.subplots_adjust(wspace=0.2,hspace=0.5)plt.show()Faredf_test.loc[df_test.Fare.isnull(), 'Fare'] = df_test['Fare'].mean()df_train['Fare'] = df_train['Fare'].map(lambda i: np.log(i) if i &gt; 0 else 0)df_test['Fare'] = df_test['Fare'].map(lambda i: np.log(i) if i &gt; 0 else 0)fig, ax=plt.subplots(1,1,figsize=(8,8))g=sns.distplot(df_train['Fare'],color='b',label='Skweness {:.2f}'.format(df_train['Fare'].skew()),ax=ax)g=g.legend(loc='best')df_train['Fare']=df_train['Fare'].map(lambda i:np.log(i) if i&gt;0 else 0)df_train['Ticket'].value_counts()1601        7347082      7CA. 2343    7347088      63101295     6           ..PC 17318    131418       1345765      1244270      1244278      1Name: Ticket, Length: 681, dtype: int64Fill Null in Agedf_train['Age'].isnull().sum()177df_train['Age'].mean()29.69911764705882# str로 변환한 뒤 extract와 정규표현식을 통해 추출df_train['Initial']= df_train.Name.str.extract('([A-Za-z]+)\\.')df_test['Initial']= df_test.Name.str.extract('([A-Za-z]+)\\.') pd.crosstab(df_train['Initial'],df_train['Sex']).T.style.background_gradient(cmap='summer_r')            Initial        Capt        Col        Countess        Don        Dr        Jonkheer        Lady        Major        Master        Miss        Mlle        Mme        Mr        Mrs        Ms        Rev        Sir                Sex                                                                                                                                                                                    female                        0                        0                        1                        0                        1                        0                        1                        0                        0                        182                        2                        1                        0                        125                        1                        0                        0                                                male                        1                        2                        0                        1                        6                        1                        0                        2                        40                        0                        0                        0                        517                        0                        0                        6                        1                df_train['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)df_test['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)df_train.groupby('Initial').mean()                  PassengerId      Survived      Pclass      Age      SibSp      Parch      Fare      FamilySize              Initial                                                                  Master      414.975000      0.575000      2.625000      4.574167      2.300000      1.375000      1.190112      4.675000              Miss      411.741935      0.704301      2.284946      21.860000      0.698925      0.537634      1.085686      2.236559              Mr      455.880907      0.162571      2.381853      32.739609      0.293006      0.151229      0.932798      1.444234              Mrs      456.393701      0.795276      1.984252      35.981818      0.692913      0.818898      1.207905      2.511811              Other      564.444444      0.111111      1.666667      45.888889      0.111111      0.111111      0.958425      1.222222      df_train.groupby('Initial')['Survived'].mean().plot.bar()&lt;AxesSubplot:xlabel='Initial'&gt;df_train.loc[(df_train['Age'].isnull())&amp;(df_train['Initial']=='Mr'),'Age']=33df_train.loc[(df_train['Age'].isnull())&amp;(df_train['Initial']=='Mrs'),'Age']=36df_train.loc[(df_train['Age'].isnull())&amp;(df_train['Initial']=='Master'),'Age']=5df_train.loc[(df_train['Age'].isnull())&amp;(df_train['Initial']=='Miss'),'Age']=22df_train.loc[(df_train['Age'].isnull())&amp;(df_train['Initial']=='Other'),'Age']=46df_test.loc[(df_test['Age'].isnull())&amp;(df_test['Initial']=='Mr'),'Age']=33df_test.loc[(df_test['Age'].isnull())&amp;(df_test['Initial']=='Mrs'),'Age']=36df_test.loc[(df_test['Age'].isnull())&amp;(df_test['Initial']=='Master'),'Age']=5df_test.loc[(df_test['Age'].isnull())&amp;(df_test['Initial']=='Miss'),'Age']=22df_test.loc[(df_test['Age'].isnull())&amp;(df_test['Initial']=='Other'),'Age']=46df_train.loc[(df_train['Initial']=='Mr'),'Age'].isnull().sum&lt;bound method Series.sum of 0      False4      False5      False6      False12     False       ...  881    False883    False884    False889    False890    FalseName: Age, Length: 529, dtype: bool&gt;Fill Null in Embarked and categorize Agedf_train['Embarked'].isnull().sum()2df_train['Embarked'].fillna('S',inplace=True)df_train['Embarked'].isnull().sum()0df_train.head()                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked      FamilySize      Initial                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      0.683603      NaN      S      2      Mr              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      1.450832      C85      C      2      Mrs              2      3      1      3      Heikkinen, Miss. Laina      female      26.0      0      0      STON/O2. 3101282      0.727559      NaN      S      1      Miss              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      35.0      1      0      113803      1.379314      C123      S      2      Mrs              4      5      0      3      Allen, Mr. William Henry      male      35.0      0      0      373450      0.735091      NaN      S      1      Mr      df_train.loc[df_train['Age']&lt;10,'Age_cat']=0df_train.loc[(df_train['Age']&gt;=10)&amp;(df_train['Age']&lt;20),'Age_cat']=1df_train.loc[(df_train['Age']&gt;=20)&amp;(df_train['Age']&lt;30),'Age_cat']=2df_train.loc[(df_train['Age']&gt;=30)&amp;(df_train['Age']&lt;40),'Age_cat']=3df_train.loc[(df_train['Age']&gt;=40)&amp;(df_train['Age']&lt;50),'Age_cat']=4df_train.loc[(df_train['Age']&gt;=50)&amp;(df_train['Age']&lt;60),'Age_cat']=5df_train.loc[(df_train['Age']&gt;=60)&amp;(df_train['Age']&lt;70),'Age_cat']=6df_train.loc[df_train['Age']&gt;=70,'Age_cat']=7df_test.loc[df_test['Age']&lt;10,'Age_cat']=0df_test.loc[(df_test['Age']&gt;=10)&amp;(df_test['Age']&lt;20),'Age_cat']=1df_test.loc[(df_test['Age']&gt;=20)&amp;(df_test['Age']&lt;30),'Age_cat']=2df_test.loc[(df_test['Age']&gt;=30)&amp;(df_test['Age']&lt;40),'Age_cat']=3df_test.loc[(df_test['Age']&gt;=40)&amp;(df_test['Age']&lt;50),'Age_cat']=4df_test.loc[(df_test['Age']&gt;=50)&amp;(df_test['Age']&lt;60),'Age_cat']=5df_test.loc[(df_test['Age']&gt;=60)&amp;(df_test['Age']&lt;70),'Age_cat']=6df_test.loc[df_test['Age']&gt;=70,'Age_cat']=7df_train.head()                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked      FamilySize      Initial      Age_cat                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      0.683603      NaN      S      2      Mr      2.0              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      1.450832      C85      C      2      Mrs      3.0              2      3      1      3      Heikkinen, Miss. Laina      female      26.0      0      0      STON/O2. 3101282      0.727559      NaN      S      1      Miss      2.0              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      35.0      1      0      113803      1.379314      C123      S      2      Mrs      3.0              4      5      0      3      Allen, Mr. William Henry      male      35.0      0      0      373450      0.735091      NaN      S      1      Mr      3.0      df_test.head()                  PassengerId      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked      Initial      Age_cat                  0      892      3      Kelly, Mr. James      male      34.5      0      0      330911      2.057860      NaN      Q      Mr      3.0              1      893      3      Wilkes, Mrs. James (Ellen Needs)      female      47.0      1      0      363272      1.945910      NaN      S      Mrs      4.0              2      894      2      Myles, Mr. Thomas Francis      male      62.0      0      0      240276      2.270836      NaN      Q      Mr      6.0              3      895      3      Wirz, Mr. Albert      male      27.0      0      0      315154      2.159003      NaN      S      Mr      2.0              4      896      3      Hirvonen, Mrs. Alexander (Helga E Lindqvist)      female      22.0      1      1      3101298      2.508582      NaN      S      Mrs      2.0      def category_age(x):    if x&lt;10:        return 0    elif x&lt;20:        return 1    elif x&lt;30:        return 2    elif x&lt;40:        return 3    elif x&lt;50:        return 4    elif x&lt;60:        return 5    elif x&lt;70:        return 6    else:        return 7df_train['Age_cat_2']=df_train['Age'].apply(category_age)df_train.head()                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked      FamilySize      Initial      Age_cat      Age_cat_2                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      0.683603      NaN      S      2      Mr      2.0      2              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      1.450832      C85      C      2      Mrs      3.0      3              2      3      1      3      Heikkinen, Miss. Laina      female      26.0      0      0      STON/O2. 3101282      0.727559      NaN      S      1      Miss      2.0      2              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      35.0      1      0      113803      1.379314      C123      S      2      Mrs      3.0      3              4      5      0      3      Allen, Mr. William Henry      male      35.0      0      0      373450      0.735091      NaN      S      1      Mr      3.0      3      (df_train['Age_cat']==df_train['Age_cat_2']).all()Truedf_train.drop(['Age','Age_cat_2'],axis=1,inplace=True)df_test.drop(['Age'],axis=1,inplace=True)Change string to categorical and Pearson coefficientdf_train.Initial.unique()array(['Mr', 'Mrs', 'Miss', 'Master', 'Other'], dtype=object)df_train.loc[df_train['Initial']=='Master','Initial']7      Master16     Master50     Master59     Master63     Master65     Master78     Master125    Master159    Master164    Master165    Master171    Master176    Master182    Master183    Master193    Master261    Master278    Master305    Master340    Master348    Master386    Master407    Master445    Master480    Master489    Master549    Master709    Master751    Master755    Master787    Master788    Master802    Master803    Master819    Master824    Master827    Master831    Master850    Master869    MasterName: Initial, dtype: objectdf_train['Initial'] = df_train['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})df_test['Initial'] = df_test['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})df_train.Embarked.unique()array(['S', 'C', 'Q'], dtype=object)df_train['Embarked'].value_counts()S    646C    168Q     77Name: Embarked, dtype: int64df_train['Embarked']=df_train['Embarked'].map({'C':0,'Q':1,'S':2})df_test['Embarked']=df_test['Embarked'].map({'C':0,'Q':1,'S':2})df_train.head()                  PassengerId      Survived      Pclass      Name      Sex      SibSp      Parch      Ticket      Fare      Cabin      Embarked      FamilySize      Initial      Age_cat                  0      1      0      3      Braund, Mr. Owen Harris      male      1      0      A/5 21171      0.683603      NaN      2      2      2      2.0              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      1      0      PC 17599      1.450832      C85      0      2      3      3.0              2      3      1      3      Heikkinen, Miss. Laina      female      0      0      STON/O2. 3101282      0.727559      NaN      2      1      1      2.0              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      1      0      113803      1.379314      C123      2      2      3      3.0              4      5      0      3      Allen, Mr. William Henry      male      0      0      373450      0.735091      NaN      2      1      2      3.0      df_train.Embarked.isnull().any()Falsedf_train['Sex'].unique()array(['male', 'female'], dtype=object)df_train['Sex']=df_train['Sex'].map({'female':0,'male':1})df_test['Sex']=df_test['Sex'].map({'female':0,'male':1})heatmap_data=df_train[['Survived','Pclass','Sex','Fare','Embarked','FamilySize','Initial','Age_cat']]heatmap_data.corr()                  Survived      Pclass      Sex      Fare      Embarked      FamilySize      Initial      Age_cat                  Survived      1.000000      -0.338481      -0.543351      0.332593      -0.167675      0.016639      -0.085529      -0.095002              Pclass      -0.338481      1.000000      0.131900      -0.659932      0.162098      0.065997      -0.133054      -0.314809              Sex      -0.543351      0.131900      1.000000      -0.271514      0.108262      -0.200988      0.051687      0.122917              Fare      0.332593      -0.659932      -0.271514      1.000000      -0.177469      0.410847      -0.016650      0.068385              Embarked      -0.167675      0.162098      0.108262      -0.177469      1.000000      0.066516      0.026550      -0.033173              FamilySize      0.016639      0.065997      -0.200988      0.410847      0.066516      1.000000      -0.204574      -0.280537              Initial      -0.085529      -0.133054      0.051687      -0.016650      0.026550      -0.204574      1.000000      0.481309              Age_cat      -0.095002      -0.314809      0.122917      0.068385      -0.033173      -0.280537      0.481309      1.000000      colormap=plt.cm.BuGnplt.figure(figsize=(12,10))plt.title('Pearson Correlation of Features',y=1.05,size=15)sns.heatmap(heatmap_data.astype(float).corr(),linewidths=0.1,vmax=2,square=True,cmap=colormap,linecolor='white',annot=True,annot_kws={'size':16},fmt='.2f')&lt;AxesSubplot:title={'center':'Pearson Correlation of Features'}&gt;One-hot encoding on the Initial and Embarkeddf_test.head()                  PassengerId      Pclass      Name      Sex      SibSp      Parch      Ticket      Fare      Cabin      Embarked      Initial      Age_cat                  0      892      3      Kelly, Mr. James      1      0      0      330911      2.057860      NaN      1      2      3.0              1      893      3      Wilkes, Mrs. James (Ellen Needs)      0      1      0      363272      1.945910      NaN      2      3      4.0              2      894      2      Myles, Mr. Thomas Francis      1      0      0      240276      2.270836      NaN      1      2      6.0              3      895      3      Wirz, Mr. Albert      1      0      0      315154      2.159003      NaN      2      2      2.0              4      896      3      Hirvonen, Mrs. Alexander (Helga E Lindqvist)      0      1      1      3101298      2.508582      NaN      2      3      2.0      df_train = pd.get_dummies(df_train, columns=['Initial'], prefix='Initial')df_test = pd.get_dummies(df_test, columns=['Initial'], prefix='Initial')df_train = pd.get_dummies(df_train, columns=['Embarked'], prefix='Embarked')df_test = pd.get_dummies(df_test, columns=['Embarked'], prefix='Embarked')df_train.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)df_test.drop(['PassengerId', 'Name',  'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)df_test.head()                  Pclass      Sex      Fare      Age_cat      Initial_0      Initial_1      Initial_2      Initial_3      Initial_4      Embarked_0      Embarked_1      Embarked_2                  0      3      1      2.057860      3.0      0      0      1      0      0      0      1      0              1      3      0      1.945910      4.0      0      0      0      1      0      0      0      1              2      2      1      2.270836      6.0      0      0      1      0      0      0      1      0              3      3      1      2.159003      2.0      0      0      1      0      0      0      0      1              4      3      0      2.508582      2.0      0      0      0      1      0      0      0      1      df_train.head()                  Survived      Pclass      Sex      Fare      FamilySize      Age_cat      Initial_0      Initial_1      Initial_2      Initial_3      Initial_4      Embarked_0      Embarked_1      Embarked_2                  0      0      3      1      0.683603      2      2.0      0      0      1      0      0      0      0      1              1      1      1      0      1.450832      2      3.0      0      0      0      1      0      1      0      0              2      1      3      0      0.727559      1      2.0      0      1      0      0      0      0      0      1              3      1      1      0      1.379314      2      3.0      0      0      0      1      0      0      0      1              4      0      3      1      0.735091      1      3.0      0      0      1      0      0      0      0      1      Machine learningl(Randomforest)from sklearn.ensemble import RandomForestClassifierfrom sklearn import metricsfrom sklearn.model_selection import train_test_splitdf_train.head()                  Survived      Pclass      Sex      Fare      FamilySize      Age_cat      Initial_0      Initial_1      Initial_2      Initial_3      Initial_4      Embarked_0      Embarked_1      Embarked_2                  0      0      3      1      0.683603      2      2.0      0      0      1      0      0      0      0      1              1      1      1      0      1.450832      2      3.0      0      0      0      1      0      1      0      0              2      1      3      0      0.727559      1      2.0      0      1      0      0      0      0      0      1              3      1      1      0      1.379314      2      3.0      0      0      0      1      0      0      0      1              4      0      3      1      0.735091      1      3.0      0      0      1      0      0      0      0      1      df_test.head()                  Pclass      Sex      Fare      Age_cat      Initial_0      Initial_1      Initial_2      Initial_3      Initial_4      Embarked_0      Embarked_1      Embarked_2                  0      3      1      2.057860      3.0      0      0      1      0      0      0      1      0              1      3      0      1.945910      4.0      0      0      0      1      0      0      0      1              2      2      1      2.270836      6.0      0      0      1      0      0      0      1      0              3      3      1      2.159003      2.0      0      0      1      0      0      0      0      1              4      3      0      2.508582      2.0      0      0      0      1      0      0      0      1      X_train=df_train.drop('Survived',axis=1).valuestarget_label=df_train['Survived'].valuesX_test=df_test.valuesX_tr, X_vld, y_tr, y_vld = train_test_split(X_train, target_label, test_size=0.3, random_state=2018)model = RandomForestClassifier()model.fit(X_tr, y_tr)prediction = model.predict(X_vld)print('총 {}명 중 {:.2f}% 정확도로 생존 맞춤'.format(y_vld.shape[0], 100 * metrics.accuracy_score(prediction, y_vld)))총 268명 중 82.09% 정확도로 생존 맞춤feature importance and prediction on test setmodel.feature_importances_array([0.09818595, 0.10792619, 0.32805606, 0.09146926, 0.1232598 ,       0.01214762, 0.04206886, 0.11624336, 0.02958443, 0.0041871 ,       0.01512252, 0.01363153, 0.01811731])from pandas import Seriesfeature_importance = model.feature_importances_Series_feat_imp = Series(feature_importance, index=df_test.columns)plt.figure(figsize=(8, 8))Series_feat_imp.sort_values(ascending=True).plot.barh()plt.xlabel('Feature importance')plt.ylabel('Feature')plt.show()submission = pd.read_csv('gender_submission.csv')submission.head()                   PassengerId      Survived                  0      892      0              1      893      1              2      894      0              3      895      0              4      896      1      prediction = model.predict(X_test)submission['Survived'] = prediction",
        "url": "/programming-kaggle2"
    }
    ,
    
    "programming-baekjoon6": {
        "title": "백준 (6) &lt;br&gt; (3085, 2563, 4673, 5635, 11170)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)    백준 (9) 우선순위 큐 알고리즘 (1927,11279,11286,1715,11766)백준 5문제를 풀어보았다.중복되는 문제도 있습니다3085번 사탕 게임https://www.acmicpc.net/problem/3085먼저 보드는 한 글자당 리스트 한 요소를 차지하게끔 이중으로 만든다.check 함수를 통해 보드에 연속된 사탕이 몇개인지 만들어준다.그 다음 for문을 통해 값들을 하나씩 변경해보고 가장 많은 값을 찾는다.num=int(input())board=[]answer=0for i in range(num):    candy=list(input())    board.append(candy)3CCPCCPPCCdef check(board):    n=len(board)    answer=1 #연속된 사탕의 결과        for i in range(n):        count=1        for j in range(1, n):            if board[i][j] == board[i][j-1]:  #열에서 같다면 +1 해주기                count += 1            else:                count=1    # 같지 않다면 1로 초기화            if count &gt; answer:                answer = count # 가장 큰 값을 answer로 반환        count=1 # 행 결과를 찾기 위한 초기화        for j in range(1, n):            if board[j][i] == board[j-1][i]: # 행에서 최대값 찾기                count += 1            else:                count=1            if count &gt; answer:                answer = count    return answeranswer=0for i in range(num):    for j in range(num):        if j+1 &lt; num:            board[i][j],board[i][j+1] =board[i][j+1],board[i][j] # 열에서 값 바꾸기            temp=check(board) # 최대 개수 확인하기            if temp &gt; answer:                answer = temp # 가장 많은 값으로 저장            board[i][j], board[i][j+1] = board[i][j+1], board[i][j] # 값 초기화하기        if i+1 &lt; num: #행에서 마찬가지로 진행            board[i][j], board[i+1][j] = board[i+1][j], board[i][j]            temp=check(board)            if temp &gt; answer:                answer = temp                        board[i][j], board[i+1][j] = board[i+1][j], board[i][j]            print(answer)32563번 색종이https://www.acmicpc.net/problem/2563100 * 100의 흰 색종이를 먼저 만들어준다.다음 입력받은 색종이만큼 0을 1로 바꿔주면 중복도 해결하면서 검은색을 표시할 수 있다.1이 된 숫자의 부분만 세면 된다.paper=[[0 for i in range(101)] for j in range(101)]for i in range(int(input())):    x,y=map(int,input().split())    for j in range(x,x+10):        for k in range(y,y+10):            paper[j][k]=1result=0for i in paper:    result += i.count(1)print(result)33 715 75 22604673번 색종이https://www.acmicpc.net/problem/4673for문을 통해 숫자들의 셀프 넘버를 구해서 초기 [1:10000]의 리스트에서 셀프 넘버가 나오면 제거해주었다.self_num=[i for i in range(1,10001)]for i in range(1,10001):    total=i    num=str(i)    for j in range(len(num)):        total=total+int(num[j])    if total in self_num:        self_num.remove(total)for i in range(len(self_num)):    print(self_num[i])135792031425364758697108110121132143154165176187198209211222233244255266277288299310312323334345356367378389400411413424435446457468479490501512514525536547558569580591602613615626637648659670681692703714716727738749760771782793804815817828839850861872883894905916918929940951962973984995100610211032104310541065107610871098110911111122113311441155116611771188119912101212122312341245125612671278128913001311131313241335134613571368137913901401141214141425143614471458146914801491150215131515152615371548155915701581159216031614161616271638164916601671168216931704171517171728173917501761177217831794180518161818182918401851186218731884189519061917191919301941195219631974198519962007202220332044205520662077208820992110211221232134214521562167217821892200221122132224223522462257226822792290230123122314232523362347235823692380239124022413241524262437244824592470248124922503251425162527253825492560257125822593260426152617262826392650266126722683269427052716271827292740275127622773278427952806281728192830284128522863287428852896290729182920293129422953296429752986299730083023303430453056306730783089310031113113312431353146315731683179319032013212321432253236324732583269328032913302331333153326333733483359337033813392340334143416342734383449346034713482349335043515351735283539355035613572358335943605361636183629364036513662367336843695370637173719373037413752376337743785379638073818382038313842385338643875388638973908391939213932394339543965397639873998400940244035404640574068407940904101411241144125413641474158416941804191420242134215422642374248425942704281429243034314431643274338434943604371438243934404441544174428443944504461447244834494450545164518452945404551456245734584459546064617461946304641465246634674468546964707471847204731474247534764477547864797480848194821483248434854486548764887489849094920492249334944495549664977498849995010502550365047505850695080509151025113511551265137514851595170518151925203521452165227523852495260527152825293530453155317532853395350536153725383539454055416541854295440545154625473548454955506551755195530554155525563557455855596560756185620563156425653566456755686569757085719572157325743575457655776578757985809582058225833584458555866587758885899591059215923593459455956596759785989600060116026603760486059607060816092610361146116612761386149616061716182619362046215621762286239625062616272628362946305631663186329634063516362637363846395640664176419643064416452646364746485649665076518652065316542655365646575658665976608661966216632664366546665667666876698670967206722673367446755676667776788679968106821682368346845685668676878688969006911692269246935694669576968697969907001701270277038704970607071708270937104711571177128713971507161717271837194720572167218722972407251726272737284729573067317731973307341735273637374738573967407741874207431744274537464747574867497750875197521753275437554756575767587759876097620762276337644765576667677768876997710772177237734774577567767777877897800781178227824783578467857786878797890790179127923792579367947795879697980799180028013802880398050806180728083809481058116811881298140815181628173818481958206821782198230824182528263827482858296830783188320833183428353836483758386839784088419842184328443845484658476848784988509852085228533854485558566857785888599861086218623863486458656866786788689870087118722872487358746875787688779879088018812882388258836884788588869888088918902891389248926893789488959897089818992900390149029904090519062907390849095910691179119913091419152916391749185919692079218922092319242925392649275928692979308931993219332934393549365937693879398940994209422943394449455946694779488949995109521952395349545955695679578958996009611962296249635964696579668967996909701971297239725973697479758976997809791980298139824982698379848985998709881989299039914992599279938994999609971998299935635번 생일https://www.acmicpc.net/problem/5635for문을 통해 글자들을 입력받고 연-월-일 부분을 정수형으로 치환한다.sort와 lambda를 이용해서 오름차순으로 정렬하면 가장 마지막 사람이 어리고 가장 처음 사람이 나이가 가장 많을 것이다.people=[]for i in range(int(input())):    person=list(input().split(' '))    person[1]=int(person[1])    person[2]=int(person[2])    person[3]=int(person[3])    people.append(person)people.sort(key=lambda x:[x[3],x[2],x[1]])print(people[-1][0])print(people[0][0])4Mickey 1 10 1991Jerry 18 9 1990Tom 15 8 1993Alice 30 12 1990[['Jerry', 18, 9, 1990], ['Alice', 30, 12, 1990], ['Mickey', 1, 10, 1991], ['Tom', 15, 8, 1993]]TomJerry11170번 0의 개수https://www.acmicpc.net/problem/11170a,b 두 숫자 사이의 모든 숫자들을 붙여서 하나의 글자로 만들어준다음 count함수를 사용하였다.for i in range(int(input())):    a,b=input().split()        word=''    for j in range(int(a),int(b)+1):        word=word+str(j)        print(word.count('0'))30 10233 10051991 40",
        "url": "/programming-baekjoon6"
    }
    ,
    
    "programming-baekjoon5": {
        "title": "백준 (5) 정렬 알고리즘 &lt;br&gt; (2750,11399,2751,1427, &lt;br&gt; 10989,1181,11650,2309)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)    백준 (9) 우선순위 큐 알고리즘 (1927,11279,11286,1715,11766)정렬에 관련된 8문제를 풀어보았다.중복되는 문제도 있습니다https://www.acmicpc.net/problemset?sort=ac_desc&amp;algo=158정렬은 병합 정렬, 분할 정복, 퀵 정렬, 힙 정렬, 계수 정렬 등 공부할 개념들이 많았다.2750번 수 정렬하기https://www.acmicpc.net/problem/2750sorted를 통해 입력받은 숫자들을 오름차순으로 정렬하는 문제였다.num=int(input())n_list=sorted([int(input()) for i in range(num)])for j in n_list:    print(j)5523411234511399번 ATMhttps://www.acmicpc.net/problem/11399total과 temp를 통해 temp에서는 n번째 사람이 걸리는 시간을 저장해주고 total에 전 사람까지 걸린 시간을 저장해주었다.people=int(input()) #사람수 입력time=sorted(list(map(int,input().split()))) #map으로 split된 값을 list로 저장 후 sorted로 오름차순 정렬53 1 4 3 2total=0 #n번째 사람까지 걸린 시간의 총합temp=0 #n번째 사람이 기다린 시간for i in time:    temp=temp+i #n번째 사람이 기다린 시간= 이전 사람이 기다린 시간+ 그 사람이 걸리는 시간    total=temp+total print(total)322751번 수 정렬하기 2https://www.acmicpc.net/problem/2751pypy3으로 제출하면 이렇게도 해결이 된다.num=int(input())n_list=sorted([int(input()) for i in range(num)])for j in n_list:    print(j)55432112345하지만 python3로는 시간초과가 발생한다.이 문제는 앞에 2750번과 다르게 N의 범위가 100만까지이다. 따라서 시간복잡도 문제라고 볼 수 있다.시간복잡도를 할 수 있는 방법은 병합 정렬, 퀵 정렬, 힙 정렬이 있다.병합 정렬병합 정렬은 데이터를 절반씩 나누어 끝까지 갔다가 다시 절반씩 합치면서 정렬하는 방법이다.이 때 분할 단계에서 깊이가 logN에 비례하지만, 깊이별로 수행되는 merge의 시간복잡도는 O(N)이다.  리스트 요소가 1개가 될때까지 나눈다.  분리한 왼쪽리스트, 오른쪽 리스트의 각각 첫번째 요소를 비교해 더 작은 값을 결과 리스트에 저장한다.  저장한 값은 리스트에서 지운다.  두 리스트 모두 요소가 하나도 안남을 때까지 반복한다.병합 정렬을 이용한 풀이병합 정렬을 이용할 때는 먼저 def로 병합 정렬 함수를 만들어준다.모든 리스트 요소가 1개가 될때까지 나눈다.따라서 중간값을 기준으로 나누어주면 된다.여기서도 input 대신 sys.stdin.readline()를 사용해야한다.import sysn=int(input())unsorted=[]result=[]# 분할def Divided(list):    #길이가 1일때 중단    if len(list)&lt;=1:        return list    #중간값을 기준으로 리스트 분할    mid = len(list)//2    less_part=list[:mid]    more_part=list[mid:]    less_part=Divided(less_part)    more_part=Divided(more_part)    return merge(less_part,more_part)#비교와 합병def merge(less,more):    merged_list=[]    l,h=0,0    #less와 more을 돌면서 대소관계 비교 후 작은 곳에 append    while l&lt;len(less) and h&lt;len(more):        if less[l]&lt;more[h]:            merged_list.append(less[l])            l=l+1        else:            merged_list.append(more[h])            h=h+1    merged_list+=less[l:]    merged_list+=more[h:]    return merged_listfor i in range(n):    num=int(input())    unsorted.append(num)result=Divided(unsorted)for i in result:    print(i)7643125712345671427번 소트인사이드https://www.acmicpc.net/problem/1427N을 문자열로 입력받아서 각 글자들을 list에 넣고 sort를 해줘도 오름차순으로 정렬이 된다.따라서 reverse를 통해 내림차순으로 만들어주고 join으로 다시 문자열로 만들어주었다.N=input()2143print(''.join(sorted([i for i in N],reverse=True)))432110989 수 정렬하기 3https://www.acmicpc.net/problem/10989메모리 초과가 발생하는 코드sys.stdin.readline()를 사용하여도, pypy3를 사용하여도 메모리 초과가 발생한다.아마 첫째 줄에 범위가 1&lt;=N&lt;=10000000 까지 넓고 수 또한 10000 이하의 자연수로 매우 크기 때문이 아닐까 생각된다.num=int(input())n_list=sorted([int(input()) for i in range(num)])for j in n_list:    print(j)1052314235171122334557계수 정렬(counting sort) 알고리즘따라서 검색을 해보니 계수 정렬 알고리즘이라는 방식이 있다고 한다.계수 정렬의 특징은  데이터의 크기 범위가 제한되어 정수 형태로 표현할 수 있을 때만 사용할 수 있다.매우 빠르다.모든 범위를 담을 수 있는 리스트를 선언해야한다.즉, 일단 범위가 되는 모든 자연수의 크기와 같은 리스트를 생성해야한다.숫자를 입력 받으면 해당하는 숫자가 나타내는 리스트 인덱스에 +1을 해주는 것이다.따라서 숫자가 입력 받았으면 1 이상이 나올 것이고 아니라면 0일 것이다.입력받은 숫자의 빈도를 물어본다면 인덱스와 값을 출력시키면 될 것이고입력받은 숫자를 정렬한다면 0이 아닌 리스트의 인덱스 값을 순서대로 출력해주면 된다.계수 정렬을 이용한 풀이따라서 nlist를 통해 자연수 범위만큼 0인 리스트를 선언해주었다.그 다음 입력받은 n값을 nlist의 인덱스에서 찾아서 +1 해주었다.그 다음 nlist의 값들을 찾으면서 0보다 큰 값들만 찾아주고 그 값들의 인덱스만 추출해주면 된다.이 문제도 input을 사용하면 시간 초과가 발생한다.#계수 정렬을 이용한 풀이import sysn=int(input())nlist=[0 for i in range(10001)]for i in range(n):    nlist[int(input())]+=1for i in range(len(nlist)):    if nlist[i]&gt;0:        for j in range(nlist[i]):            print(i)105231423517112233455711650번 좌표 정렬하기https://www.acmicpc.net/problem/11650 sort() 함수에서 key 파라미터에 (x,y)를 넣어주면 x를 우선 정렬해주고 y를 정렬해준다.주피터 노트북을 사용하면서 sys.stdin.readline()을 사용한 적이 없었는데 주피터 노트북에선 stdin이 잘 구현이 안된다고 한다.따라서 밑에 코드에는 input()을 사용하였지만 백준에 제출할 때는 sys.stdin.readline()으로 제출하였다.input()을 하게되면 시간 초과 결과가 나오고 pypy3로 제출하면 input()을 사용해도 괜찮은 것으로 보인다.import sysN=input()nlist=[]for i in range(int(N)):    x,y=map(int,input().split()) #실제 제출에서는 sys.stdin.readline()을 사용하였다    nlist.append((x,y))53 41 11 -12 23 3nlist.sort(key=lambda x: (x[0],x[1]))for j in nlist:    print(j[0],j[1])1 -11 12 23 33 42309번 일곱 난쟁이https://www.acmicpc.net/problem/2309이 문제는 input()을 사용하여도 시간초과는 나오지 않는다.n_list에서 두 가지씩 고르는 모든 경우의 수를 찾아야하기 때문에 이중 for문을 사용하였다.j는 i와 같이 않아야하기 때문에 i+1로 중복을 피했다.모든 값의 합이 100 이 되기 때문에 두 개씩 골라서 빼주었을 때 100 이 되면 그 두 값을 remove를 통해 제거하고 sorted로 내림차순 정렬한 뒤 출력해주면 된다.첫 for문이 끝나야하기 때문에 조건에 맞는 답을 골라주면 멈추면 된다.n_list=[]for i in range(1,10):    tall=int(input())    n_list.append(tall)2072319101525813result= sum(n_list)for i in range(9):    for j in range(i+1,9):        if result-(n_list[i]+n_list[j])==100:            a,b=n_list[i],n_list[j]            n_list.remove(a)            n_list.remove(b)            n_list=sorted(n_list)            for k in range(7):                print(n_list[k])            break    if len(n_list)==7:        break781013192023",
        "url": "/programming-baekjoon5"
    }
    ,
    
    "study-ml2": {
        "title": "머신러닝 정리 (2) &lt;br&gt; 지도학습 (2)",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학    머신러닝 정리 (6)-모델 평가와 성능 향상머신러닝 정리 (2) - 지도학습 (2)본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python지도학습 (2)  결정 트리  결정 트리의 앙상블  배깅, 엑스트라 트리, 에이다부스트          배깅      엑스트라 트리      선형 모델      에이다부스트        커널 서포트 벡터 머신  신경망 (딥러닝)  분류 예측의 불확실성 추정          결정 함수      예측 확률      다중 분류에서의 불확실성      결정 트리Decision Tree 결정 트리는 분류와 회귀 문제에서 사용되는 모델입니다.스무고개처럼 예/아니오로 나눌 수 있는 조건을 통해서 결정에 다다르게 됩니다.질문과 정답은 노드가 되고 특히 마지막 노드는 리프라고 합니다. 결정트리의 구조는 왼쪽 하단에 사진처럼 가장 위에는 Root node, 질문과 답을 연결하는 Edge, 내부의 Internal node, 마지막 노드는 Leaf node, 그리고 Depth로 구성됩니다.결정 트리 만들기 결정 트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예/아니오 질문 (TEST) 목록을 학습한다는 뜻입니다.보통 데이터들은 예/아니오 특성으로 구분되지 않고  연속적인 특성을 가진 2차원 데이터 셋에서 보통 ‘특성 i는 값 a보다 큰가?’의 형태와 같은 테스트를 가집니다.이 데이터들을 X[1]&lt;=0.6인 테스트로 나누어 봅니다.알고리즘은 가능한 모든 테스트에서 타깃 값에 대해 가장 많은 정보를 가진 것을 고르게 됩니다.따라서 X[1]&lt;=0.6인 테스트를 선택하게 됩니다.결정 트리에서 각 테스트는 하나의 축을 따라 데이터를 나눕니다.하나의 질문당 하나의 축을 만들어서 영역이 한 개의 타깃값을 가질 때까지 반복됩니다. 결정 트리의 멈춤 조건입니다.즉, 미리 정의한 조건들이 없다면 가지를 만들 수 있을 때까지 만드는 것을 알 수 있습니다.결정트리의 예측은 그 포인트가 어느 리프에 들어갈지 확인하는 것인데 분류는 타깃 값 중 다수인 것이 예측 결과가 되고 회귀의 경우 리프 노드의 훈련 데이터 평균값이 결과로 출력됩니다.결정 트리 복잡도 제어하기 결정 경계가 클래스 포인트에 멀리 떨어진 이상치에 민감하게 되어 모든 리프 노드가 순수 노드가 될 때까지 진행하면 모델이 복잡해지고 과대적합이 발생합니다.과대적합을 막기 위한 방법은 크게 사전가지치기, 사후 가지치기 두 가지입니다.사전 가지치기는 이름에서 알 수 있듯이 모델을 만들 때 깊이나 리프의 개수 또는 테스트의 최소 개수를 미리 제한하는 것입니다.미리 제한하기 때문에 정말로 중요한 포인트를 분류하지않을 수 있습니다.사후 가지치기 역시 이름에서 알 수 있듯이 트리가 만들어진 뒤 포인트가 적은 노드를 삭제 혹은 병합하게 되는데 에러감소 프루닝, 룰 포스트 프루닝 같은 방법들이 있습니다.참고에러감소 프루닝  모든 노드를 프루닝 대상으로 고려노드 제거 후 검증을 통해 제거 전, 후 정확도 비교제거 전보다 정확도가 낮아지기 전까지 반복룰 포스트 프루닝  의사결정 트리를 룰셋으로 변환 (룰은 루트부터 리프까지의 경로)이 룰셋 속성들에 정확도를 떨어뜨리는 속성을 제거프루닝 완료 후 정확도 순으로 정렬해 이 순서대로 적용결정 트리는 다음과 같이 만들 수 있고 정확도를 확인할 수 있습니다.Default값은 모든 리프가 순수 노드가 되는 모델을 만들기 때문에 훈련 세트의 정확도가 100%가 됩니다.하지만 트리가 무한정 깊어지고 복잡해지고 일반화가 잘 되지 않습니다.from sklearn.tree import DecisionTreeClassifiercancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split(    cancer.data, cancer.target, stratify=cancer.target, random_state=42)tree = DecisionTreeClassifier(random_state=0)tree.fit(X_train, y_train)print(\"훈련 세트 정확도: {:.3f}\".format(tree.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(tree.score(X_test, y_test)))훈련 세트 정확도: 1.000테스트 세트 정확도: 0.937과대적합 때문에 반드시 훈련 세트의 정확도가 테스트 정확도와 비례하지 않아서 max_depth와 같은 파라미터를 통해 과대적합을 줄이고 테스트 세트 정확도를 높일 수 있습니다.tree = DecisionTreeClassifier(max_depth=4, random_state=0)tree.fit(X_train, y_train)print(\"훈련 세트 정확도: {:.3f}\".format(tree.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(tree.score(X_test, y_test)))훈련 세트 정확도: 0.988테스트 세트 정확도: 0.951결정 트리 분석 결정 트리를 생성하고 시각화하기 위해서는 다음과 같은 모듈이 필요합니다.# 트리 모델 생성from sklearn.tree import DecisionTreeClassifier # 트리의 시각화_1from sklearn.tree import export graphviz # 트리의 시각화_2 (.dot 파일을 만들지 않아도 가능)from sklearn.tree import plot_tree 그래프를 시각화하는 코드는 다음과 같이 쓸 수 있습니다.# graphviz 이용from sklearn.tree import export_graphvizexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"악성\", \"양성\"],                feature_names=cancer.feature_names, impurity=False, filled=True)#plot_treefrom sklearn.tree import plot_treeplot_tree(tree, class_names=[\"악성\", \"양성\"], feature_names=cancer.feature_names,         impurity=False, filled=True, rounded=True, fontsize=4)filled=True를 넣어주면 다음과 같이 색상이 들어가는 트리 모델을 얻을 수 있습니다.트리의 특성 중요도tree.feature_importane를 통해 특성 중요도를 알 수 있습니다.특성 중요도는 0부터 1 사이에 존재하는데 0은 전혀 사용되지 않은 특성, 1은 완벽하게 타깃 클래스를 예측한 특성을 의미합니다. 특성 중요도가 낮다는 유용하지 않다가 아닌 모델이 만들어질 때 특성을 선택하지 않았거나 특성과 중복되는 정보가 있다는 것을 의미합니다.전체 합은 1이 되고 따라서 특성중요도는 ‘이 모델이 만들어지는데 어떤 특성의 비율이 높은가?’ 정도의 해석이라고 생각하면 될 것 같습니다.Worst_radius만 보고 ‘반지름이 크면 양성이다?’ 를 알 수 없는 것처럼 특성 중요도는 어떤 클래스를 지지하는지 알려주지 않습니다.결정 트리의 회귀도 분류와 비슷하게 적용됩니다.단, 결정 트리를 회귀 모델로 사용하게 되면 훈련 데이터 범위 밖의 정보가 없어서 그 부분에 대한 예측이 불가능하게 됩니다.다음 모델은 트리 복잡도에 제한을 두지않아서 훈련 데이터는 완벽하게 예측하지만 데이터 범위 밖으로 나가면 마지막 포인트로 예측값을 출력합니다.따라서 트리 모델은 가격의 등락과 같은 예측을 할 때는 좋은 예측 모델을 만들 수 있지만 시계열 데이터에서는 데이터가 가진 시간 범위 밖의 예측은 안되기 때문에 잘 맞지 않습니다.장단점과 매개변수장점  해석력이 높습니다.데이터의 스케일에 구애받지 않습니다. 정규화나 표준화 같은 전처리 불필요합니다.특성의 스케일이 다르거나 이진특성, 연속적인 특성이 혼합되어도 잘 작동합니다.단점  과대적합되는 경향이 있어 일반화 성능이 좋지 않습니다.  축 평행을 구분하여 일부 관계에서 모델링이 어려움이 있습니다. 훈련 데이터에 대한 약간의 변경은 전체 결정논리에 큰 변화를 야기하여 샘플에 민감합니다.            매개변수      설명                  min_samples_split      - 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용  - Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가              min_samples_leaf      - 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수- min_samples_split과 함께 과적합 제어 용도- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요              max_features      - 최적의 분할을 위해 고려할 최대 feature 개수- Default = None → 데이터 세트의 모든 피처를 사용- int형으로 지정 →피처 갯수 / float형으로 지정 →비중- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정              max_depth      - 트리의 최대 깊이- default = None→ 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요              max_leaf_nodes      리프노드의 최대 개수      여기서 max_depth, max_leaf_nodes,min_samples_leaf 중 하나만 지정해도 과대적합을 막는데 충분한 역할을 합니다.결정 트리의 앙상블Ensemble앙상블은 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법입니다.책에서는 결정 트리의 앙상블로 한정하고 가장 많이 쓰이는 랜덤포레스트나 부스팅 모델은 트리 기반 모델이지만 앙상블은 다른 분류 모델을 결합하여 사용할 수도 있습니다.  Voting – 서로 다른 알고리즘을 가진 분류기를 결합  Bagging – 각각의 분류기는 모두 같은 유형의 알고리즘 기반, 모델을 다양하게 만들기 위해 데이터를 재구성 (랜덤포레스트)  Boosting – 맞추기 어려운 데이터에 대해 좀 더 가중치를 두어 학습 (Adaboost, Gradient Boosting)  Stacking – 모델의 output 값을 새로운 독립변수로 사용앙상블의 조건입니다.랜덤 포레스트랜덤 포레스트는 조금씩 다른 결정 트리의 묶음입니다.      데이터의 일부에 과대적합되는 경향을 이용하여 서로 다른 방향으로 과대적합된 트리를 많이 만들어 그 결과를 평균냄으로써 예측 성능은 유지되면서 결과적으론 과대적합이 줄어드는 아이디어에 기초합니다.결정 트리를 많이 만들면서 각 트리는 타깃 예측을 잘 해야 하고 다른 트리와 구별되어야 합니다.따라서 무작위성을 주입하는데 트리를 만들 때 사용하는 데이터 포인트를 무작위로 선택하거나 분할 테스트에서 특성을 무작위로 선택하는 방법을 이용합니다.랜덤 포레스트 구축from sklearn.ensemble import RandomForestClassifier (or RandomForestRegressor)n_estimators로 생성할 트리의 개수를 정합니다.부트스트랩 샘플은 n_samples개의 데이터 포인트 중에서 n_samples 횟수만큼 무작위로 중복 가능하게 반복 추출하는 것을 의미합니다.따라서 데이터 셋이 원래 크기와 같지만 누락되거나 중복되는 데이터가 만들어집니다.각 노드에서 전체 특성을 대상으로 최선의 테스트를 찾는 것이 아닌 알고리즘이 각 노드에서 후보 특성을 무작위로 선택한 후 이 후보들 중에서 최선의 테스트를 찾습니다. (max_features) 부트스트랩 샘플링을 통해 트리가 조금씩 다른 데이터셋을 이용해 만들어지도록 합니다.각 노드에서 특성의 일부만 사용하기 때문에 트리의 각 분기는 각기 다른 특성 부분 집합을 사용됩니다.max_features=n_features는 특성 선택에 무작위성이 들어가지 않습니다. (부트스트랩 샘플링에는 무작위성 그대로 입니다.)max_feature=1 트리의 분기는 테스트할 특성을 고를 필요가 없게 되고 무작위로 선택한 특성의 임계값 찾기만 하면 됩니다.max_feature이 커지면 랜덤 포레스트 트리들은 매우 비슷하고 가장 두드러진 특성으로 데이터에 잘 맞춰질 것이고 작으면 트리들은 서로 많이 달라지고 각 트리는 데이터에 맞추기 위해 깊이가 깊어지게 됩니다.랜덤 포레스트 예측의 경우 알고리즘이 모델에 있는 모든 트리의 예측을 만듭니다.회귀의 경우 이 예측들을 평균하여 최종 예측을 만듭니다.분류의 경우 약한 투표 전략을 사용합니다.약한 투표 전략은 각 알고리즘이 가능성 있는 출력 레이블의 확률을 제공하고 예측한 확률을 평균으로 가장 높은 확률을 가진 클래스가 예측값이 됩니다.참고로 강한 투표 전략은 다수의 분류기가 결정한 예측값을 최대로 하는 것을 말합니다.랜덤 포레스트 분석랜덤 포레스트 훈련 모델from sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import make_moonsX, y = make_moons(n_samples=100, noise=0.25, random_state=3)X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)forest = RandomForestClassifier(n_estimators=5, random_state=2)forest.fit(X_train, y_train)부트스트랩 샘플링 때문에 한쪽 트리에 나타나는 훈련 포인트가 다른 트리에는 포함되지 않을 수 있어 각 트리는 불완전하지만 랜덤포레스트의 결과는 좋은 결정경계를 보여줍니다.단일 트리와 다르게 랜덤 포레스트에서 가장 특성 중요도가 높은 특성은 worst perimeter입니다.랜덤 포레스트에서 더 많은 특성이 0 이상의 중요도를 갖고 따라서 더 넓은 시각으로 데이터를 바라볼 수 있습니다.X_train, X_test, y_train, y_test = train_test_split(    cancer.data, cancer.target, random_state=0)forest = RandomForestClassifier(n_estimators=100, random_state=0)forest.fit(X_train, y_train)print(\"훈련 세트 정확도: {:.3f}\".format(forest.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(forest.score(X_test, y_test)))훈련 세트 정확도: 1.000테스트 세트 정확도: 0.972랜덤 포레스트에선 훈련 데이터 정확도가 100% 이지만 단일 트리에 비해서 테스트 정확도가 상승한 것을 확인 할 수 있습니다.장단점과 매개변수장점  매개변수 튜닝을 많이 하지 않습니다. 데이터의 스케일에 구애받지 않습니다. 단일 트리의 단점을 보완하고 장점을 그대로 가지고 있습니다.단점  랜덤 포레스트의 트리는 특성의 일부만 사용하므로 결정 트리보다 더 깊어지는 경향이 있습니다.다른 random_state를 지정하면 전혀 다른 모델이 만들어집니다.텍스트 데이터와 같은 차원이 높고 희소한 데이터에 잘 작동하지 않습니다.선형 모델에 비해 많은 메모리를 사용하며 훈련과 예측이 느림            매개변수      설명                  n_estimators      - 결정트리의 갯수를 지정- Default = 10 (0.22버전부터 100)- 무작정 트리 갯수를 늘리면 성능 좋아지는 것 대비 시간이 걸릴 수 있음              min_samples_split      - 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용- Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가              min_samples_leaf      - 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수- min_samples_split과 함께 과적합 제어 용도- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요              max_features      - 최적의 분할을 위해 고려할 최대 feature 개수- Default = ‘auto’ (결정트리에서는 default가 none이었음)- int형으로 지정 →피처 갯수 / float형으로 지정 →비중- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정 (RandomForestClassifier-sqrt(n_feature), RandomForestRegressor-n_feature)- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정              max_depth      - 트리의 최대 깊이- default = None→ 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요              max_leaf_nodes      리프노드의 최대 개수      N_estimatiors는 클수록 좋고 max_features와 max_depth와 같은 사전 가지치기 옵션은 단일 트리와 같이 주어집니다.그레이디언트 부스팅 회귀 트리이름은 회귀이지만 회귀와 분류 모두 사용됩니다. (GradientBoostingClassifier, GradientBoostingRegressor)그레이디언트 부스팅은 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듭니다.따라서 기본적으로 무작위성이 없습니다.대신 강력한 사전 가지치기가 사용되고 깊지 않은 트리를 사용합니다.각 트리는 데이터의 일부에 대해서만 예측을 잘 수행하여 트리가 많이 추가될수록 성능이 향상됩니다.이때 손실 함수를 정의하고 경사 하강법을 사용해서 다음 값을 보정합니다.random_state=0 만 입력했을 때from sklearn.ensemble import GradientBoostingClassifier​X_train, X_test, y_train, y_test = train_test_split(    cancer.data, cancer.target, random_state=0)​gbrt = GradientBoostingClassifier(random_state=0)gbrt.fit(X_train, y_train)​print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_test, y_test)))훈련 세트 정확도: 1.000테스트 세트 정확도: 0.965random_state=0, max_depth=1 을 입력했을 때gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)gbrt.fit(X_train, y_train)​print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_test, y_test)))훈련 세트 정확도: 0.991테스트 세트 정확도: 0.972random_state=0, learning_rate=0.01을 입력했을 때gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)gbrt.fit(X_train, y_train)print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_test, y_test)))훈련 세트 정확도: 0.988테스트 세트 정확도: 0.965훈련 세트의 정확도가 100%로 과대적합이 된 모델은 max_depth나 learning_rate로 보완할 수 있습니다.Random_state는 고정시켜야 같은 모델이 나오는 것을 볼 수 있습니다.Learning_rate는 오차에 곱을 해서 예측값을 업데이트 해주는 값입니다.랜덤 포레스트에 비해 그레이디언트 부스팅은 특성들이 더 적습니다.안정성에서는 랜덤 포레스트가 더 좋지만 그레이디언트가 성능적으로 더 좋은 모습을 보여줄수 있습니다.참고XGBoost  XGBoost는 데이터 별 오류를 다음 round 학습에 반영 시킨다는 측면에서 기존 Gradient Boosting과 큰 차이는 없음Gradient Boosting과 달리 학습을 위한 목적식(loss function)에 Regularization term이 축가되어 모델이 과적합 되는 것을 방지해줌Regularization term을 통해 XGBoost는 복잡한 모델에 패널티를 부여함LighGBM  XGBoost와 다르게 lear-wise loss 사용 (loss를 더 줄일 수 있음)XGBoost 대비 2배 이상 빠른 속도 (동일 파라미터 기준)과대적합에 민감하여, 대량의 학습데이터를 필요로 함장단점과 매개변수장점  이진 특성이나 연속적인 특성에도 잘 작동합니다. 데이터의 스케일에 구애받지 않습니다.단점  매개변수의 조정이 필수입니다.휸련시간이 깁니다.  차원이 높고 희소한 데이터에 잘 작동하지 않습니다.N_estimators가 클수록 랜덤 포레스트는 좋았지만 그래이디언트 부스팅에서는 과대적합될 가능성이 높아집니다.N_estimator을 정하고 난 뒤에 learning_rate를 정하게 되는데 learning_rate를 낮추면 비슷한 복잡도의 모델을 만들기 위해 더 많은 트리를 추가해야합니다.            매개변수      설명                  n_estimators      - 트리의 개수를 지정- 커지면 모델이 복잡해지고 과대적합 가능성 높아짐              learning_rate      - 관례상 n_estimators를 맞추고 learning_rate를 찾음 - 이전 트리의 오차를 보정하는 정도              n_iter_no_change / validation_fraction      -조기 종료를 위한 매개변수 (default값: n_iter_no_change =None (조기 종료 x), validation_fraction=0.1) - validation_fraction 비율만큼 검증 데이터로 사용하여 n_iter_no_change 만큼 반복하여 향상되지 않으면 훈련 종료              max_depth / max_leaf_nodes      -각 트리의 복잡도를 낮춤 - max_depth는 보통 매우 작게 설정하며 트리의 깊이가 5보다 깊어지지 않게 함      배깅, 엑스트라 트리, 에이다부스트Baggingfrom sklearn.ensemble import BaggingClassifier배깅은 중복을 허용한 랜덤샘플링으로 만든 훈련 세트를 사용해 분류기를 각기 다르게 학습합니다.랜덤포레스트는 배깅의 일종이지만 설명변수도 무작위로 선택하는 것이 차이가 있습니다.predict_proba() 지원하면 메서드를 통해 확률값을 평균하여 예측을 수행합니다. (지원하지 않는다면 가장 빈도가 높은 클래스 레이블)oob_score=True로 지정하면 매개변수는 부트스트래핑에 포함되지 않은 샘플로 훈련된 모델을 평가할 수 있습니다. (OOB 오차, default=False)from sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import BaggingClassifierbagging = BaggingClassifier(LogisticRegression(), n_estimators=100, oob_score=True, n_jobs=-1, random_state=42)bagging.fit(Xc_train, yc_train)print(\"훈련 세트 정확도: {:.3f}\".format(bagging.score(Xc_train, yc_train)))print(\"테스트 세트 정확도: {:.3f}\".format(bagging.score(Xc_test, yc_test)))print(\"OOB 샘플의 정확도: {:.3f}\".format(bagging.oob_score_))훈련 세트 정확도: 0.953테스트 세트 정확도: 0.951OOB 샘플의 정확도: 0.946배깅은 랜덤포레스트와 달리 max_samples에 부트스트랩 샘플의 크기를 정할 수 있습니다. 또한 로지스틱 회귀가 들어갈 수도 있고 결정 트리가 들어갈 수도 있습니다.Extra Tree후보 특성을 무작위로 분할한 다음 최적의 분할을 찾습니다.엑스트라 트리도 랜덤 포레스트와 비슷하지만 splitter=‘random’을 사용합니다. 랜덤 포레스트는 splitter=‘best’가 고정입니다.Splitter=‘best’의 의미는 모든 변수의 정보 이득을 계산하고 그중 가장 설명력이 높은 변수를 선택하는 것입니다.또한 부트스트랩 샘플링을 적용하지 않습니다. 무작위성을 증가시키면 모델 편향은 늘어나지만 분산이 감소하는 모습을 보입니다.개별 트리는 매우 복잡하지만 결정 경계는 안정적입니다.계산 비용은 위 splitter에서의 feature의 차이 때문에 랜덤 포레스트보다 적지만  일반화 성능을 높이려면 많은 트리를 만들어야합니다.from sklearn.ensemble import ExtraTreesClassifierxtree = ExtraTreesClassifier(n_estimators=100, n_jobs=-1, random_state=0)xtree.fit(Xc_train, yc_train)print(\"훈련 세트 정확도: {:.3f}\".format(xtree.score(Xc_train, yc_train)))print(\"테스트 세트 정확도: {:.3f}\".format(xtree.score(Xc_test, yc_test))훈련 세트 정확도: 1.000테스트 세트 정확도: 0.972Adaptive Boosting이전의 모델이 잘못 분류한 샘플에 가중치를 높여서 다음 모델을 훈련합니다.훈련된 각 모델은 성능에 따라 가중치 부여합니다.예측을 만들 때는 모델이 예측한 레이블을 기준으로 모델의 가중치를 합산하여 가장 높은 값을 가진 레이블을 선택합니다.AdaBoostClassifier은 기본값으로 DecisionTreeClassifier(max_depth=1)를 갖습니다.AdaBoostRegressor은 기본값으로 DecisionTreeRegressor(max_depth=3)을 갖습니다. (base_estimator을 이용하여 다른 모델 지정 가능)에이다 부스팅의 원리와 수식예측정확도와 가중치의 곱의 합이 되어 높은 정확도를 만들게 됩니다.from sklearn.ensemble import AdaBoostClassifierada = AdaBoostClassifier(n_estimators=100, random_state=42)ada.fit(Xc_train, yc_train)print(\"훈련 세트 정확도: {:.3f}\".format(ada.score(Xc_train, yc_train)))print(\"테스트 세트 정확도: {:.3f}\".format(ada.score(Xc_test, yc_test)))훈련 세트 정확도: 1.000테스트 세트 정확도: 0.986커널 서포트 벡터 머신커널 서포트 벡터 머신은 보통 SVM이라고 한다.입력 데이터에서 단순한 초평면으로 정의되지 않는 더 복잡한 모델을 만들 수 있도록 확장한 것이다.분류와 회귀 모두 사용 가능하다. (SVC는 분류, SVR은 회귀)from sklearn.svm import LinearSVC #선형 모델LinearSVC(max_iter=)from sklearn.svm import SVCSVC(kernel='',C=,gamma=) # kernel, C, gamma 파라미터 존재선형 모델과 비선형 특성선형 모델은 직선으로만 데이터 포인트를 나눌 수 있어 밑에 같은 데이터는 잘 들어맞지 않는다.SVM 모델은 3차원에서 2차원으로 투영해본다면 더이상 선형 모델이 아니다.커널 기법커널 기법은 실제로 데이터를 확장하지 않고 확장된 특성에 대한 데이터 포인트들의 거리를 계산한다.$ (특성1)^2 * (특성2)^5 $하는 다항식 커널이 있고 가우시안 커널로 불리우는 RBF 커널이 있다.가우시안 커널은 차원이 무한한 특성 공간에 매핑하는 것이다.모든 차수의 모든 다항식을 고려하지만 특성의 중요도는 고차항이 될수록 줄어든다.SVM 이해하기두 클래스 사이에 경계한 데이터 포인트들을 서포트 벡터라고 한다.새로운 데이터 포인트에 대해 예측하려면 각 서포트 벡터와의 거리를 측정한다.서포트 벡터의 중요도는 훈련 과정에서 학습하는데 dual_coef_ 속성에 저장된다.가우시안 커널 공식 사진가우시안 커널에 의해 계산되며 $ X_1, X_2 $는 데이터 포인트이며 $ ||X_1 - X_2|| $는 유클리디안 거리이고 $Γ$ 은 가우시안 커널의 폭을 제어하는 매개변수이다.SVM 매개변수 튜닝$Γ$는 가우시안 커널 폭의 역수에 해당하는데 하나의 훈련 샘플이 미치는 영향의 범위를 결정한다.(1~0 사이의 범위이다.)작은 값은 넓은 영역을 뜻하고, 큰 값은 영향이 미치는 범위가 제한적이다.즉 커널의 반경이 클수록 훈련 샘플의 영향 범위도 커진다.작은 $Γ$ 값은 모델의 복잡도를 낮출 수 있다.C 매개 변수는 규제 매개변수이다. dual_coef_값을 제한합니다.작은 C는 매우 제약이 큰 모델을 만들고 각 데이터 포인트의 영향력이 작다.C를 증가시키면 이 포인트들이 영향을 크게 줘서 결정 경계를 휘게 만든다.SVM을 위한 데이터 전처리커널 SVM에서는 데이터셋의 특성 자릿수가 완전히 다르면 영향을 크게 미친다.따라서 특성 값을 평균이 0이고 단위 분산이 되도록 하거나, 0과 1 사이로 맞추는 방법을 많이 사용한다.(StandardScaler와 MinMaxScalar)장단점과 매개변수SVM은 저차원과 고차원의 데이터에 모두 잘 작동하지만 샘플이 많으면 잘 맞지 않는다.또한 전처리와 매개변수 설정에 신경을 많이 써야하는데 그래서 랜덤 포레스트나 그레이디언트 부스팅과 같은 전처리가 거의 필요 없는 트리 기반 모델이 선호된다.SVM은 분석도 어려워서 예측이 어떻게 결정되었는지 설명하기가 난해하다.하지만 모든 특성이 비슷한 단위이고 스케일이 비슷하다면 시도해볼 만하다.중요한 매개변수는 C이고 어떤 커널을 사용할지와 각 커널에 따른 매개변수이다.RBF는 $Γ$ 매개변수를 갖지만 다른 커널 종류도 많다.신경망 (딥러닝)다층 퍼셉트론은(MLP)는 간단하게 분류와 회귀에서 쓰일 수 있다.신경망 모델MLP는 여러 단계를 거처 결정을 만들어내는 선형 모델의 일반화된 모습이다.선형 회귀 모델의 예측 공식 사진$\\hat Y $는 x[0]에서 x[p]까지의 입력특성과 학습된 계수의 가중치의 합이다.퍼셉트론 사진왼쪽 노드는 입력 특성을 나타내며 연결선은 학습된 계수를 표현하고 오른쪽 노드는 입력의 가중치 합, 즉 출력을 나타낸다.MLP는 가중치 합을 만드는 과정이 여러 번 반복되며 먼저 중간 단계를 구성하는 은닉 유닛을 계산하고 이를 이용하여 최종 결과를 산출하기 위해 다시 가중치 합을 계산한다.다중 퍼셉트론 사진각 은닉 유닛의 가중치 합을 계산한 후 결과에 비선형 함수인 렐루나 하이퍼볼릭 탄젠트, 시그모이드 함수를 적용합니다.회귀 분석 사진w는 입력 x와 은닉층 h 사이의 가중치이고, v는 은닉층 h와 출력 $\\hat Y$ 사이의 가중치입니다.w와 v는 훈련 데이터에서 학습하고 x는 입력 특성이며 $ \\hat Y $는 계산된 출력, h는 중간 계산값 입니다.신경망 튜닝더 복잡도가 낮은 모델을 만들고 싶다면 hidden_layer_size를 통해 은닉 유닛의 개수를 줄인다.은닉 유닛을 추가하거나, 은닉층을 추가하거나 활성화함수를 바꾸면 더 매끄러운 결정 경계를 얻을 수도 있다.선형 분류와 리지 회귀 처럼 L2 페널티를 사용해서 가중치를 0에 가깝게 감소시킬 수도 있다.(default는 매우 낮다)신경망에서는 학습을 시작하기 전에 가중치를 무작위로 설정하며 이 무작위한 초기화가 모델의 학습에 영향을 준다.따라서 같은 매개변수를 사용하더라도 초깃값이 다르면 모델이 많이 달라질 수 있다.신경망도 입력 특성이 평균은 0 분산이 1이 되도록 변형하는 것이 좋다.은닉 유닛에서 작은 가중치를 가진 특성은 모델에 덜 중요하다고 추론할 수 있다.from sklearn.neural_network import MLPClassifier # MLP분류MLPClassifier(solver='',activation='',random_state=,hidden_layer_sizes=[,],max_itter=,alpha=) #solver에 최적화 알고리즘,activation에 활성화 함수 ,hidden_layer_size로 은닉 유닛의 개수 설정(default=100),max_itter은 반복 횟수,alpha는 L2 페널티장단점과 매개변수머신러닝 알고리즘을 뛰어넘는 성능을 보일 수 있지만 학습이 오래걸리고 데이터 전처리를 주의해서 해야한다.모든 특성이 같은 의미를 가지면 SVM, 다른 종류의 특성이라면 트리 기반 메딜이 더 잘 작동할 수 있다.신경망의 복잡도 추정가장 중요한 매개변수는 은닉층의 개수와 각 은닉층의 유닛 수이다.복잡도에 관해 연관된 측정치는 학습된 가중치 또는 계수의 수이다.특성이 100개 은닉 유닛 100개인 이진 분류라면 입력층과 첫 번째 은닉층 사이에는 편향을 포함하여 $ 100 * 100 + 100 = 10100 $개의 가중치가 있습니다.은닉층과 출력층 사이에 $ 100 * 1 + 1 = 101 $개의 가중치가 더 있어 가중치는 10201개 이다.이렇게 가중치는 은닉층을 추가할수록 훨씬 커지게 된다.매개변수를 조정하는 일반적인 방법은 충분히 과대적합되어 문제를 해결할만한 큰 모델을 만든 뒤 훈련 데이터가 충분히 학습될 수 있다고 생각되면 신경망 구조를 줄이거나 규제 강화를 위해 alpha 값을 증가시켜 일반화 성능을 향상시킨다.층의 개수, 층당 유닛 개수, 규제, 비선형성으로 모델 구성을 할 수 있으며, solver 매개변수를 통해서 학습시키는 방법을 지정할 수 있다.solver의 경우 기본값은 adam이고 데이터 스케일에 민감하다.lbfgs는 안정적이지만 규모가 크면 시간이 오래 걸린다sgd는 momentum과 nesterovs_momentom의 영향을 받는데 다른 여러 매개변수와 함께 튜닝하여 최선의 결과를 만들 수 있다.분류 예측의 불확실성 추정decision_function과 predict_proba로 추정 할 수 있다.결정 함수decision_function의 반환값의 크기는 (n_samples,)이며각 샘플이 하나의 실수 값을 반환한다.모델이 데이터 포인트가 양성 클래스인 클래스 1에 속한다고 믿는 정도이다.즉, 음수값은 다른 클래스에 속함을 의미한다.값의 범위는 데이터와 모델 파라미터에 따라 달라지게 된다.예측 확률predict_proba의 출력은 각 클래스에 대한 확률이고 이진 분류에서 이 값의 크기는 항상 (n_samples,2)이다.두 클래스의 확률 합은 1이므로 두 클래스 중 하나는 50% 이상의 확신을 가질 것이고 그 클래스가 예측값이 된다.데이터에 있는 불확실성이 얼마나 이 값에 잘 반영되는지는 모델과 매개변수 설정에 달렸다.그래서 과대적합된 모델 혹은 잘못된 예측도 예측의 확신이 강한 편이다.복잡도가 낮을 수록 예측에 불확실성이 더 많다.불확실성과 모델의 정확도가 동등하면 이 모델이 보정되었다고 한다.다중 분류에서의 불확실성다중 분류에서도 decision_funcion과 predict_proba를 사용할 수 있다.decision_function에서는 (n_samples, n_classes)가 결과값이 된다.글 클래스에 대한 확신 점수를 담고 그 수치가 크면 그 클래스일 가능성이 크다.데이터 포인트마다 점수들에서 가장 큰 값을 찾아 예측 결과를 재현할 수 있다.predict_proba는 (n_samples,n_classes)가 출력값이 된다.마찬가지로 각 데이터 포인트에서 클래스 확률의 합은 1이다.argmax 함수를 적용해서 예측 결과를 재현할 수 있지만 클래스가 문자열이거나 정수형을 사용하지만 연속적이지 않고 0부터 시작하지 않을 수 있다.따라서 predict 결과와 decision_function, predict_proba의 결과를 비교하기 위해서는 분류기의 classes_ 속성을 사용해 클래스의 실제 이름을 얻어야 한다.",
        "url": "/study-ML2"
    }
    ,
    
    "programming-baekjoon4": {
        "title": "백준 (4) &lt;br&gt; (1157, 1546, 2577, 2675, 2908, &lt;br&gt; 1018, 1436, 1259, 7568, 10250)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)    백준 (9) 우선순위 큐 알고리즘 (1927,11279,11286,1715,11766)백준 10문제를 풀어보았다.중복되는 문제도 있습니다1157번 단어공부https://www.acmicpc.net/problem/1157upper을 이용한 대문자 받기count로 dictionary형태로 word 개수 세기개수가 중복되는 단어들이 있으므로 max_list에서 따로 추출하기조건에 맞게 최댓값이 하나면 알파벳을, 아니면 물음표를 출력하기word=input().upper()count={}for i in word:    if i not in count:        count[i]=0    count[i]+=1max_list=[j for j,k in count.items() if max(count.values())==k]if len(max_list)==1:    print(max_list[0])else:    print('?')Mississipi?1546번 평균https://www.acmicpc.net/problem/1546map 함수를 통해서 점수 입력받기new_mean()이라는 함수는 문제에서 나온 $점수/최대점수*100$ 이다.map함수를 통해 new_mean함수를 score에 모두 적용해주고 sum을 통해 총합을 구한 뒤,count로 나눠주면 된다.count=int(input())score=list(map(int,input().split())) # 점수 입력받기340 80 60max_score=max(score) # 점수 최댓값 찾기def new_mean(x): #최댓값과의 비율로 새로운 점수 만드는 함수    return x/max_score*100print(sum(map(new_mean,score))/count) #평균을 구하는 식75.02577번 숫자의 개수https://www.acmicpc.net/problem/2577입력받는 숫자가 3개로 한정되어있으니까 for문을 통해서 세 숫자의 곱을 구했다.count함수의 인덱스 0-9까지를 0-9숫자가 나왔을 때 하나씩 늘려주는 방법을 택했다.mul=1for i in range(3): # 세 숫자의 곱 구하기    a=int(input())    mul=mul*aprint(mul)15026642717037300count=[0]*10 #0-9의 개수가 들어갈 listfor j in str(mul): #str(mul)로 해줘야 for문이 성립된다.    for k in range(10): #0-9까지의 숫자를 확인하는 for문        if int(j)==k:            count[k]=count[k]+1 #숫자가 등장했을때 1 늘려주기for l in count:    print(l)31020002002675번 문자열 반복https://www.acmicpc.net/problem/2675각 글자마다 R번 반복해서 출력해주는 문제이다.case를 통해서 몇 번 반복할지를 결정해준다.word_list에 각 단어마다 R번씩 반복하여 append해준다.join으로 리스트에 있는 단어들을 문장으로 만들어준다.case=int(input()) # 시도할 횟수for i in range(case):     R,P=input().split() # R,P입력받기    word_list=[] #반복한 뒤 append해줄 list    for j in P: #P에 있는 글자 순서대로 반복해주기        for k in range(int(R)): #R을 str로 입력받아서 int로 변경해줘야함            word_list.append(j) #반복된 글자를 append    print(''.join(word_list)) #list안에 글자들 붙여서 출력해주기23 ABCAAABBBCCC5 /HTP/////HHHHHTTTTTPPPPP2908번 상수https://www.acmicpc.net/problem/2908list(A)로 숫자들을 리스트화 해준다.[::-1]로 거꾸로 뒤집어준다. A로 다시 저장하기 싫으면 reverse() 함수를 사용하면 된다.join을 통해 숫자로 바꿔주고 int 형태로 바꿔준 뒤, max를 통해 최댓값을 찾는다.A,B=input().split() # A,B 숫자 거꾸로 만들기A=list(A)[::-1] B=list(B)[::-1]# A,B 다시 숫자로 만든 뒤 대소비교A=int(''.join(A))B=int(''.join(B))print(max(A,B))734 8934371018번 체스판 다시 칠하기https://www.acmicpc.net/problem/1018N,M 크기를  받고 보드 만들기nXm형태의 위치를 파악하기 쉽게 리스트 형태로 받았다.n, m=map(int,input().split())if 8&lt;=n&lt;=50 and 8&lt;=m&lt;=50:    board = [input() for i in range(n)]10 13BBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBWWWWWWWWWWBWBWWWWWWWWWWBWB위치가 짝수일 때와 홀수일 때로 나눠서  W, B가 아닐 때마다 점수를 추가해준 다음 가장 최소가 되는 값만 찾아내면 된다.따라서 n * m의 보드에서 가능한 경우의 수는 n-7 * m-7이다. ex)10 13을 입력받을 경우 18가지.8 * 8로 잘라주기 위해서 k와 l을 (i,i+8), (j,j+8)로 한정짓는다.k+l이 홀수일 경우와 짝수일 경우, W로 시작할 경우와 B로 시작할 경우를 나눠서 모든 경우의 수를 반복문으로 확인해준다.마지막으로 total_score에 들어있는 값들 중 최솟값을 구해준다.total_score=[]# 보드에서 경우의 수 나누어주기for i in range(n-7):    for j in range(m-7):        count_w=0 #w가 아닐때        count_b=0 #b가 아닐때        #8*8 크기로 잘라주기        for k in range(i,i+8):             for l in range (j,j+8):                #각 경우의 수마다 비교해서 점수 추가하기                if (k+l)%2==0:                    if board[k][l]!='W':                                                    count_w=count_w+1                    if board[k][l]!='B':                        count_b=count_b+1                else:                    if board[k][l]!='W':                        count_b=count_b+1                    if board[k][l]!='B':                                                    count_w=count_w+1        # 점수들 한 list에 모아주기        total_score.append(count_w)        total_score.append(count_b)print(min(total_score)) #최솟값 출력121436번 영화감독 숌https://www.acmicpc.net/problem/1436666이 적어도 3개이상 연속으로 들어가는 수를 만든다처음에 문제를 풀 때 중간에 666이 3개 이상 들어가는 경우를 제외해서 틀렸다.list666에 가장 작은 숫자인 666부터 ‘666’이 문자열로 들어가있는 숫자들을 확인해서 추가하였다.입력받은 숫자가 list666의 길이보다 크면 계속 추가해주었고 list666[num-1]을 통하여 값을 출력해준다.num=int(input())list666=[]i=666while len(list666)&lt;num:    if '666' in str(i):        list666.append(i)    i=i+1print(list666[num-1]) 326661259번 팰린드롬수https://www.acmicpc.net/problem/1259앞에서 읽어도 뒤에서 읽어도 같은 숫자 찾기0을 입력하면 반복문이 끝나게 while과 if를 이용하였다.입력받은 숫자는 위치를 찾기 편하게 문자형으로 입력받았다.입력받은 숫자의 길이/2 만큼의 반복문을 돌리면 반대쪽은 (숫자의 길이-i-1)로 대응된다.한가지 숫자라도 값이 다르면 False 값을 가지고 ‘no’를 출력하면 ‘yes’를 출력하는 것을 만들 때보다 길이가 짧아질 수 있다.while True:    word=input() # 숫자를 무한으로 입력받기 위해 while문 사용    quest=True # 한가지 입력값을 처리하고나서 True, False값을 True로 초기화    if word=='0': # 0을 입력하면 반복문 종료        break    else:        word_len=len(word)        for i in range(int((word_len)/2)): #단어 길이의 반만 확인하면 반대쪽 숫자와 대응된다.                if word[i]!=word[word_len-1-i]: #반대쪽 숫자와 대응하기 위해서 word_len-1-i 사용                    quest=False # 하나의 경우라도 False가 나오면 반복문 종료                    continue        if quest==False: # False가 나오면 바로 'no' 출력            print('no')        else: print('yes')121yes1231no12421yes07568번 덩치https://www.acmicpc.net/problem/7568처음에 문제를 풀 때 너무 복잡하게 생각해서 무게 따로, 키 따로 점수 매기고 sort해서 index로 출력하려고 했는데 예제 문제는 옳게 나오지만 다른 경우에서 틀렸었다.또 and 말고 &amp; 로 써서 한 번 더 틀렸는데 이건 bitwise 연산자라서 답이 다르게 나왔다.# r값 입력 받고 (무게, 키) 형태로 리스트 만들기count=int(input())human=[]for i in range(count):    weight,tall=input().split()    human.append((int(weight),int(tall)))print(human)555 18558 18388 18660 17546 155[(55, 185), (58, 183), (88, 186), (60, 175), (46, 155)]for j in human:    rank=1 # 등수는 1등 부터니까 1    for k in human:        if j[0]&lt;k[0] and j[1]&lt;k[1]: # 둘 다 k가 우세할 경우에만 rank에 1 추가-&gt;순위 하락            rank+=1    print(rank,end=' ')2 2 1 2 5 for m in human:    rank=1    for n in human:        if m[0]&lt;n[0] &amp; m[1]&lt;n[1]: # &amp;는 비트 연산자기 때문에 결과가 다르게 나온다.            rank+=1    print(rank,end=' ')3 1 1 1 1 10250번 ACM호텔https://www.acmicpc.net/problem/102501호가 우선시 된다면 H명씩 순서대로 채운다고 생각하면 편하다.따라서 층수는 N/H의 나머지가 되고 호수는 N/H의 몫+1이 된다.하지만 N이 H의 배수일 때, 층수가 최고층이 되기때문에 H를 대신 입력해준다.또한 호수도 N/H의 몫과 같아지기 때문에 +1 한 것을 다시 -1 해준다.또한 호수가 10보다 작을 때 앞에 0을 적어줘야한다.for i in range(int(input())):    H,W,N=map(int,input().split())    floor=N%H #나머지가 층수가 된다.    order=(N//H)+1 #몫을 정수로 받은 뒤 +1해주면 호수가 된다.    if floor==0: #N이 H의 배수일 때만 따로 구분해서 값을 준다.        floor=H        order=order-1    if order&lt;10: # 호수 앞에 0을 붙여준다.        print(str(floor)+'0'+str(order))    else:        print(str(floor)+str(order))23 2 33014 1 8402 ```",
        "url": "/programming-baekjoon4"
    }
    ,
    
    "programming-baekjoon3": {
        "title": "백준 (3) 문자열 알고리즘 &lt;br&gt;(11720, 8958, 1152, 10809, &lt;br&gt; 1157, 9012, 11718)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)    백준 (9) 우선순위 큐 알고리즘 (1927,11279,11286,1715,11766)문자열에 관련된 7문항을 풀어보았다.https://www.acmicpc.net/problemset?sort=ac_desc&amp;algo=15811720번 숫자의 합https://www.acmicpc.net/problem/11720공백없는 숫자들의 합구하기방법 1. for문을 이용해서 풀기n=int(input())m=input()554321range를 이용한 풀이 방법result=0for i in range(n):    result=int(m[i])+resultprint(result)15m을 읽어가면서 더해주는 방법result=0for i in m:    result=int(i)+resultprint(result)15방법 1. sum과 map을 이용해서 풀기print(sum(map(int,input())))54321158958 번 OX퀴즈https://www.acmicpc.net/problem/8958for문을 이용하여 입력받을 ox의 개수를 입력받고 for문 중첩을 이용하여 ox의 길이를 파악하고 if문으로 ox 여부를 확인하였다.ox의 연속성에 따른 점수변화를 num_score로 두고 total_score을 num_score의 합으로 설정하였다.num=int(input())for i in range(num):    ox=input()    total_score=0    num_score=0    for i in range(len(ox)):        if (ox[i]=='O') is True:            num_score=num_score+1        else:            num_score=0        total_score=total_score+num_score    print(total_score)5OOXXOXXOOO10OOXXOOXXOO9OXOXOXOXOXOXOX7OOOOOOOOOO55OOOOXOOOOXOOOOX301152번 단어의 개수https://www.acmicpc.net/problem/1152단어의 개수=공백의 위치마다 구분해줘서 입력받았다.sentence=list(map(str,input().split()))print(len(sentence))The Curious Case of Benjamin Button610809번 알파벳찾기https://www.acmicpc.net/problem/10809for문과 알파벳 list 선언s=list(map(str,input()))alpha=list('abcdefghijklmnopqrstuvwxyz')array=[-1 for i in range(len(alpha))]for i in range(len(s)):    if array[alpha.index(s[i])]==-1:        array[alpha.index(s[i])]=ifor j in array:    print(j,end=' ')baekjoon1 0 -1 -1 2 -1 -1 -1 -1 4 3 -1 -1 7 5 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 아스키코드 이용알파벳의 아스키코드는 (97,123)이다.s=input()alpha=list(range(97,123))for i in alpha:    print(s.find(chr(i)),end=' ')baekjoon1 0 -1 -1 2 -1 -1 -1 -1 4 3 -1 -1 7 5 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 1157번 단어 공부https://www.acmicpc.net/problem/1157upper을 이용한 대문자 받기count로 dictionary형태로 word 개수 세기개수가 중복되는 단어들이 있으므로 max_list에서 따로 추출하기조건에 맞게 최댓값이 하나면 알파벳을, 아니면 물음표를 출력하기word=input().upper()count={}for i in word:    if i not in count:        count[i]=0    count[i]+=1max_list=[j for j,k in count.items() if max(count.values())==k]if len(max_list)==1:    print(max_list[0])else:    print('?')Mississipi?9012번 괄호https://www.acmicpc.net/problem/9012 while문을 통해 ‘()’가 vps에 존재한다면 계속 replace를 통해 제거해주었다.만약 VPS 문장이었다면 문자열에 아무것도 남지않고 VPS 문장이 아니라면 어떤 문자라도 남았을 것이다.따라서 결과물의 길이로 YES와 NO를 구분지어줬다.num=int(input())for i in range(num):    vps=input()    while '()' in vps:        vps=vps.replace('()','')    if len(vps)==0:        print('YES')    else:        print('NO')6(())())NO(((()())()NO(()())((()))YES((()()(()))(((())))()NO()()()()(()()())()YES(()((())()(NO11718번 그대로 출력하기https://www.acmicpc.net/problem/11718while True로 반복문을 만들어주고 try~except문으로 오류가 발생했을때는 멈출수 있게 해준다.while True:    try:        print(input())    except:        breakOnline JudgeOnline Judge",
        "url": "/programming-baekjoon3"
    }
    ,
    
    "study-ml1": {
        "title": "머신러닝 정리 (1) &lt;br&gt; 지도학습 (1)",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학    머신러닝 정리 (6)-모델 평가와 성능 향상머신러닝 정리 (1) - 지도학습 (1)본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python지도학습 (1)  분류와 회귀  일반화, 과대적합, 과소적합          모델복잡도와 데이터셋 크기의 관계        지도 학습 알고리즘          예제에 사용할 데이터셋      k-nn 모델      선형 모델      Naive Bayes 분류기      분류와 회귀분류란? 미리 정의된 가능성 있는 여러 클래스 레이블 중 하나를 예측하는 것이다.이진 분류: 예, 아니오로 구분할 수 있다. ex) 이 이메일은 스팸인가요?다중 분류: 셋 이상의 클래스로 분류된다. ex) 붓꽃 데이터회귀란? 부동소수점수(수학으로 말하면 실수)를 예측하는 것이다.어떤 사람의 여러 조건들을 통해 연간 소득을 예측하는 것과 같은 문제이다.출력 값에 연속성이 있다면 회귀 문제 없다면 분류 문제이다.일반화, 과대적합, 과소적합모델이 처음 보는 데이터에 대해 정확히 예측할 수 있다면 훈련 세트(train_set)에서 테스트 세트(test_set)으로 일반화되었다고 한다.훈련 세트에 대해서 정확히 예측하고 테스트 세트에서도 정확히 예측하길 바란다.하지만 모델이 복잡하다면 훈련 세트에서만 정확한 모델이 될 수 있다.예를 들어 “내 주변 20대가 모두 아이폰을 쓰기 때문에 다른 20대도 모두 아이폰을 살 것이다.”라는 예측을 한다면 훈련 세트가 내 주변 20대가 될 것이고 테스트 세트가 다른 모든 20대가 될 것이다.이처럼 알고리즘이 새로운 데이터를 잘 처리하는지 측정하는 방법은 테스트 세트로 평가를 해야한다.이 때, 너무 복잡한 모델을 만들어 훈련 세트에 집중되어 테스트 세트에 일반화가 부족하다면 과대적합(overfitting)이라 한다.반대로 너무 간단한 모델이라 훈련 세트에도 잘 맞지 않다면 과소적합(underfitting)이라 한다.모델복잡도와 데이터셋 크기의 관계모델의 복잡도는 훈련 데이터 셋에 담긴 입력 데이터의 다양성과 관련이 있다.데이터셋에 데이터 포인트가 다양하면 과대적합 없이 복잡한 모델을 만들 수 있다.따라서 중복이거나 비슷한 데이터를 모으는 것은 도움이 되지않는다.위의 예를 생각해보면 내 주변 20대라는 특징말고 대학 동기, 친구라는 데이터를 얻더라도 모두 20대라는 범주안에 들어갈테니 불필요한 데이터라 할 수 있다.따라서 좋은 데이터를 많이 얻는 것이 좋다.지도 학습 알고리즘각 모델의 장단점과 어떤 데이터와 어울리는지, 매개변수와 옵션의 의미를 알아보도록 하자.scikit-learn 문서를 참고하면 더 자세한 정보를 얻을 수 있다.예제에 사용할 데이터셋forge 데이터셋은 인위적으로 만든 이진 분류 데이터셋이다.k-NN 모델k-NN 모델은 단순히 데이터셋을 분류하는 것이다.k개의 레이블 중에서 어느 쪽에 더 가까운 것인지 투표를 하여 결정한다고 생각하면 이해하기 편하다.k=1일 때는 가장 가까운 것이 빨간색이었다면, k=3일 때는 파란색 두 개와 빨간색 한 개가 가까울 수도 있다.그렇게 된다면 k=3일 때는 파란색으로 분류가 된다.KNeighborClassifier(n_neighbors)을 통해 분류 모델을 만들 수 있다.이를 통해 확인할 수 있는 것은 k의 값이 커질수록 보다 단순한 모델이 만들어질 수 있다는 것이다.하지만 반드시 k가 커진다고 좋은 모델은 아니다. 정확도가 낮아질 수 있기 때문이다.KNeighborsRegressor(n_neighbors)을 통해 회귀 모델 또한 만들 수 있다.회귀 모델에서도 k를 너무 적게 쓴다면 모든 데이터를 지나가고 불안정한 모델이 만들어진다.장단점과 매개변수가장 중요한 매개변수는 거리를 재는 방법과 k값이다.보통 거리를 재는 방법은 유클리디안 거리 방식을 사용한다.장점은 이해하기가 쉬운 모델이고 조정을 많이 하지않아도 좋은 성능을 발휘할 수 있다는 것이다.단점은 훈련 세트가 크면 예측이 느려지고 전처리 과정이 중요하다는 것이다.데이터가 특성이 많거나 대부분이 0인 데이터셋에서는 잘 작동하지 않는다.선형 모델선형 모델은 $y=w[0]*x[0]+b$와 같은 모델을 갖는 예측 함수이다.y는 예측값, w와 b는 모델이 학습할 파라미터, x는 데이터의 특성이다. 위 식은 특성이 하나인 데이터 셋의 선형 함수이다.선형 회귀(최소제곱법)선형 회귀는 예측과 훈련 세트에 있는 타깃 사이의 평균제곱오차(MSE)를 최소화하는 파라미터를 찾는 것이다.평균제곱오차는 예측값과 타깃값의 차이를 제곱하여 더한 후에 샘플의 개수로 나눈 것이다.매개변수가 없는 것이 장점이지만 복잡도를 제어할 방법도 없다.LinearRegression()리지 회귀리지 회귀도 예측 함수를 사용하지만 가중치의 절댓값을 가능한 작게 만드는 목적을 갖는다.이런 제약을 규제라고 하며 L2 규제라고 한다.리지는 덜 자유로운 모델이라 과대적합이 적다. 따라서 일반화에 도움이 된다.Ridge(alpha)라소리지의 대안으로 라소가 있다. L1 규제라고도 하며 완전히 제외하는 특성이 생긴다.일부 계수가 0이 되고 모델을 이해하기 쉬워지며 중요한 특성을 찾기 쉽ㄴ다.max_itter을 조절하여 과소적합을 줄인다.Lasso(alpha, max_itter())분류용 선형 모델이진 분류의 경우 선형 회귀와 비슷하지만 가중치 합을 그냥 사용하는 대신 예측값을 임계치 0과 비교한다.0보다 작으면 -1, 크면 1이라고 예측한다. 결정 경계를 선형 함수로 잡는다.LogisticRegression과 LinearSVC가 잘 알려져있는데 규제의 강도를 결정하는 매개변수 C를 주의해야한다.C가 높으면 훈련 세트에 최대로 맞추려 노력하고 낮추면 계수 벡터(w)가 0에 가까워지도록 만든다.로지스틱 회귀분석을 제외하면 다중 클래스를 대부분 지원하지 않는다.장단점과 매개변수회귀 모델에서는 alpha, LinearSVC와 LogisticRegression에서는 C가 중요하다.alpha가 클수록, C가 작을수록 모델이 단순해진다. 로그 스케일로 최적치를 정한다.L1, L2를 결정하는 것도 정해야한다. 중요한 특성이 적으면 L1, 그렇지 않으면 L2를 이용한다.선형 모델은 학습 속도와 예측 속도가 빠르다. solver=’sag’ 옵션을 이용하면 더 빨리 처리할 수 있다.아니면 SGDClassifier과 SGDRegressor을 이용할 수도 있다.또한 선형 모델은 예측이 어떻게 만들어지는지 비교적 이해하기 쉽다. 하지만 계수의 값이 명확하지가 않다.Naive Bayes 분류기나이브 베이즈 분류기는 선형 분류기보다 훈련 속도가 빠르지만 일반화 성능이 뒤쳐진다.개별적으로 파라미터를 학습하고 특성에서 클래스별 통계를 단순하게 취합한다.GaussianNB, BernoulliNB, MultinomialNB를 scikit-learn에서 구현되어있다.GaussianNB는 연속적인, BernoulliNB는 이진 데이터를, MultinomialNB는 카운트 데이터를 적용한다.BernoulliNB는 클래스의 특성중 0이 아닌 것이 몇 개인지 센다.MultinomialNB는 클래스별 특성의 평균을, GaussianNB는 클래스별로 각 특성의 표준편차와 평균을 저장한다.MultinomialNB와 GaussianNB는 선형 모델과 예측 공식이 갖지만 coef_는 기울기 w가 아니라 의미는 다르다.장단점과 매개변수MultinomialNB와 BernoulliNB는 모델 복잡도를 조절하는 alpha변수가 하나이다.alpha가 주어지면 알고리즘이 모든 특성에 양의 값을 가진 데이터 포인트를 alpha 개수만큼 추가한다.alpha가 크면 더 완만하고 덜 복잡한 모델이 나오지만 성능 변동은 비교적 크지 않다.GaussianNB는 고차원 데이터 셋을 사용한다. 다른 모델은 데이터를 카운트하는 데 사용된다.",
        "url": "/study-ML1"
    }
    ,
    
    "programming-kaggle1": {
        "title": "캐글 (1) &lt;br&gt; Simple Matplotlib &amp; Visualization Tips 공부하기",
            "author": "keonju",
            "category": "",
            "content": "kaggle 관련 글    캐글 (1) Simple Matplotlib &amp; Visualization Tips 공부하기    캐글 (2) 타이타닉 튜토리얼 1,2 공부하기    캐글 (3) 타이타닉 생존자 EDA 부터 분류까지    캐글 (4) 타이타닉 생존자 앙상블과 스태킹까지    캐글 (5) 메타 데이터를 이용한 데이터 관찰 및 준비Simple Matplotlib &amp; Visualization Tips 공부하기https://www.kaggle.com/subinium/simple-matplotlib-visualization-tips/notebook해당과정을 필사하였으며 영어로 되어있는 부분은 다시 정리해서 적었다.  Table of Contents          Settingdpifigsizetitle        Alignments          subplots, tight_layoutsubplot2gridadd_axesadd_gridspec        Colormap          divergingqualitativesequentialscientific        Text &amp; Annotate &amp; Patch          parametertext examplepatches example        Details &amp; Example          font weight, color, size, etcHorizontal and Vertical (barplot)Border(edge) color and thicknessMain Color &amp; Sub ColorTransparencySpan        MEME          xkcd style      import numpy as npimport matplotlib as mplimport matplotlib.pyplot as plt#matplotlib.pyplot 모듈의 각각의 함수를 사용해서 간편하게 그래프를 만들고 변화를 줄 수 있습니다.import matplotlib.gridspec as gridspec# gridspec 에는 Figure 내에서 격자 모양의 패턴으로여러 Axes 레이아웃하는 데 도움이되는 클래스가 포함되어 있습니다.import seaborn as sns# Seaborn은 Matplotlib에 기반하여 제작된 파이썬 데이터 시각화 모듈print(f\"Matplotlib Version :{mpl.__version__}\")print(f\"Seaborn Version :{sns.__version__}\")import pandas as pdnetflix_titles=pd.read_csv(\"netflix_titles.csv\")Matplotlib Version :3.3.2Seaborn Version :0.11.0Setting해상도 설정 matplotlib의 기본 해상도는 떨어지는 편이라고 한다.또한 그래프의 모양에 따라서 느낌이 달라지기 때문에 크기를 많이 변경해봐야한다.plt.title() 그래프의 제목 그리기ax.set_title() 개별 서브 플롯에 제목을 추가하는 데 사용 fig.suptitle() 모든 서브 플롯에 공통 인 메인 타이틀을 추가plt.rcParams['figure.dpi'] = 200 # or dpi=200Alignmentsmatplotlib 레이아웃과 설계의 조합이다.두 개의 그래프가 한 개의 그래프보다 시각적으로 의미적으로 모두 좋다.두 개를 비교하기 위해 가장 쉬운 방법은 직사각형으로 배치하는 것이다.subplot을 통하여 초기 크기로 시작할 수 있다.subplots() 하나의 그림에 여러 플롯을 그리기 subplot2grid() 일반 그리드 내부의 특정 위치에 서브플롯을 생성 add_axes() 축 추가하기 gridspec() Figure 내에 서브플롯을 배치 add_subplot() Figure 내에 서브플롯을 배치inset_axes() 하위에 축 추가하기make_axes_locatable() 축 배치하기fig, axes = plt.subplots(2, 3, figsize=(8, 5))plt.show()# tight_layout을 통해 그래프 사이에 여유공간이 생김fig,axes =plt.subplots(2, 3, figsize=(8, 5))plt.tight_layout()plt.show()# subplot이 항상 같을 필요가 없음. 이 때 subplot2grid를 사용fig=plt.figure(figsize=(8,5)) #시작 크기 지정ax=[None for _ in range(6)]ax[0]=plt.subplot2grid((3,4),(0,0),colspan=4)ax[1]=plt.subplot2grid((3,4),(1,0),colspan=1)ax[2]=plt.subplot2grid((3,4),(1,1),colspan=1)ax[3]=plt.subplot2grid((3,4),(1,2),colspan=1)ax[4]=plt.subplot2grid((3,4),(1,3),colspan=1,rowspan=2)ax[5]=plt.subplot2grid((3,4),(2,0),colspan=3)for ix in range(6):     ax[ix].set_title('ax[{}]'.format(ix)) # subplot에 제목 생성    #ticks는 축에 표시되는 숫자    ax[ix].set_xticks([])    ax[ix].set_yticks([])    fig.tight_layout()plt.show()fig = plt.figure(figsize=(8, 5))ax = [None for _ in range(4)]#add_axes를 이해하기 힘들어서 값들을 눈에 띄게 변경해보았다.[왼쪽, 아래쪽, 너비, 높이]ax[0]=fig.add_axes([0.1,0.2,0.3,0.4])ax[1]=fig.add_axes([0.3,0.4,0.5,0.6])ax[2]=fig.add_axes([0.5,0.6,0.7,0.8]) ax[3]=fig.add_axes([0.6,0.7,0.8,0.9])for ix in range(4):    ax[ix].set_title('ax[{}]'.format(ix))    ax[ix].set_xticks([])    ax[ix].set_yticks([])plt.show()# gridspec을 이용하여 만들기fig=plt.figure(figsize=(8, 5))gs=fig.add_gridspec(3, 3) #(3x3 크기)ax=[None for _ in range(5)]ax[0]=fig.add_subplot(gs[0, :]) ax[0].set_title('gs[0, :]')ax[1]=fig.add_subplot(gs[1, :-1])ax[1].set_title('gs[1, :-1]')ax[2]=fig.add_subplot(gs[1:, -1])ax[2].set_title('gs[1:, -1]')ax[3]=fig.add_subplot(gs[-1, 0])ax[3].set_title('gs[-1, 0]')ax[4]=fig.add_subplot(gs[-1, -2])ax[4].set_title('gs[-1, -2]')for ix in range(5):    ax[ix].set_xticks([])    ax[ix].set_yticks([])plt.tight_layout()plt.show()fig,ax=plt.subplots()axin1=ax.inset_axes([0.8, 0.1, 0.15, 0.15])plt.show()from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatablefig,ax=plt.subplots(1, 1)ax_divider=make_axes_locatable(ax)ax=ax_divider.append_axes(\"right\", size=\"7%\", pad=\"2%\")plt.show()Colormaphttps://medium.com/nightingale/how-to-choose-the-colors-for-your-data-visualizations-50b2557fa335그래프에서 색상은 중요한 요소이다diverging 중앙을 기준으로 색상 변경qualitative 범주별로 색상 변경sequential 순차적으로 색상 변경scientific 다양한 색상으로 순차적 변경def cmap_plot(cmap_list, ctype):    cmaps=cmap_list    n=len(cmaps)    fig=plt.figure(figsize=(8.25, n*.20), dpi=200)    ax=plt.subplot(1, 1, 1, frameon=False, xlim=[0,10], xticks=[], yticks=[])    fig.subplots_adjust(top=0.99, bottom=0.01, left=0.18, right=0.99)    y,dy,pad = 0, 0.3, 0.08    ticks,labels = [], []    for cmap in cmaps[::-1]:        Z=np.linspace(0,1,512).reshape(1,512)        plt.imshow(Z, extent=[0,10,y,y+dy], cmap=plt.get_cmap(cmap))        ticks.append(y+dy/2)        labels.append(cmap)        y = y + dy + pad    ax.set_ylim(-pad,y)    ax.set_yticks(ticks)    ax.set_yticklabels(labels)    ax.tick_params(axis='y', which='both', length=0, labelsize=5)    plt.title(f'{ctype} Colormap', fontweight='bold', fontsize=8)    plt.show()#diverging#양쪽 끝으로 갈수록 색이 어두워진다.diverge_cmap=('PRGn', 'PiYG', 'RdYlGn', 'BrBG', 'RdGy', 'PuOr', 'RdBu', 'RdYlBu',  'Spectral', 'coolwarm_r', 'bwr_r', 'seismic_r')cmap_plot(diverge_cmap, 'Diverging')#Qualitative Colormap#최대 10가지 색상을 구성하고 점점 더 작은 범주, 다른 범주와 그룹화#유사한 색상은 피하고 채도나 밝기보다 색을 분명하게 바꾸는 것이 좋다.qualitative_cmap=('tab10', 'tab20', 'tab20b', 'tab20c',         'Pastel1', 'Pastel2', 'Paired',         'Set1', 'Set2', 'Set3', 'Accent', 'Dark2' )cmap_plot(qualitative_cmap, 'Qualitative')#Sequential Colormap#밀도 표현에 효과적이고 지도 그래프에도 효과적이다.#밝기의 변화에 따라서 값을 비교할 수 있다.sequential_cmap=('Greys', 'Reds', 'Oranges',          'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',         'Purples', 'YlGnBu', 'Blues', 'PuBu', 'GnBu', 'PuBuGn', 'BuGn',         'Greens', 'YlGn','bone', 'gray', 'pink', 'afmhot', 'hot', 'gist_heat', 'copper',          'Wistia', 'autumn_r', 'summer_r', 'spring_r', 'cool', 'winter_r')            cmap_plot(sequential_cmap, 'Sequential')netflix_date = netflix_titles[['date_added']].dropna()netflix_date['year'] = netflix_date['date_added'].apply(lambda x : x.split(', ')[-1])netflix_date['month'] = netflix_date['date_added'].apply(lambda x : x.lstrip().split(' ')[0])month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'][::-1]df = netflix_date.groupby('year')['month'].value_counts().unstack().fillna(0)[month_order].T#응용하면 이런 그래프도 만들어진다.plt.figure(figsize=(10,7),dpi=200)plt.pcolor(df,cmap='gist_heat_r',edgecolors='white',linewidths=2)plt.xticks(np.arange(0.5,len(df.columns),1),df.columns,fontsize=7,fontfamily='serif')plt.yticks(np.arange(0.5,len(df.index),1),df.index,fontsize=7,fontfamily='serif')plt.title('Netflix Contents Update',fontsize=12,fontfamily='serif',fontweight='bold',position=(0.23,1.0+0.02))cbar=plt.colorbar()cbar.ax.tick_params(labelsize=8) cbar.ax.minorticks_on()plt.show()#Scientific Colormapscientific_cmap=('viridis','plasma','inferno','magma')cmap_plot(scientific_cmap,'Scientific')Text &amp; Annotate &amp; Patch그래프에 세부 사항을 넣을 수 있다.ax.text와 ax.annotate는 유사하지만 다르다.ax.text 그래프의 비율 좌표를 표현ax.annotate 그래프의 좌표를 표현va, ha 현재 좌표가 텍스트의 중심, 왼쪽, 오른쪽을 결정color 색상과 RGB값 선택 bbox 텍스트를 포장하는 상자 표현facecolor와 edgecolor가 분리되어있음pad로 html에서 처럼 padding 가능boxstyle로 직사각형의 모서리 조정 가능fig,ax=plt.subplots(figsize=(5, 5), dpi=100)# Gray Boxax.text(0.1, 0.9, 'Test', color='gray', va=\"center\", ha=\"center\")# Red Boxax.text(0.3, 0.7, 'Test', color='red', va=\"center\", ha=\"center\",        bbox=dict(facecolor='none', edgecolor='red'))# Blue Boxax.text(0.5, 0.5, 'Test', color='blue', va=\"center\", ha=\"center\",        bbox=dict(facecolor='none', edgecolor='blue', pad=10.0))# Green Boxax.text(0.7, 0.3, 'Test', color='green', va=\"center\", ha=\"center\",        bbox=dict(facecolor='none', edgecolor='green', boxstyle='round'))# Blackax.text(0.9, 0.1, 'Test', color='black', va=\"center\", ha=\"center\",        bbox=dict(facecolor='none', edgecolor='black', boxstyle='round, pad=0.5'))ax.set_xticks([])ax.set_yticks([])plt.show()효과적인 표현을 위한 그림들import matplotlib.path as mpathimport matplotlib.lines as mlinesimport matplotlib.patches as mpatchesfrom matplotlib.collections import PatchCollectiondef label(xy, text):    y = xy[1] - 0.15      plt.text(xy[0], y, text, ha=\"center\", family='sans-serif', size=14)fig, ax = plt.subplots()grid = np.mgrid[0.2:0.8:3j, 0.2:0.8:3j].reshape(2, -1).Tpatches = []# 원circle = mpatches.Circle(grid[0], 0.1, ec=\"none\")patches.append(circle)label(grid[0], \"Circle\")# 직사각형rect = mpatches.Rectangle(grid[1] - [0.025, 0.05], 0.05, 0.1, ec=\"none\")patches.append(rect)label(grid[1], \"Rectangle\")# wedgewedge = mpatches.Wedge(grid[2], 0.1, 30, 270, ec=\"none\")patches.append(wedge)label(grid[2], \"Wedge\")# Polygonpolygon = mpatches.RegularPolygon(grid[3], 5, 0.1)patches.append(polygon)label(grid[3], \"Polygon\")# ellipseellipse = mpatches.Ellipse(grid[4], 0.2, 0.1)patches.append(ellipse)label(grid[4], \"Ellipse\")# arrowarrow = mpatches.Arrow(grid[5, 0] - 0.05, grid[5, 1] - 0.05, 0.1, 0.1,                       width=0.1)patches.append(arrow)label(grid[5], \"Arrow\")# path patchPath = mpath.Pathpath_data = [    (Path.MOVETO, [0.018, -0.11]),    (Path.CURVE4, [-0.031, -0.051]),    (Path.CURVE4, [-0.115, 0.073]),    (Path.CURVE4, [-0.03, 0.073]),    (Path.LINETO, [-0.011, 0.039]),    (Path.CURVE4, [0.043, 0.121]),    (Path.CURVE4, [0.075, -0.005]),    (Path.CURVE4, [0.035, -0.027]),    (Path.CLOSEPOLY, [0.018, -0.11])]codes, verts = zip(*path_data)path = mpath.Path(verts + grid[6], codes)patch = mpatches.PathPatch(path)patches.append(patch)label(grid[6], \"PathPatch\")# fancy boxfancybox = mpatches.FancyBboxPatch(    grid[7] - [0.025, 0.05], 0.05, 0.1,    boxstyle=mpatches.BoxStyle(\"Round\", pad=0.02))patches.append(fancybox)label(grid[7], \"FancyBboxPatch\")# linex, y = np.array([[-0.06, 0.0, 0.1], [0.05, -0.05, 0.05]])line = mlines.Line2D(x + grid[8, 0], y + grid[8, 1], lw=5., alpha=0.3)label(grid[8], \"Line2D\")colors = np.linspace(0, 1, len(patches))collection = PatchCollection(patches, cmap=plt.cm.hsv, alpha=0.3)collection.set_array(np.array(colors))ax.add_collection(collection)ax.add_line(line)plt.axis('equal')plt.axis('off')plt.tight_layout()plt.show()Details &amp; Examples그림에 대한 다양한 설정Horizontal and Vertical (barplot)Border(edge) color and thicknessMain Color &amp; Sub ColorTransparencySpanFont Weight, Color, Family, Size …글꼴 굵기, 크기를 설정할 수 있다.serifs와 sans serifs도 차이가 있다.fontsize, color, fontweight, fontfamily같은 키워들을 이용할 수 있다.Horizontal keyboard &amp; Vertical (barplot)x축의 수가 많으면 가독성이 낮다. 예를들면 countplot은 x축과 겹치는 경우가 발생한다.이럴 때는 수직으로 배치하면 가독성이 좋다.Border(Edge) Color &amp; Thickness (Width)별도의 테두리를 이용하여 가독성을 높일 수 있다.R에서 자주 이용한다.경계선은 밝기와 투명도를 조절하여 구별하는 것이 좋다.Main Color &amp; Sub Color화려한 색상보다 정보의 전달을 편하게 하는 것에 목적을 둔다. 그래프의 종류에 따라 색상도 변경하는 것이 좋다.특정 부품을 강조 표시할 때 목록으로 전달하는 것이 좋다.from matplotlib.ticker import FuncFormatterdef age_band(num):    for i in range(1, 100):        if num &lt; 10*i :             return f'under {i*10}'titanic_train=pd.read_csv(\"train.csv\")titanic_train['age_band']=titanic_train['Age'].apply(age_band)titanic_age = titanic_train[['age_band', 'Survived']].groupby('age_band')['Survived'].value_counts().sort_index().unstack().fillna(0)titanic_age['Survival rate']=titanic_age[1] / (titanic_age[0] + titanic_age[1]) * 100fig, ax=plt.subplots(1, 2, figsize=(18, 7), dpi=300)# ax1ax[0].bar(titanic_age['Survival rate'].index, titanic_age['Survival rate'], color='gray')ax[0].set_title('Age Band &amp; Survival Rate(Before)')# ax2color_map=['gray' for _ in range(9)]color_map[0] = color_map[8] = '#3caea3'ax[1].bar(titanic_age['Survival rate'].index, titanic_age['Survival rate'], alpha=0.7, color=color_map, width=0.6, edgecolor='black', linewidth=1.2)ax[1].set_title('Age Band &amp; Survival Rate(After)', fontsize=15, fontweight='bold', position=(0.25, 1.0+0.05))for i in titanic_age['Survival rate'].index:    ax[1].annotate(f\"{titanic_age['Survival rate'][i]:.02f}%\",                    xy=(i, titanic_age['Survival rate'][i] + 2),                   va = 'center', ha='center',fontweight='bold', color='#383838')ax[1].yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:}%')) plt.suptitle('* Focus on survival rates for young and old', x=0.65, y=0.94, color='gray')plt.subplots_adjust(left=0.5, right=0.8)plt.tight_layout()plt.show()Main Color &amp; Sub Color산점도와 같이 많은 점들이 겹치면 투명도가 중요하다. 투명도를 이용해서 둘 이상의 그림을 함께 배치할 수 있다.import seaborn as snsexam_data=pd.read_csv(\"StudentsPerformance.csv\")fig, ax=plt.subplots(1, 2, figsize = (15, 7), dpi=150)ax[0].scatter(x='math score', y='reading score',data=exam_data, color='gray')ax[0].set_title('Before')ax[1].scatter(x='math score', y='reading score',data=exam_data[exam_data['gender']=='male'], color='skyblue', alpha=0.5, label='Male', s=70)ax[1].scatter(x='math score', y='reading score',data=exam_data[exam_data['gender']=='female'], color='salmon', alpha=0.5, label='Female', s=70)ax[1].set_title('After', fontsize=15, fontweight='bold')ax[1].legend()plt.gca().spines['top'].set_visible(False)plt.gca().spines['right'].set_visible(False)plt.show()Spanaxvspan: 수직axhspan: 수평sns.set_style('whitegrid') # plot with gridmovie=netflix_titles[netflix_titles['type'] == 'Movie']    rating_order=['G', 'TV-Y', 'TV-G', 'PG', 'TV-Y7', 'TV-Y7-FV', 'TV-PG', 'PG-13', 'TV-14', 'R', 'NC-17', 'TV-MA']movie_rating=movie['rating'].value_counts()[rating_order] fig, ax=plt.subplots(1, 1, figsize=(14, 7), dpi=200)ax.bar(movie_rating.index, movie_rating,  color=\"#d0d0d0\", width=0.6, edgecolor='black')ax.set_title(f'Distribution of Movie Rating (Before)', fontweight='bold')plt.show()def rating_barplot(data, title, height, h_lim=None):    fig, ax=plt.subplots(1,1, figsize=(14, 7), dpi=200)    if h_lim:        ax.set_ylim(0, h_lim)    ax.bar(data.index, data,  color=\"#e0e0e0\", width=0.52, edgecolor='black')    color=['green',  'blue',  'orange',  'red']    span_range=[[0, 2], [3,  6], [7, 8], [9, 11]]    for idx, sub_title in enumerate(['Little Kids', 'Older Kids', 'Teens', 'Mature']):        ax.annotate(sub_title,xy=(sum(span_range[idx])/2 ,height),xytext=(0,0), textcoords='offset points',                    va=\"center\", ha=\"center\",color=\"w\", fontsize=16, fontweight='bold',                    bbox=dict(boxstyle='round4', pad=0.4, color=color[idx], alpha=0.6))        ax.axvspan(span_range[idx][0]-0.4,span_range[idx][1]+0.4,  color=color[idx], alpha=0.07)    ax.set_title(f'Distribution of {title} Rating (After)', fontsize=15, fontweight='bold', position=(0.20, 1.0+0.03))    plt.show()rating_barplot(movie_rating,'Movie', 1200, 1400)MEME : xkcd themeimport matplotlibmatplotlib.font_manager._rebuild()with plt.xkcd():    fig=plt.figure()    ax=fig.add_axes((0.1, 0.2, 0.8, 0.7))    ax.spines['right'].set_color('none')    ax.spines['top'].set_color('none')    ax.set_xticks([])    ax.set_yticks([])    ax.set_ylim([-30, 10])    data=np.ones(100)    data[70:]-=np.arange(30)    ax.annotate('THE DAY I REALIZED\\nI COULD COOK BACON\\nWHENEVER I WANTED',        xy=(70, 1), arrowprops=dict(arrowstyle='-&gt;'), xytext=(15, -10))    ax.plot(data)    ax.set_xlabel('time')    ax.set_ylabel('my overall health')    fig.text(0.5, 0.05,'\"Stove Ownership\" from xkcd by Randall Munroe',ha='center')with plt.xkcd():    fig=plt.figure()    ax=fig.add_axes((0.1, 0.2, 0.8, 0.7))    ax.bar([0, 1], [0, 100], 0.25)    ax.spines['right'].set_color('none')    ax.spines['top'].set_color('none')    ax.xaxis.set_ticks_position('bottom')    ax.set_xticks([0, 1])    ax.set_xticklabels(['CONFIRMED BY\\nEXPERIMENT', 'REFUTED BY\\nEXPERIMENT'])    ax.set_xlim([-0.5, 1.5])    ax.set_yticks([])    ax.set_ylim([0, 110])    ax.set_title(\"CLAIMS OF SUPERNATURAL POWERS\")    fig.text(0.5, -0.05,'\"The Data So Far\" from xkcd by Randall Munroe',ha='center')plt.show()",
        "url": "/programming-kaggle1"
    }
    ,
    
    "programming-baekjoon2": {
        "title": "백준 (2) &lt;br&gt; (1018, 1085, 1181, 1259, &lt;br&gt; 1436, 1654, 1874, 1920)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)    백준 (9) 우선순위 큐 알고리즘 (1927,11279,11286,1715,11766)solved.ac의 class 2단계 8문항을 풀어보았다.파이썬 알고리즘 과제는 알고리즘 공부를 많이 하지 않은 나로써는 조금 어려웠다.https://solved.ac/search?query=in_class:21018번 체스판 다시 칠하기https://www.acmicpc.net/problem/1018N,M 크기를  받고 보드 만들기nXm형태의 위치를 파악하기 쉽게 리스트 형태로 받았다.n, m=map(int,input().split())if 8&lt;=n&lt;=50 and 8&lt;=m&lt;=50:    board = [input() for i in range(n)]10 13BBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBWWWWWWWWWWBWBWWWWWWWWWWBWB위치가 짝수일 때와 홀수일 때로 나눠서  W, B가 아닐 때마다 점수를 추가해준 다음 가장 최소가 되는 값만 찾아내면 된다.따라서 n * m의 보드에서 가능한 경우의 수는 n-7 * m-7이다. ex)10 13을 입력받을 경우 18가지.8 * 8로 잘라주기 위해서 k와 l을 (i,i+8), (j,j+8)로 한정짓는다.k+l이 홀수일 경우와 짝수일 경우, W로 시작할 경우와 B로 시작할 경우를 나눠서 모든 경우의 수를 반복문으로 확인해준다.마지막으로 total_score에 들어있는 값들 중 최솟값을 구해준다.total_score=[]# 보드에서 경우의 수 나누어주기for i in range(n-7):    for j in range(m-7):        count_w=0 #w가 아닐때        count_b=0 #b가 아닐때        #8*8 크기로 잘라주기        for k in range(i,i+8):             for l in range (j,j+8):                #각 경우의 수마다 비교해서 점수 추가하기                if (k+l)%2==0:                    if board[k][l]!='W':                                                    count_w=count_w+1                    if board[k][l]!='B':                        count_b=count_b+1                else:                    if board[k][l]!='W':                        count_b=count_b+1                    if board[k][l]!='B':                                                    count_w=count_w+1        # 점수들 한 list에 모아주기        total_score.append(count_w)        total_score.append(count_b)print(min(total_score)) #최솟값 출력121085번 직사각형에서 탈출https://www.acmicpc.net/problem/1085x,y,w,h 입력받기x,y,w,h =map(int,input().split())6 2 10 30과 가까운 경계선은 x, y로 w,h와 가까운 경계선은 w-x, h-y로 표현해주는 대신 음수가 나올수 있기때문에 abs()를 이용하여 절댓값으로 표현.print(min(x,y,abs(w-x),abs(h-y)))11181번 단어 정렬https://www.acmicpc.net/problem/1181단어 리스트 입력받기n=int(input())word_list=[input() for i in range(n)]13butiwonthesitatenomorenomoreitcannotwaitimyours길이가 짧은 것부터 같으면 사전 순으로 정렬하기 (단, 중복 제외)set함수로 중복을 먼저 제거하였다.단어의 길이를 먼저, 그다음에 단어를 하나의 tuple로 만들어 리스트를 다시 만들어주었다.sort를 이용하면 앞에 숫자가 들어갔기 떄문에 길이, 알파벳 순으로 정렬된다.# 중복 단어 제거word_list=list(set(word_list))len_word_list=[]# (단어 길이, 단어) 형태의 tuple 만들기for i in word_list:    len_word_list.append((len(i),i))# 정렬하기len_word_list.sort()print(len_word_list)# 출력하기for j,k in len_word_list:    print(k)[(1, 'i'), (2, 'im'), (2, 'it'), (2, 'no'), (3, 'but'), (4, 'more'), (4, 'wait'), (4, 'wont'), (5, 'yours'), (6, 'cannot'), (8, 'hesitate')]iimitnobutmorewaitwontyourscannothesitate1259번 팰린드롬수https://www.acmicpc.net/problem/1259앞에서 읽어도 뒤에서 읽어도 같은 숫자 찾기0을 입력하면 반복문이 끝나게 while과 if를 이용하였다.입력받은 숫자는 위치를 찾기 편하게 문자형으로 입력받았다.입력받은 숫자의 길이/2 만큼의 반복문을 돌리면 반대쪽은 (숫자의 길이-i-1)로 대응된다.한가지 숫자라도 값이 다르면 False 값을 가지고 ‘no’를 출력하면 ‘yes’를 출력하는 것을 만들 때보다 길이가 짧아질 수 있다.while True:    word=input() # 숫자를 무한으로 입력받기 위해 while문 사용    quest=True # 한가지 입력값을 처리하고나서 True, False값을 True로 초기화    if word=='0': # 0을 입력하면 반복문 종료        break    else:        word_len=len(word)        for i in range(int((word_len)/2)): #단어 길이의 반만 확인하면 반대쪽 숫자와 대응된다.                if word[i]!=word[word_len-1-i]: #반대쪽 숫자와 대응하기 위해서 word_len-1-i 사용                    quest=False # 하나의 경우라도 False가 나오면 반복문 종료                    continue        if quest==False: # False가 나오면 바로 'no' 출력            print('no')        else: print('yes')121yes1231no12421yes01436번 영화감독 숌https://www.acmicpc.net/problem/1436666이 적어도 3개이상 연속으로 들어가는 수를 만든다처음에 문제를 풀 때 중간에 666이 3개 이상 들어가는 경우를 제외해서 틀렸다.list666에 가장 작은 숫자인 666부터 ‘666’이 문자열로 들어가있는 숫자들을 확인해서 추가하였다.입력받은 숫자가 list666의 길이보다 크면 계속 추가해주었고 list666[num-1]을 통하여 값을 출력해준다.num=int(input())list666=[]i=666while len(list666)&lt;num:    if '666' in str(i):        list666.append(i)    i=i+1print(list666[num-1])326661654번 랜선 자르기https://www.acmicpc.net/problem/1654숫자 입력받기k,n=map(int,input().split())lan=[int(input()) for i in range(k)]4 11802743457539시간초과된 코드for문을 이용하니까 연산자가 너무 많아서 시간이 초과되었던 것 같다.div_list=[]for i in range(min(lan)):    div=int(min(lan))-i    count=[]    for w in lan:        count.append(w//div)    n_result=0    for j in range(k):        n_result=n_result+count[j]    if n_result==n:        div_list.append(div)print(max(div_list))200이진탐색을 이용하여보자.이진탐색이란 마치 병뚜껑 숫자맞추기를 할 때 50을 먼저 외치고 다음에 25나 75를 외치는 것처럼 가운데에 위치한 값들을 기반으로 탐색하는 것이다.정렬된 데이터일 때 사용 가능하다.따라서 입력받은 값들을 기준으로 max값과 1을 양 끝 값으로 놓는다.시작과 끝이 같을 때 까지 while문을 돌련준다.중간값을 구하고 이 값으로 입력받은 값들을 나누어준다.이때 잘라진 갯수가 n보다 크면 시작값에 중간값+1을, 작으면 끝값을 중간값-1을 해준다.while문이 다 돌고 나면 그 중 작은 값이 정답이다.start , end= 1,max(lan) #시작값과 끝값 구하기while start&lt;=end: #루프를 돌기위한 조건문    mid=(start+end)//2 #중간값 설정    cutting=0 #잘라진 선의 갯수 선언    for i in lan:        cutting+=i//mid #잘라진 선의 갯수 구하는 for문    if cutting&gt;=n: #n과 잘라진 선의 크기 비교를 통한 중간값 찾기        start=mid+1     else:        end=mid-1print(min(start,end))2001874번 스택 수열https://www.acmicpc.net/problem/1874스택과 푸쉬, 팝 이해하기push를 세 번 하면 [1,2,3] 스택이 쌓이게 되고 여기서 pop을 하면 3이 출력된다.n을 통해 입력할 숫자의 갯수를 입력받고 num을 통해 숫자를 입력받는다.count는 입력받을 숫자가 stack에 입력되도록 해준다. 0으로 두면 0부터 시작이다.따라서 1로 한다.result를 통해 +와 -를 입력받고 stack에는 count에 생긴 숫자들을 쌓아둔다.while문을 통해 stack을 완성하고 if문을 통해 해당 숫자가 나오면 -를 입력한 뒤pop해서 숫자를 제거한다.n=int(input())count=1 #count=1로 해줘야 0부터 숫자를 세지않는다.result=[] # +와 -를 저장하기 위한 리스트stack=[] # 쌓인 숫자를 저장하기 위한 리스트temp=True # 불가능한 경우에 False처리하기 위한 tempfor i in range(n):    num=int(input())     while count&lt;=num: #num과 같거나 작아질때 까지 stack에 숫자를 입력받는다.        stack.append(count)        result.append('+') #입력받은 숫자만큼 결과에 +를 입력해준다.        count=count+1    if stack[-1]==num: #스택의 마지막 숫자가 num과 같을 경우 해당 숫자를 pop하고 -를 입력해준다.        stack.pop()        result.append('-')    else:        temp=False # if문이 적용되지 않는 경우에는 False를 전달해준다.        if temp==False:    print('NO')else:    for j in result:        print(j)843687521++++--++-++-----1920번 스택 수열https://www.acmicpc.net/problem/1920시간초과list를 사용했을 때는 시간초과가 나왔고 set을 이용했을 때는 정상적으로 나왔다.리스트의 in연산자를 통한 포함 여부의 시간 복잡도는 O(N)이다.이분 탐색의 시간 복잡도는 O(logN) 이다.Set과 Dictionary의 in연산을 통한 포함 여부 확인의 시간 복잡도는 O(1)이다.따라서 N만 set으로 받아줘도 시간이 매우 단축된다.n=int(input())N=set(map(int,input().split()))m=int(input())M=list(map(int,input().split()))54 1 5 2 351 3 7 9 5for i in range(m):    if M[i] in N:        print(1)    else:        print(0)11001",
        "url": "/programming-baekjoon2"
    }
    ,
    
    "programming-baekjoon1": {
        "title": "백준 (1) &lt;br&gt; (2557, 8958, 1000, 1001, 1008, &lt;br&gt; 2935, 2753, 2884, 5063,4101)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)    백준 (9) 우선순위 큐 알고리즘 (1927,11279,11286,1715,11766)백준 10문제를 풀어보았다.2557. Hello Worldhttps://www.acmicpc.net/problem/2557Hello World!를 출력해야하는데 Hello World를 출력하여서 한번 틀렸다. 문제를 잘 읽어야한다.print(\"Hello World!\")Hello World!8958. OX퀴즈https://www.acmicpc.net/problem/8958for문을 이용하여 입력받을 ox의 갯수를 입력받고 for문 중첩을 이용하여 ox의 길이를 파악하고 if문으로 ox 여부를 확인하였다.ox의 연속성에 따른 점수변화를 num_score로 두고 total_score을 num_score의 합으로 설정하였다.num=int(input())for i in range(num):    ox=input()    total_score=0    num_score=0    for i in range(len(ox)):        if (ox[i]=='O') is True:            num_score=num_score+1        else:            num_score=0        total_score=total_score+num_score    print(total_score)5OOXXOXXOOO10OOXXOOXXOO9OXOXOXOXOXOXOX7OOOOOOOOOO55OOOOXOOOOXOOOOX301000. A+Bhttps://www.acmicpc.net/problem/1000map을 이용하여 a와 b를 사이 공백으로 분류시켜주는 것이 필요한 문제이다.a,b=map(int,input().split())if a &gt;0 and b&lt;10:    print(a+b)1 231001. A-Bhttps://www.acmicpc.net/problem/1001위 문제와 마찬가지로 map을 이용하여 a와 b를 사이 공백으로 분류시켜주는 것이 필요한 문제이다.a,b=map(int,input().split())if a &gt;0 and b&lt;10:    print(a-b)3 211008. A/Bhttps://www.acmicpc.net/problem/1008위 문제와 마찬가지로 map을 이용하여 a와 b를 사이 공백으로 분류시켜주는 것이 필요한 문제이다.무한 소수일 경우a,b=map(int,input().split())if a &gt;0 and b&lt;10:    print(a/b)1 30.3333333333333333유한 소수일 경우a,b=map(int,input().split())if a &gt;0 and b&lt;10:    print(a/b)4 50.82935. 소음https://www.acmicpc.net/problem/2935a와 b는 정수형으로 입력받고 + 와 * 는 문자형으로 입력받았다.a와 b가 10의 제곱 형태이므로 반복문을 통하여 10 ** i, 10 ** j로 제곱 형태를 판별하였고 + 와 * 는 if문으로 구분지어서 계산을 해주었다.파이썬에서는 제곱을 ** 형태로 표현하는 것을 상기해야한다.*를 이용한 경우a=int(input())cal=input()b=int(input())for i in range(99):    for j in range(99):        if (a==10**i) is True and (b==10**j) is True:            if cal=='+':                print(a+b)            if cal=='*':                print(a*b)1000*100100000+를 이용한 경우a=int(input())cal=input()b=int(input())for i in range(99):    for j in range(99):        if (a==10**i) is True and (b==10**j) is True:            if cal=='+':                print(a+b)            if cal=='*':                print(a*b)10000+10100102753. 윤년https://www.acmicpc.net/problem/2753입력받은 연도를 1 이상 4000 이하로 제한하고 연도를 4로 나눈 나머지가 0, 100으로 나눈 나머지가 0이 아닌 경우로 하나, 400으로 나눈 나머지가 0인 경우 하나로 나누어서 1을 출력해주고 나머지는 0을 출력하는 형태로 만들었다.나머지는 %로 구한다는 것을 상기해주었다.윤년인 경우year=int(input())if year&gt;=1 and year&lt;=4000:    if year%4==0 and year%100!=0:        print(1)    elif year%400==0:        print(1)    else:        print(0)20001윤년이 아닌 경우year=int(input())if year&gt;=1 and year&lt;=4000:    if year%4==0 and year%100!=0:        print(1)    elif year%400==0:        print(1)    else:        print(0)199902884. 알람 시계https://www.acmicpc.net/problem/2884위에 사칙연산 문제와 같이 map을 이용해서 시와 분을 분리하여 입력받는다.우선 시를 0 이상 23 이하, 분을 0 이상 59 이하로 한정해었다.첫번째 경우 분이 45 이상일 경우 분에서 45를 빼주더라도 시간은 변하지 않는다. 따라서 시간, 분-45 를 출력해주면 된다.두번째 경우 분이 45 미만일 경우 시가 하나 작아진다. 0 시 45 분 이전에는 날짜가 바뀌므로 23 에서 시를 빼주고 나머지 경우는 시에서 1 빼준다.분이 45 미만일 경우 60 - (45 - 분) 해주면 바뀐 분이 나온다. 따라서 분 + 15 로 표현해 주었다.시와 분 모두 바뀔 때h,m=map(int,input().split())if 0&lt;=h&lt;=23 and 0&lt;=m&lt;=59 :    if m-45&gt;=0:        print(h,m-45)    else:        if h-1&lt;0:            print(23-h,m+15)        else:            print(h-1,m+15)10 109 25날짜 까지 바뀔 때h,m=map(int,input().split())if 0&lt;=h&lt;=23 and 0&lt;=m&lt;=59 :    if m-45&gt;=0:        print(h,m-45)    else:        if h-1&lt;0:            print(23-h,m+15)        else:            print(h-1,m+15)0 3023 455063. TGNhttps://www.acmicpc.net/problem/5063test_case를 입력받아서 for문의 횟수를 한정시킨다.r, e, c를 map을 이용하여 입력받았으며 범위를 한정시켜주었다. 이때 (-10) ** 6으로 잘못 작성하여서 코드가 실행되지 않았다.광고 비용이 광고 수익과 일반 수익의 차보다 작을 때 광고를 하고 같으면 소용이 없고 크면 광고를 하지 않아야하므로 if문으로 구분시켰다.test_case=int(input())for i in range(test_case):    r,e,c=map(int,input().split())    if -(10**6)&lt;=r&lt;=(10**6) and -(10**6)&lt;=e&lt;=(10**6) and 0&lt;=c&lt;=(10**6):        if e-r&gt;c:            print('advertise')        elif e-r==c:            print('does not matter')        else:            print('do not advertise')30 100 70advertise100 130 30does not matter-100 -70 40do not advertise4101. 크냐?https://www.acmicpc.net/problem/4101while 반복문을 사용하여 계속 두 숫자를 입력받았으며 map을 통하여 공백을 기준으로 숫자를 나누었다.먼저 두 숫자가 0이면 해당 while문이 정지를 하게 만들어주고, 그 뒤에 두 숫자의 범위가 True면 두 숫자의 대소비교를 진행하였다.while True:    a,b=map(int,input().split())    if a==0 and b==0:            break    if 0&lt;a&lt;=10**6 and 0&lt;b&lt;=10**6:        if a&gt;b:            print(\"Yes\")        else:            print(\"No\")1 19No4 4No23 14Yes0 0",
        "url": "/programming-baekjoon1"
    }
    ,
    
    "programming-webcrawling1": {
        "title": "웹크롤링 (1) &lt;br&gt; urllib사용을 통한 크롤링",
            "author": "keonju",
            "category": "",
            "content": "웹크롤링 관련 글    웹크롤링 (1)-urllib사용을 통한 크롤링urllib사용을 통한 크롤링  url을 입력하여 작동하는 라이브러리로 통신을 통해 데이터를 주고받는 기능을 한다.  데이터를 받아오거나 다운로드할 수 있다1.urlretrieve  url로 표시된 네트워크 정보를 파일로 저장할 수 있는 기능 (이미지 , html)  (filename, headers) 튜플로 반환  ex file, header = req.urlretrieve(url, path)import urllib.request as requrl 에 접근할 url주소를 담고, path에 저장할 경로와 파일명을 적으면 된다.파일명만 적을 경우 현재 위치로 저장이 된다.url = \"https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMTA4MjBfMTQx%2FMDAxNjI5NDIxNjQ5NzM5.D1F-l6COowiUicFVRlpfQeSJRtkR4f9lkbVZgwJm6r4g.lBjYtG_wiubtJdiCYg8reMDwyC3wkFhPy5Ou0VXWRIQg.JPEG.hyun_0930%2F1629420539963.jpg&amp;type=sc960_832\"path = \"test1.jpg\"url'https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMTA4MjBfMTQx%2FMDAxNjI5NDIxNjQ5NzM5.D1F-l6COowiUicFVRlpfQeSJRtkR4f9lkbVZgwJm6r4g.lBjYtG_wiubtJdiCYg8reMDwyC3wkFhPy5Ou0VXWRIQg.JPEG.hyun_0930%2F1629420539963.jpg&amp;type=sc960_832'path'test1.jpg'현재위치 조회import osos.getcwd()file, header = req.urlretrieve(url,path)print(file)test1.jpgprint(header)accept-ranges: bytescache-control: max-age=2592000content-length: 37532content-type: image/jpegexpires: Sun, 26 Sep 2021 23:53:24 GMTlast-modified: Fri, 27 Aug 2021 23:53:24 GMTp3p: CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"date: Fri, 27 Aug 2021 23:53:24 GMTage: 665797server: Testa/5.1.1strict-transport-security: max-age=31536000connection: closehtml저장url2 = \"https://www.naver.com/\"path2 = \"naver.html\"file2, header2 = req.urlretrieve(url2, path2)print(\"----------------------------------------------------\")print(f\"file name: {file}\")print(\"----------------------------------------------------\")print(\"Header Info :\")print(header)print(\"----------------------------------------------------\")print(f\"file name: {file2}\")print(\"----------------------------------------------------\")print(\"Header Info :\")print(header2)----------------------------------------------------file name: test1.jpg----------------------------------------------------Header Info :accept-ranges: bytescache-control: max-age=2592000content-length: 37532content-type: image/jpegexpires: Sun, 26 Sep 2021 23:53:24 GMTlast-modified: Fri, 27 Aug 2021 23:53:24 GMTp3p: CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"date: Fri, 27 Aug 2021 23:53:24 GMTage: 665797server: Testa/5.1.1strict-transport-security: max-age=31536000connection: close----------------------------------------------------file name: naver.html----------------------------------------------------Header Info :Server: NWSDate: Sat, 04 Sep 2021 16:50:00 GMTContent-Type: text/html; charset=UTF-8Transfer-Encoding: chunkedConnection: closeSet-Cookie: PM_CK_loc=4d397054570413bd11c4b9094901203f9fef0e3df1bd78a55cef2ac0fd1d9e5e; Expires=Sun, 05 Sep 2021 16:50:00 GMT; Path=/; HttpOnlyCache-Control: no-cache, no-store, must-revalidatePragma: no-cacheP3P: CP=\"CAO DSP CURa ADMa TAIa PSAa OUR LAW STP PHY ONL UNI PUR FIN COM NAV INT DEM STA PRE\"X-Frame-Options: DENYX-XSS-Protection: 1; mode=blockStrict-Transport-Security: max-age=63072000; includeSubdomainsReferrer-Policy: unsafe-url2.urlerror  크롤링에서 발생할 수 있는 에러처리  에러처리를 통해서 어떤 에러가 발생하였는지 파악하고 코드를 수정  URLError: 요청한 곳의 서버가 없거나 네트워크 연결이 없는 상황  HTTPError: HTTP응답에 있는 status에 따라서 상태를 반환, status코드에 따라서 에러 유형이 다름  주의사항: URLError가 HTTPError도 잡기 때문에 HTTPError처리를 먼저 해줘야함1번 예제와 다르게 list에 넣고 for문을 통한 이미지 다운로드를 실시from urllib.error import URLError, HTTPErrorurl_list = ['https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMTA4MjBfMTQx%2FMDAxNjI5NDIxNjQ5NzM5.D1F-l6COowiUicFVRlpfQeSJRtkR4f9lkbVZgwJm6r4g.lBjYtG_wiubtJdiCYg8reMDwyC3wkFhPy5Ou0VXWRIQg.JPEG.hyun_0930%2F1629420539963.jpg&amp;type=sc960_832',            'https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMDA5MTNfNjMg%2FMDAxNjAwMDAxNjM1NzQ2.TuGLdOsJ8vLFnN589WEiiA5j5XrsWRA7lJUJicpozJwg.694y_QRQKQwqd7QR41nweA3T4vYnAGT4OqVuxWvJdrYg.JPEG.ecoanimal%2F51d63faf6312a3bc4873ee24d98cdfed.jpg&amp;type=a340']name_list = ['nuguli1.jpg', 'nuguli2.jpg']for i,url in enumerate(url_list):    # 예외 처리    try:        # 웹 수신 정보 읽기        response = req.urlopen(url)                # 수신 내용        contents = response.read()        print('----------------------------------------------------------------------------------------------------------------')        # 상태 정보 중간 출력        print(f'file_name : {name_list[i]}')        print('&lt;Header Info&gt;')        print(f'{response.info()}')        print(f'Status Code : {response.getcode()}')        print()        print('----------------------------------------------------------------------------------------------------------------')        # 파일 쓰기        with open(name_list[i], 'wb') as c:            c.write(contents)            except HTTPError as e: # HTTP 에러        print(\"다운로드 실패.\")        print('HTTPError Code : ', e.code)    except URLError as e: # URL 에러        print(\"Download failed.\")        print('URL Error Reason : ', e.reason)        # 성공    else:        print()        print(f'{name_list[i]}이미지 다운 완료.')----------------------------------------------------------------------------------------------------------------file_name : nuguli1.jpg&lt;Header Info&gt;accept-ranges: bytescache-control: max-age=2592000content-length: 37532content-type: image/jpegexpires: Sun, 26 Sep 2021 23:53:24 GMTlast-modified: Fri, 27 Aug 2021 23:53:24 GMTp3p: CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"date: Fri, 27 Aug 2021 23:53:24 GMTage: 665797server: Testa/5.1.1strict-transport-security: max-age=31536000connection: closeStatus Code : 200----------------------------------------------------------------------------------------------------------------nuguli1.jpg이미지 다운 완료.----------------------------------------------------------------------------------------------------------------file_name : nuguli2.jpg&lt;Header Info&gt;accept-ranges: bytescache-control: max-age=2592000content-length: 50098content-type: image/jpegexpires: Fri, 24 Sep 2021 14:11:17 GMTlast-modified: Wed, 25 Aug 2021 14:11:17 GMTp3p: CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"date: Wed, 25 Aug 2021 14:11:17 GMTage: 873524server: Testa/5.1.1strict-transport-security: max-age=31536000connection: closeStatus Code : 200----------------------------------------------------------------------------------------------------------------nuguli2.jpg이미지 다운 완료.3.urlopen/ urlparseimport urllib.request as reqfrom urllib.parse import urlparseurl=\"https://www.seoultech.ac.kr/index.jsp\"ele=req.urlopen(url)print('type : {}'.format(type(ele)))print()print(\"geturl : {}\".format(ele.geturl()))print()print(\"status : {}\".format(ele.status))print()print(\"headers : {}\".format(ele.getheaders()))print()print()print('parse : {}'.format(urlparse('https://www.smu.ac.kr/ko/index.do?param=test').query))print()type : &lt;class 'http.client.HTTPResponse'&gt;geturl : https://www.seoultech.ac.kr/index.jspstatus : 200headers : [('Date', 'Sat, 04 Sep 2021 16:50:01 GMT'), ('Content-Type', 'text/html; charset=UTF-8'), ('Set-Cookie', 'JSESSIONID=5BabR1bj8eGHwllZbNNO9LXiYD2V1HlqI1KZJiR7EG01ZnEBpYTlBwaFVwCi61YT.web1_servlet_www;Path=/;HttpOnly'), ('X-Cache', 'MISS from cf4.seoultech.ac.kr'), ('X-Cache-Lookup', 'HIT from cf4.seoultech.ac.kr:3128'), ('Transfer-Encoding', 'chunked'), ('Via', ''), ('Connection', 'close')]parse : param=testprint(ele.info())Date: Sat, 04 Sep 2021 16:50:01 GMTContent-Type: text/html; charset=UTF-8Set-Cookie: JSESSIONID=5BabR1bj8eGHwllZbNNO9LXiYD2V1HlqI1KZJiR7EG01ZnEBpYTlBwaFVwCi61YT.web1_servlet_www;Path=/;HttpOnlyX-Cache: MISS from cf4.seoultech.ac.krX-Cache-Lookup: HIT from cf4.seoultech.ac.kr:3128Transfer-Encoding: chunkedVia: Connection: closeheaders에 데이터 추가하기# pip install fake_useragentfrom fake_useragent import UserAgentua = UserAgent()### fake_useragentua = UserAgent()print(ua.random)print(ua.ie)print(ua.msie)print(ua['Internet Explorer'])print(ua.opera)print(ua.chrome)print(ua.google)print(ua['google chrome'])print(ua.firefox)print(ua.ff)print(ua.safari)Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.3319.102 Safari/537.36Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/4.0; GTB7.4; InfoPath.1; SV1; .NET CLR 2.8.52393; WOW64; en-US)Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.2; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0)Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0; FunWebProducts)Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/5.0 Opera 11.11Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1468.0 Safari/537.36Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:23.0) Gecko/20131011 Firefox/23.0Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20130401 Firefox/31.0Mozilla/5.0 (Windows; U; Windows NT 6.0; de-DE) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.3 Safari/533.19.4야후 파이낸스 데이터 받아오기import jsonimport urllib.request as reqfrom fake_useragent import UserAgent  url 주소를 찾는 것에서 시간이 조금 걸림이미지같은 경우 주소가 연동되지만 변동되는 데이터는 적용되지않음크롬 개발자도구에서 Network항목에서 RequestURL 찾기import jsonimport urllib.request as reqfrom fake_useragent import UserAgent# Fake Header 정보(가상으로 User-Agent 생성)ua = UserAgent()# 헤더 선언headers = {    'User-Agent': ua.ie,    'referer': 'https://finance.yahoo.com/'}# 다음 주식 요청 URLurl = \"https://query1.finance.yahoo.com/v7/finance/quote?formatted=true&amp;crumb=5b5ru0zoR.q&amp;lang=en-US&amp;region=US&amp;symbols=ADA-USD%2CBTC-USD%2CDOGE-USD%2CETH-USD%2CZM&amp;fields=symbol%2CshortName%2ClongName%2CregularMarketPrice%2CregularMarketChange%2CregularMarketChangePercent&amp;corsDomain=finance.yahoo.com\"res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')# 응답 데이터 str -&gt; json 변환 및 data 값 저장_json_data = json.loads(res)print( _json_data, '\\n'){'quoteResponse': {'result': [{'fullExchangeName': 'CCC', 'exchangeTimezoneName': 'Europe/London', 'symbol': 'ADA-USD', 'regularMarketChange': {'raw': -0.12517643, 'fmt': '-0.13'}, 'gmtOffSetMilliseconds': 3600000, 'firstTradeDateMilliseconds': 1506812400000, 'exchangeDataDelayedBy': 0, 'language': 'en-US', 'regularMarketTime': {'raw': 1630774030, 'fmt': '5:47PM BST'}, 'regularMarketChangePercent': {'raw': -4.1827593, 'fmt': '-4.18%'}, 'exchangeTimezoneShortName': 'BST', 'quoteType': 'CRYPTOCURRENCY', 'marketState': 'REGULAR', 'regularMarketPrice': {'raw': 2.8674967, 'fmt': '2.87'}, 'market': 'ccc_market', 'quoteSourceName': 'CoinMarketCap', 'tradeable': False, 'exchange': 'CCC', 'sourceInterval': 15, 'shortName': 'Cardano USD', 'region': 'US', 'regularMarketPreviousClose': {'raw': 2.9606647, 'fmt': '2.96'}, 'triggerable': True}, {'fullExchangeName': 'CCC', 'exchangeTimezoneName': 'Europe/London', 'symbol': 'BTC-USD', 'regularMarketChange': {'raw': -611.96094, 'fmt': '-611.96'}, 'gmtOffSetMilliseconds': 3600000, 'firstTradeDateMilliseconds': 1410908400000, 'exchangeDataDelayedBy': 0, 'language': 'en-US', 'regularMarketTime': {'raw': 1630774082, 'fmt': '5:48PM BST'}, 'regularMarketChangePercent': {'raw': -1.2106228, 'fmt': '-1.21%'}, 'exchangeTimezoneShortName': 'BST', 'quoteType': 'CRYPTOCURRENCY', 'marketState': 'REGULAR', 'regularMarketPrice': {'raw': 49937.133, 'fmt': '49,937.13'}, 'market': 'ccc_market', 'quoteSourceName': 'CoinMarketCap', 'tradeable': False, 'exchange': 'CCC', 'sourceInterval': 15, 'shortName': 'Bitcoin USD', 'region': 'US', 'regularMarketPreviousClose': {'raw': 49922.355, 'fmt': '49,922.36'}, 'triggerable': True}, {'fullExchangeName': 'CCC', 'exchangeTimezoneName': 'Europe/London', 'symbol': 'DOGE-USD', 'regularMarketChange': {'raw': 0.0015876293, 'fmt': '0.00'}, 'gmtOffSetMilliseconds': 3600000, 'firstTradeDateMilliseconds': 1410908400000, 'exchangeDataDelayedBy': 0, 'language': 'en-US', 'regularMarketTime': {'raw': 1630774083, 'fmt': '5:48PM BST'}, 'regularMarketChangePercent': {'raw': 0.52755743, 'fmt': '0.53%'}, 'exchangeTimezoneShortName': 'BST', 'quoteType': 'CRYPTOCURRENCY', 'marketState': 'REGULAR', 'regularMarketPrice': {'raw': 0.30252412, 'fmt': '0.30'}, 'market': 'ccc_market', 'quoteSourceName': 'CoinMarketCap', 'tradeable': False, 'exchange': 'CCC', 'sourceInterval': 15, 'shortName': 'Dogecoin USD', 'region': 'US', 'regularMarketPreviousClose': {'raw': 0.29575953, 'fmt': '0.30'}, 'triggerable': True}, {'fullExchangeName': 'CCC', 'exchangeTimezoneName': 'Europe/London', 'symbol': 'ETH-USD', 'regularMarketChange': {'raw': -63.86255, 'fmt': '-63.86'}, 'gmtOffSetMilliseconds': 3600000, 'firstTradeDateMilliseconds': 1438902000000, 'exchangeDataDelayedBy': 0, 'language': 'en-US', 'regularMarketTime': {'raw': 1630774082, 'fmt': '5:48PM BST'}, 'regularMarketChangePercent': {'raw': -1.608492, 'fmt': '-1.61%'}, 'exchangeTimezoneShortName': 'BST', 'quoteType': 'CRYPTOCURRENCY', 'marketState': 'REGULAR', 'regularMarketPrice': {'raw': 3906.476, 'fmt': '3,906.48'}, 'market': 'ccc_market', 'quoteSourceName': 'CoinMarketCap', 'tradeable': False, 'exchange': 'CCC', 'sourceInterval': 15, 'shortName': 'Ethereum USD', 'region': 'US', 'regularMarketPreviousClose': {'raw': 3933.8274, 'fmt': '3,933.83'}, 'triggerable': True}, {'fullExchangeName': 'NasdaqGS', 'symbol': 'ZM', 'gmtOffSetMilliseconds': -14400000, 'language': 'en-US', 'regularMarketTime': {'raw': 1630699203, 'fmt': '4:00PM EDT'}, 'regularMarketChangePercent': {'raw': 1.084419, 'fmt': '1.08%'}, 'quoteType': 'EQUITY', 'tradeable': False, 'regularMarketPreviousClose': {'raw': 295.09, 'fmt': '295.09'}, 'exchangeTimezoneName': 'America/New_York', 'regularMarketChange': {'raw': 3.2000122, 'fmt': '3.20'}, 'firstTradeDateMilliseconds': 1555594200000, 'exchangeDataDelayedBy': 0, 'exchangeTimezoneShortName': 'EDT', 'marketState': 'CLOSED', 'regularMarketPrice': {'raw': 298.29, 'fmt': '298.29'}, 'market': 'us_market', 'quoteSourceName': 'Delayed Quote', 'priceHint': 2, 'exchange': 'NMS', 'sourceInterval': 15, 'shortName': 'Zoom Video Communications, Inc.', 'region': 'US', 'triggerable': True, 'longName': 'Zoom Video Communications, Inc.'}], 'error': None}} data_list=_json_data['quoteResponse']['result']from pprint import pprintpprint(data_list)[{'exchange': 'CCC',  'exchangeDataDelayedBy': 0,  'exchangeTimezoneName': 'Europe/London',  'exchangeTimezoneShortName': 'BST',  'firstTradeDateMilliseconds': 1506812400000,  'fullExchangeName': 'CCC',  'gmtOffSetMilliseconds': 3600000,  'language': 'en-US',  'market': 'ccc_market',  'marketState': 'REGULAR',  'quoteSourceName': 'CoinMarketCap',  'quoteType': 'CRYPTOCURRENCY',  'region': 'US',  'regularMarketChange': {'fmt': '-0.13', 'raw': -0.12517643},  'regularMarketChangePercent': {'fmt': '-4.18%', 'raw': -4.1827593},  'regularMarketPreviousClose': {'fmt': '2.96', 'raw': 2.9606647},  'regularMarketPrice': {'fmt': '2.87', 'raw': 2.8674967},  'regularMarketTime': {'fmt': '5:47PM BST', 'raw': 1630774030},  'shortName': 'Cardano USD',  'sourceInterval': 15,  'symbol': 'ADA-USD',  'tradeable': False,  'triggerable': True}, {'exchange': 'CCC',  'exchangeDataDelayedBy': 0,  'exchangeTimezoneName': 'Europe/London',  'exchangeTimezoneShortName': 'BST',  'firstTradeDateMilliseconds': 1410908400000,  'fullExchangeName': 'CCC',  'gmtOffSetMilliseconds': 3600000,  'language': 'en-US',  'market': 'ccc_market',  'marketState': 'REGULAR',  'quoteSourceName': 'CoinMarketCap',  'quoteType': 'CRYPTOCURRENCY',  'region': 'US',  'regularMarketChange': {'fmt': '-611.96', 'raw': -611.96094},  'regularMarketChangePercent': {'fmt': '-1.21%', 'raw': -1.2106228},  'regularMarketPreviousClose': {'fmt': '49,922.36', 'raw': 49922.355},  'regularMarketPrice': {'fmt': '49,937.13', 'raw': 49937.133},  'regularMarketTime': {'fmt': '5:48PM BST', 'raw': 1630774082},  'shortName': 'Bitcoin USD',  'sourceInterval': 15,  'symbol': 'BTC-USD',  'tradeable': False,  'triggerable': True}, {'exchange': 'CCC',  'exchangeDataDelayedBy': 0,  'exchangeTimezoneName': 'Europe/London',  'exchangeTimezoneShortName': 'BST',  'firstTradeDateMilliseconds': 1410908400000,  'fullExchangeName': 'CCC',  'gmtOffSetMilliseconds': 3600000,  'language': 'en-US',  'market': 'ccc_market',  'marketState': 'REGULAR',  'quoteSourceName': 'CoinMarketCap',  'quoteType': 'CRYPTOCURRENCY',  'region': 'US',  'regularMarketChange': {'fmt': '0.00', 'raw': 0.0015876293},  'regularMarketChangePercent': {'fmt': '0.53%', 'raw': 0.52755743},  'regularMarketPreviousClose': {'fmt': '0.30', 'raw': 0.29575953},  'regularMarketPrice': {'fmt': '0.30', 'raw': 0.30252412},  'regularMarketTime': {'fmt': '5:48PM BST', 'raw': 1630774083},  'shortName': 'Dogecoin USD',  'sourceInterval': 15,  'symbol': 'DOGE-USD',  'tradeable': False,  'triggerable': True}, {'exchange': 'CCC',  'exchangeDataDelayedBy': 0,  'exchangeTimezoneName': 'Europe/London',  'exchangeTimezoneShortName': 'BST',  'firstTradeDateMilliseconds': 1438902000000,  'fullExchangeName': 'CCC',  'gmtOffSetMilliseconds': 3600000,  'language': 'en-US',  'market': 'ccc_market',  'marketState': 'REGULAR',  'quoteSourceName': 'CoinMarketCap',  'quoteType': 'CRYPTOCURRENCY',  'region': 'US',  'regularMarketChange': {'fmt': '-63.86', 'raw': -63.86255},  'regularMarketChangePercent': {'fmt': '-1.61%', 'raw': -1.608492},  'regularMarketPreviousClose': {'fmt': '3,933.83', 'raw': 3933.8274},  'regularMarketPrice': {'fmt': '3,906.48', 'raw': 3906.476},  'regularMarketTime': {'fmt': '5:48PM BST', 'raw': 1630774082},  'shortName': 'Ethereum USD',  'sourceInterval': 15,  'symbol': 'ETH-USD',  'tradeable': False,  'triggerable': True}, {'exchange': 'NMS',  'exchangeDataDelayedBy': 0,  'exchangeTimezoneName': 'America/New_York',  'exchangeTimezoneShortName': 'EDT',  'firstTradeDateMilliseconds': 1555594200000,  'fullExchangeName': 'NasdaqGS',  'gmtOffSetMilliseconds': -14400000,  'language': 'en-US',  'longName': 'Zoom Video Communications, Inc.',  'market': 'us_market',  'marketState': 'CLOSED',  'priceHint': 2,  'quoteSourceName': 'Delayed Quote',  'quoteType': 'EQUITY',  'region': 'US',  'regularMarketChange': {'fmt': '3.20', 'raw': 3.2000122},  'regularMarketChangePercent': {'fmt': '1.08%', 'raw': 1.084419},  'regularMarketPreviousClose': {'fmt': '295.09', 'raw': 295.09},  'regularMarketPrice': {'fmt': '298.29', 'raw': 298.29},  'regularMarketTime': {'fmt': '4:00PM EDT', 'raw': 1630699203},  'shortName': 'Zoom Video Communications, Inc.',  'sourceInterval': 15,  'symbol': 'ZM',  'tradeable': False,  'triggerable': True}]result_list = []for data in data_list:    _set={}    _set['symbol'] = data['symbol']    _set['Last_price'] = data['regularMarketPrice']['fmt']    _set['Change'] = data['regularMarketChange']['fmt']    _set['%Change'] = data['regularMarketChangePercent']['fmt']    result_list.append(_set)import pandas as pddf = pd.DataFrame(result_list)df                  symbol      Last_price      Change      %Change                  0      ADA-USD      2.87      -0.13      -4.18%              1      BTC-USD      49,937.13      -611.96      -1.21%              2      DOGE-USD      0.30      0.00      0.53%              3      ETH-USD      3,906.48      -63.86      -1.61%              4      ZM      298.29      3.20      1.08%      ",
        "url": "/programming-webcrawling1"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>
            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://keonju2.github.io/">주건나's Blog</a> &copy; 2023</section>
                <!--
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                -->
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    <a href="https://facebook.com/monkeykeonju" target="_blank" rel="noopener">Facebook</a>
                    
                    <!--
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                     -->
                </nav>

            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
    <div id="subscribe" class="subscribe-overlay">
        <a class="subscribe-overlay-close" href="#"></a>
        <div class="subscribe-overlay-content">
            
            <h1 class="subscribe-overlay-title">Search 주건나's Blog</h1>
            <p class="subscribe-overlay-description">
            검색어를 입력해주세요 </p>
            <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
        </div>
    </div>
    


    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-6FJ2289869', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
