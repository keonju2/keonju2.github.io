<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
    
    <!-- custom.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />
    
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- 웹폰트 추가-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">

    <!-- syntax.css 추가-->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="데이터 사이언티스트를 꿈꾸는 블로그입니다." />
    <link rel="shortcut icon" href="https://keonju2.github.io/assets/built/images/favicon.jpg" type="image/png" />
    <link rel="canonical" href="https://keonju2.github.io/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="주건나's Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="데이터 사이언티스트를 꿈꾸는 블로그입니다." />
    <meta property="og:url" content="https://keonju2.github.io/search" />
    <meta property="og:image" content="https://keonju2.github.io/assets/built/images/blog-cover1.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/monkeykeonju" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="데이터 사이언티스트를 꿈꾸는 블로그입니다." />
    <meta name="twitter:url" content="https://keonju2.github.io/" />
    <meta name="twitter:image" content="https://keonju2.github.io/assets/built/images/blog-cover1.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="주건나's Blog" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "주건나's Blog",
        "logo": "https://keonju2.github.io/"
    },
    "url": "https://keonju2.github.io/search",
    "image": {
        "@type": "ImageObject",
        "url": "https://keonju2.github.io/assets/built/images/blog-cover1.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://keonju2.github.io/search"
    },
    "description": "데이터 사이언티스트를 꿈꾸는 블로그입니다."
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="https://keonju2.github.io/">주건나's Blog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-R" role="menuitem"><a href="/tag/programming/">프로그래밍 언어</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/project/">프로젝트</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/license/">자격증</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/study/">이론 공부</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Tag별 Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
                <a class="social-link social-link-fb" href="https://facebook.com/monkeykeonju" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>
</a>
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "study-ml6": {
        "title": "머신러닝 정리 (6) &lt;br&gt; 모델 평가와 성능 향상",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학  모델 평가와 성능 향상          교차 검증        그리드 서치  평가 지표와 측정          이진 분류의 평가 지표      머신러닝 정리 (6) 모델 평가와 성능 향상본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python모델 평가와 성능 향상분류에서 score 메소드를 통해 얼마나 정확히 분류된 샘플의 비율을 알 수 있습니다.이번 단원에서는 일반화 성능 측정 방법인 교차 검증과 score 메소드가 제공하는 정확도와 R^2값 이외에 분류와 회귀 성능을 측정하는 다른 방법 그리고 지도 학습 모델의 매개변수를 조정하는 데 유용한 그리드 서치에 관해서 학습하겠습니다.교차 검증교차 검증은 훈련 세트와 테스트 세트로 나누는 것보다 더 안정적인 평가 방법입니다.데이터를 여러번 반복해 나누고 여러 모델을 학습합니다.k-fold 교차검증은 k라는 fold를 지정해 거의 비슷한 크기를 가진 부분 집합 k개로 나눕니다.첫 번째 폴드를 테스트 세트로 사용하고 나머지 폴드를 훈련 세트로 사용하여 학습합니다.이렇게 k 번을 반복하여 분할마다 정확도를 측정하여 k개의 정확도를 알 수 있습니다.scikit-learn 교차 검증model_selection 모듈의 cross_val_score 함수로 구현됩니다.cross_val_score 함수의 매개변수는 평가하려는 모델과 훈련 데이터, 타깃 레이블입니다.from sklearn.model_selection import cross_val_scorefrom sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionscores = cross_val_score(logreg, iris.data, iris.target, cv=10)다음과 같은 방식으로 평가할 수 있습니다.cv 매개변수는 fold의 개수를 지정할 수 있습니다.정확도를 간단하게 나타내려면 평균을 사용하면 됩니다.모델이 폴드에 매우 의존적이거나 데이터셋이 작으면 정확도의 차이가 커집니다.cross_validate는 분할마다 훈련과 테스트에 걸린 시간을 담은 딕셔너리를 반환하고 테스트 점수와 훈련 점수도 얻을 수 있습니다.교차 검증의 장단점train_test_split은 상황에 따라 테스트 세트는 너무 쉬운 데이터만 들어갈 수 있지만 교차 검증을 사용하면 각 샘플에 정확하게 한 번씩 들어가기 때문에 한 번씩은 테스트 세트가 될 수 있습니다.또한 훈련 데이터에 얼마나 민감한지 알 수 있으며 train_data에 더 많은 데이터를 학습할 수 있습니다.하지만 모델을 k개 만들어서 학습하므로 연산 비용이 k배 늘어나게 됩니다.계층별 k-fold 교차 검증과 그 외 전략k-fold는 한 분할에 같은 데이터가 몰리게 된다면 분할에 따라 너무 높거나 너무 낮게 나올 수 있습니다.따라서 stratified k-fold를 통해 폴드 안의 클래스 비율이 전체 데이터 셋의 클래스 비율과 같도록 데이터를 나눌 수 있습니다.교차 검증 상세 옵션from sklearn.model_selection import KFoldkfold = KFold(n_splits=5)cross_val_score(logreg, iris.data, iris.target, cv=kfold)앞서 cv에 fold 개수를 지정하지만 scikit-learn에서는 KFold(n_splits=n)을 통해서 fold의 개수를 지정해줍니다.KFold의 shuffle 매개변수를 True로 만들어 데이터를 섞어서 샘플의 순서를 바꿀 수 있습니다.random_state를 통해 똑같으 작업을 재현할 수 있으며 사용하지 않는다면 매번 다른 결과가 나옵니다.LOOCV (Leave-One-Out cross-validation)폴드 하나에 샘플 하나만 들어 있는 교차 검증으로 생각할 수 있습니다.각 반복에서 하나의 데이터 포인트를 선택해 테스트 세트로 사용합니다.from sklearn.model_selection import LeaveOneOutloo = LeaveOneOut()scores = cross_val_score(logreg, iris.data, iris.target, cv=loo)임의 분할 교차 검증train_size만큼의 포인트로 훈련 세트를 만들고 훈련 세트와 중첩되지 않은 test_Size만큼의 포인트로 테스트 세트를 만들도록 분할합니다.n_splits 만큼 반복됩니다.따라서 훈련 세트나 테스트 세트의 크기와 독립적으로 조절할 때 유용합니다.또한 데이터의 일부만 사용하게 되기 때문에 부분 샘플링 하게 됩니다.계층별 버전으로 사용한다면 StratifiedShuffleSplit을 사용하면 됩니다.from sklearn.model_selection import ShuffleSplitshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)scores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)그룹별 교차 검증Groups를 통해 훈련 세트와 테스트 세트에서 분리되지 않아야할 그룹을 지정할 수 있습니다.from sklearn.model_selection import GroupKFoldgroups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]scores = cross_val_score(logreg, X, y, groups=groups, cv=GroupKFold(n_splits=3))반복 교차 검증RepeatedKFold, RepeatedStratifiedKFlold를 통해 반복 교차 검증을 할 수 있습니다.회귀에서는 RepeatedKFold, 분류에서는 RepeatedStratifiedKFold를 사용합니다.KFold와 StratifiedKFold를 통해 분할합니다.n_splits를 통해 분할 폴드 수를, n_repeats를 통해 반복 횟수를 지정합니다.from sklearn.model_selsection import RepeatedStratifiedKFoldrskfold=RepeatedStratifiedKFold(random_state=42)그리드 서치매개변수를 튜닝하여 일반화 성능을 개선합니다.관심있는 매개변수를 대상으로 가능한 모든 조합을 시도해보는 것입니다.매개변수를 조정하기 위해 사용한 테스트 세트가 아닌 새로운 테스트 세트를 만들어 모델을 평가해야 합니다.따라서 처음에 훈련 세트를 통해 모델 순련, 검증 세트를 통해 매개변수 선택, 테스트 세트를 통해 모델 평가를 하는 방법을 사용합니다.매개변수를 선택한 후 훈련 세트와 검증 세트를 합해 모델을 다시 만듭니다.그리드 서치에서도 교차 검증을 이용해 각 매개변수 조합의 성능을 평가할 수 있습니다.best_params_를 통해 선택한 매개변수를 알 수 있습니다.best_score_을 통해 최상의 교차 검증 정확도를 알 수 있습니다.best_estimator_을 통해서 속성을 얻을 수 있습니다.교차 검증의 결과는 cv_results_ 속성에 담겨 있습니다.from sklearn.model_selection import GridSearchCVfrom sklearn.svm import SVCgrid_search = GridSearchCV(SVC(), param_grid, cv=5, return_train_score=True)중첩 교차 검증원본 데이터를 교차 검증 분할 방식을 사용하여 나누는 것을 중첩 교차 검증이라고 합니다.바깥쪽 루프에서 데이터를 훈련 세트와 테스트 세트로 나눈 뒤 각 훈련 세트에 대해 그리드 서치를 실행합니다.그 다음 분할된 테스트 세트의 점수를 최적 매개변수 설정을 사용해 각각 측정합니다.따라서 테스트 점수 목록을 만들어줍니다.최적 매개변수가 모델을 얼마나 잘 일반화 시키는지 확인하기 위한 방법입니다.그리드 서치는 n_jobs를 통해 병렬화하여 연산을 빠르게 할 수 있습니다.평가 지표와 측정이진 분류의 평가 지표정확도가 높은 분류가 반드시 좋은 분류라고 할 수 없습니다.어떤 경우에는 1종오류가 없어야하는 경우도 있습니다. (암 통계에서의 거짓 음성)불균형 데이터셋한 클래스가 다른 것보다 훨씬 많은 데이터셋오차 행렬classfication_report를 통해서 여러가지 오차 행렬 결과 요약을 볼 수 있습니다.각 분석에 중요성에 따라 사용하는 성능 지표가 달라집니다.정밀도-재현율 곡선과 ROC 곡선문제를 더 잘 이해하기 위해 정밀도-재현율 곡선을 사용합니다.from sklearn.metrics import precision_recall_curveprecision, recall, thresholds = precision_recall_curve(    y_test, svc.decision_function(X_test))곡선이 오른쪽 위로 갈수록 좋은 분류기입니다.오른 쪽 위 지점은 한 임계값에서 정밀도와 재현율이 모두 높은 곳입니다.임계값이 높을수록 정밀도는 높아지는 쪽으로 이동하고 재현율은 낮아집니다.f1_score은 정밀도-재현율 곡선의 기본 임계값에 대한 점수입니다.ROC 곡선TPR과 FPR을 나타냅니다.TPR은 재현율을 나타냅니다.ROC 곡선은 왼쪽 위에 가까울 수록 이상적입니다.FPR이 낮게 유지되면서 재현율이 높은 분류기가 좋은 것입니다.곡선 아래 면적을 AUC라고 하며 roc_auc_score 함수로 계산합니다.다중 분류의 평가 지표클래스가 불균형할 때는 좋은 평가 방법이 되지 못합니다.다중 클래스용 f1-score이 있습니다.한 클래스를 양성으로 잡고 나머지 클래스는 음성으로 간주하고 f1-score을 계산합니다.‘macro’ 평균은 클래스별 f1-score에 가중치를 주지않습니다.‘weighted’ 평균은 클래스별 샘플 수로 가중치를 두어 f1-score 점수의 평균을 계싼합니다.‘micro’평균은 모든 클래스의 FP, FN, TP의 총 수를 파악한 뒤 정밀도, 재현율, f1-score로 이 수치를 계산합니다.회귀의 평가 지표회귀에서는 R^2로 평가하는 것이 가장 좋습니다.",
        "url": "/study-ML6"
    }
    ,
    
    "programming-baekjoon8": {
        "title": "백준 (8) &lt;br&gt; (10845,1158,1966,2164, &lt;br&gt; 11866,18258)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)큐에 관련된 6문제를 풀어보았다.https://www.acmicpc.net/problemset?sort=ac_desc&amp;algo=723190번 뱀을 풀지 못했다…큐란?큐는 선입선출의 개념이다.10845번 큐https://www.acmicpc.net/problem/10845이전에 풀었던 스택에서와 유사한 문제이다.먼저 명령어를 입력받았고 그 명령에 ‘push’가 있다면 띄어쓰기를 기준으로 나누어 que_list에 추가해주었다.제일 앞에 숫자를 추출하는 것은 list의 index를 사용하였다.que_list=[]for i in range(int(input())):    command=input()    if 'push' in command:        a,b=command.split()        que_list.append(b)    if 'front' in command:        if len(que_list)==0:            print('-1')        else:            print(que_list[0])    if 'back' in command:        if len(que_list)==0:            print('-1')        else:            print(que_list[-1])    if 'pop' in command:        if len(que_list)==0:            print('-1')        else:            a=que_list[0]            que_list.remove(a)            print(a)    if 'size' in command:        print(len(que_list))    if 'empty' in command:        if len(que_list)==0:            print('1')        else:            print('0')15push 1push 2front1back2size2empty0pop1pop2pop-1size0empty1pop-1push 3empty0front31158번 요세푸스 문제https://www.acmicpc.net/problem/1158먼저 N개의 숫자가 들어가있는 que 리스트를 만들고 que에서 빠질 숫자들을 저장하는 result 리스트를 만들었습니다.num을 통해 que의 인덱싱을 조절해줍니다.하나의 숫자가 que에서 빠지면 뒤에 숫자가 그 빠진 위치에 들어오기 때문에 num은 K-1 만큼 더해줬습니다.num이 que의 인덱스 범위에 벗어나게 되면 처음으로 돌아가야하고 그 숫자는 num/len(que)의 나머지가 됩니다.그렇게 반복문으로 나온 순서대로 result에 append 시켜주면 문제는 해결됩니다.마지막으로 ‘[]’가 아닌 ‘&lt;&gt;’로 묶여있기 때문에 리스트 전체를 문자열로 만들어서 ‘[]’를 ‘&lt;&gt;’로 replace를 통해 바꿔주었습니다.N,K=map(int,input().split())result=[]num=0que=[]for i in range(1,N+1):    que.append(i)for i in range(N):    num=num+K-1    if num &gt;=len(que):        num=num%len(que)    result.append(que.pop(num))result=str(result)result=result.replace('[','&lt;')result=result.replace(']','&gt;')print(result)7 3&lt;3, 6, 2, 7, 5, 1, 4&gt;1966번 프린터 큐https://www.acmicpc.net/problem/1966목표하는 index 값을 ‘target’으로 바꿔서 처리하기 수월했다.pop으로 숫자를 꺼내서 뒤에 append를 추가하였다.test_cases = int(input())for _ in range(test_cases):    n,m = list(map(int, input().split( )))    prior = list(map(int, input().split( )))    index = list(range(len(prior)))    index[m] = 'target'    order = 0        while True:        if prior[0]==max(prior):            order += 1            if index[0]=='target':                print(order)                break            else:                prior.pop(0)                index.pop(0)        else:            prior.append(prior.pop(0))            index.append(index.pop(0))31 0514 21 2 3 426 01 1 9 1 1 152164번 카드 2https://www.acmicpc.net/problem/2164deque를 이용해서 한번은 숫자를 없애고 한번은 뒤로 append해주고 남은 하나의 숫자를 출력하면 된다.import sysfrom collections import dequecard=deque()for i in range(int(input())):    card.append(i+1)    while len(card) != 1:    card.popleft()    a=card.popleft()    card.append(a)print(card[0])6411866번 요세푸스 문제 0https://www.acmicpc.net/problem/11866위에 문제와 같이 풀어도 됐다…N,K=map(int,input().split())result=[]num=0que=[]for i in range(1,N+1):    que.append(i)for i in range(N):    num=num+K-1    if num &gt;=len(que):        num=num%len(que)    result.append(que.pop(num))result=str(result)result=result.replace('[','&lt;')result=result.replace(']','&gt;')print(result)7 3&lt;3, 6, 2, 7, 5, 1, 4&gt;3190번 뱀https://www.acmicpc.net/problem/3190n=int(input())board=[[0]*n for i in range(n)]print(board)6[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]K=int(input())for i in range(K):    a,b=map(int,input().split())    board[a-1][b-1]=1print(board)33 42 55 3[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0]]18258번 큐 2https://www.acmicpc.net/problem/18258위에서 list와 인덱싱을 통해 처리했던 문제를 deque를 통해서 시간을 절약하는 문제이다.import sysfrom collections import dequeque_list=deque()for i in range(int(input())):    command=input()    if 'push' in command:        a,b=command.split()        que_list.append(b)    if 'front' in command:        if len(que_list)==0:            print('-1')        else:            print(que_list[0])    if 'back' in command:        if len(que_list)==0:            print('-1')        else:            print(que_list[-1])    if 'pop' in command:        if len(que_list)==0:            print('-1')        else:            print(que_list.popleft())    if 'size' in command:        print(len(que_list))    if 'empty' in command:        if len(que_list)==0:            print('1')        else:            print('0')15push 1push 2front1back2size2empty0pop1pop2pop-1size0empty1pop-1push 3empty0front3",
        "url": "/programming-baekjoon8"
    }
    ,
    
    "programming-kaggle3": {
        "title": "캐글 (3) &lt;br&gt; 타이타닉 생존자 EDA 부터 분류까지",
            "author": "keonju",
            "category": "",
            "content": "kaggle 관련 글    캐글 (1) Simple Matplotlib &amp; Visualization Tips 공부하기    캐글 (2) 타이타닉 튜토리얼 1,2 공부하기    캐글 (3) 타이타닉 생존자 EDA 부터 분류까지https://www.kaggle.com/ash316/eda-to-prediction-dietanic/notebook캐글의 타이타닉 데이터를 통한 EDA부터 예측까지 필사를 진행해보았습니다.데이터 탐색import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsplt.style.use('fivethirtyeight')import warningswarnings.filterwarnings('ignore')%matplotlib inlinedata=pd.read_csv('train.csv')data.head()                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      7.2500      NaN      S              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      71.2833      C85      C              2      3      1      3      Heikkinen, Miss. Laina      female      26.0      0      0      STON/O2. 3101282      7.9250      NaN      S              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      35.0      1      0      113803      53.1000      C123      S              4      5      0      3      Allen, Mr. William Henry      male      35.0      0      0      373450      8.0500      NaN      S      data.isnull().sum()PassengerId      0Survived         0Pclass           0Name             0Sex              0Age            177SibSp            0Parch            0Ticket           0Fare             0Cabin          687Embarked         2dtype: int64얼마나 많은 사람들이 살아남았나?f,ax=plt.subplots(1,2,figsize=(18,8))data['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)ax[0].set_title('Survived')ax[0].set_ylabel('')sns.countplot('Survived',data=data,ax=ax[1])ax[1].set_title('Survived')plt.show()train_set에서 891명의 승객 중, 350명이 살아남았습니다.  범주형 변수Sex, Embarked  순서형 변수PClass  연속형 변수Age특징 분석# Sex -&gt; Categorical Featuredata.groupby(['Sex','Survived'])['Survived'].count()Sex     Survivedfemale  0            81        1           233male    0           468        1           109Name: Survived, dtype: int64f,ax=plt.subplots(1,2,figsize=(18,8))data[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])ax[0].set_title('Survived vs Sex')sns.countplot('Sex',hue='Survived',data=data,ax=ax[1])ax[1].set_title('Sex:Survived vs Dead')plt.show()&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;배에 탄 남자의 수가 여자의 수보다 많지만 생존율은 여성이 훨씬 높습니다.# Pclass -&gt; Ordinal Featurepd.crosstab(data.Pclass,data.Survived,margins=True).style.background_gradient(cmap='summer_r')            Survived        0        1        All                Pclass                                                                    1                        80                        136                        216                                                2                        97                        87                        184                                                3                        372                        119                        491                                                All                        549                        342                        891                f,ax=plt.subplots(1,2,figsize=(18,8))data['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])ax[0].set_title('Number Of Passengers By Pclass')ax[0].set_ylabel('Count')sns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])ax[1].set_title('Pclass:Survived vs Dead')plt.show()Pclass=3인 승객 수가 훨씬 많았지만 생존율은 매우 낮은 모습을 보이고 있습니다.따라서 부유한 사람의 생존율이 더 높다고 할 수 있습니다.pd.crosstab([data.Sex,data.Survived],data.Pclass,margins=True).style.background_gradient(cmap='summer_r')                    Pclass        1        2        3        All                Sex        Survived                                                                            female                        0                        3                        6                        72                        81                                                        1                        91                        70                        72                        233                                                male                        0                        77                        91                        300                        468                                                        1                        45                        17                        47                        109                                                All                                                216                        184                        491                        891                sns.factorplot('Pclass','Survived',hue='Sex',data=data)plt.show()CrossTab과 FactorPlot을 통해 성별과 Pclass간의 관계를 볼 수 있습니다.# Age--&gt; Continous Featureprint('Oldest Passenger was of:',data['Age'].max(),'Years')print('Youngest Passenger was of:',data['Age'].min(),'Years')print('Average Age on the ship:',data['Age'].mean(),'Years')Oldest Passenger was of: 80.0 YearsYoungest Passenger was of: 0.42 YearsAverage Age on the ship: 29.69911764705882 Yearsf,ax=plt.subplots(1,2,figsize=(18,8))sns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[0])ax[0].set_title('Pclass and Age vs Survived')ax[0].set_yticks(range(0,110,10))sns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[1])ax[1].set_title('Sex and Age vs Survived')ax[1].set_yticks(range(0,110,10))plt.show()Pclass에 따라 연령대의 분포를 볼 수 있습니다.Sex와 Age의 관계를 본다면 연령층이 낮으면 성별과 상관없이 우수한 생존율을 볼 수 있습니다.# 정규식을 통한 Initial 추출data['Initial']=0for i in data:    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.')pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r')            Initial        Capt        Col        Countess        Don        Dr        Jonkheer        Lady        Major        Master        Miss        Mlle        Mme        Mr        Mrs        Ms        Rev        Sir                Sex                                                                                                                                                                                    female                        0                        0                        1                        0                        1                        0                        1                        0                        0                        182                        2                        1                        0                        125                        1                        0                        0                                                male                        1                        2                        0                        1                        6                        1                        0                        2                        40                        0                        0                        0                        517                        0                        0                        6                        1                Miss, Mr뿐만 아니라 다른 명칭으로 적힌 사람들이 많다.따라서 이에 대한 처리가 필요하다.data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)data.groupby('Initial')['Age'].mean()InitialMaster     4.574167Miss      21.860000Mr        32.739609Mrs       35.981818Other     45.888889Name: Age, dtype: float64data.loc[(data.Age.isnull())&amp;(data.Initial=='Mr'),'Age']=33data.loc[(data.Age.isnull())&amp;(data.Initial=='Mrs'),'Age']=36data.loc[(data.Age.isnull())&amp;(data.Initial=='Master'),'Age']=5data.loc[(data.Age.isnull())&amp;(data.Initial=='Miss'),'Age']=22data.loc[(data.Age.isnull())&amp;(data.Initial=='Other'),'Age']=46data.Age.isnull().any()Falsef,ax=plt.subplots(1,2,figsize=(20,10))data[data['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')ax[0].set_title('Survived= 0')x1=list(range(0,85,5))ax[0].set_xticks(x1)data[data['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')ax[1].set_title('Survived= 1')x2=list(range(0,85,5))ax[1].set_xticks(x2)plt.show()연령의 분포를 확인할 수 있으며 가장 많은 사망자를 가진 연령층도 파악할 수 있습니다.sns.factorplot('Pclass','Survived',col='Initial',data=data)plt.show()# Embarked--&gt; Categorical Valuepd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=True).style.background_gradient(cmap='summer_r')                    Sex        female        male        All                        Survived        0        1        0        1                        Embarked        Pclass                                                                                    C                        1                        1                        42                        25                        17                        85                                                        2                        0                        7                        8                        2                        17                                                        3                        8                        15                        33                        10                        66                                                Q                        1                        0                        1                        1                        0                        2                                                        2                        0                        2                        1                        0                        3                                                        3                        9                        24                        36                        3                        72                                                S                        1                        2                        46                        51                        28                        127                                                        2                        6                        61                        82                        15                        164                                                        3                        55                        33                        231                        34                        353                                                All                                                81                        231                        468                        109                        889                # Chances for Survival by Port Of Embarkation¶sns.factorplot('Embarked','Survived',data=data)fig=plt.gcf()fig.set_size_inches(5,3)plt.show()f,ax=plt.subplots(2,2,figsize=(20,15))sns.countplot('Embarked',data=data,ax=ax[0,0])ax[0,0].set_title('No. Of Passengers Boarded')sns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])ax[0,1].set_title('Male-Female Split for Embarked')sns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])ax[1,0].set_title('Embarked vs Survived')sns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])ax[1,1].set_title('Embarked vs Pclass')plt.subplots_adjust(wspace=0.2,hspace=0.5)plt.show()S에서 탑승한 사람이 가장 많습니다.그리고 이 사람들은 대부분 3 Class인 것을 확인할 수 있습니다.sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data)plt.show()PClass와 Embarked에 따른 생존률을 파악할 수 있습니다.data['Embarked'].fillna('S',inplace=True)data.Embarked.isnull().any()False# SibSip--&gt;Discrete Featurepd.crosstab([data.SibSp],data.Survived).style.background_gradient(cmap='summer_r')            Survived        0        1                SibSp                                                            0                        398                        210                                                1                        97                        112                                                2                        15                        13                                                3                        12                        4                                                4                        15                        3                                                5                        5                        0                                                8                        7                        0                sns.barplot('SibSp','Survived',data=data)&lt;AxesSubplot:xlabel='SibSp', ylabel='Survived'&gt;sns.factorplot('SibSp','Survived',data=data)&lt;seaborn.axisgrid.FacetGrid at 0x2b48e376fa0&gt;pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap='summer_r')            Pclass        1        2        3                SibSp                                                                    0                        137                        120                        351                                                1                        71                        55                        83                                                2                        5                        8                        15                                                3                        3                        1                        12                                                4                        0                        0                        18                                                5                        0                        0                        5                                                8                        0                        0                        7                가족 수가 많은 class는 3 class인 것을 알 수 있으며 가족 수가 적을수록 더 좋은 생존률을 볼 수 있습니다.Parchpd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r')            Pclass        1        2        3                Parch                                                                    0                        163                        134                        381                                                1                        31                        32                        55                                                2                        21                        16                        43                                                3                        0                        2                        3                                                4                        1                        0                        3                                                5                        0                        0                        5                                                6                        0                        0                        1                sns.barplot('Parch','Survived',data=data)&lt;AxesSubplot:xlabel='Parch', ylabel='Survived'&gt;sns.factorplot('Parch','Survived',data=data)&lt;seaborn.axisgrid.FacetGrid at 0x2b48ea62160&gt;# Fare--&gt; Continous Featureprint('Highest Fare was:',data['Fare'].max())print('Lowest Fare was:',data['Fare'].min())print('Average Fare was:',data['Fare'].mean())Highest Fare was: 512.3292Lowest Fare was: 0.0Average Fare was: 32.2042079685746f,ax=plt.subplots(1,3,figsize=(20,8))sns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])ax[0].set_title('Fares in Pclass 1')sns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])ax[1].set_title('Fares in Pclass 2')sns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])ax[2].set_title('Fares in Pclass 3')plt.show()Pclass에서는 1등석 승객이 되면 생존 확률이 높은 것을 알 수 있습니다.나이는 어리면 생존율이 높다는 것을 알 수 있습니다.가족이 있다면 적은 가족일 때 생존율이 높은 것을 알 수 있습니다.특징들간의 상관관계sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()--&gt;correlation matrixfig=plt.gcf()fig.set_size_inches(10,8)plt.show()히트맵에서 상관관계를 볼 수 있습니다.양의 상관 관계와 음의 상관 관계가 있습니다.상관 관계가 높다면 둘은 multicolinearity라고 불리는 거의 동일한 정보를 포함하고 있다는 것을 알 수 있습니다.여기서는 SibSp와 Parch가 가장 높지만 0.41이기 때문에 multicolinearity가 없다고 할 수 있습니다.데이터 클리닝데이터를 모두 중요하게 사용할 필요는 없기 때문에 관찰을 통해 새로운 데이터로 변경하는 것을 할 수 있습니다.data['Age_band']=0data.loc[data['Age']&lt;=16,'Age_band']=0data.loc[(data['Age']&gt;16)&amp;(data['Age']&lt;=32),'Age_band']=1data.loc[(data['Age']&gt;32)&amp;(data['Age']&lt;=48),'Age_band']=2data.loc[(data['Age']&gt;48)&amp;(data['Age']&lt;=64),'Age_band']=3data.loc[data['Age']&gt;64,'Age_band']=4data.head(2)                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked      Initial      Age_band                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      7.2500      NaN      S      Mr      1              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      71.2833      C85      C      Mrs      2      data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')                    Age_band                                            1                        382                                                2                        325                                                0                        104                                                3                        69                                                4                        11                sns.factorplot('Age_band','Survived',data=data,col='Pclass')plt.show()가족 수와 홀로 온 사람들 사이의 생존율을 볼 수 있습니다.data['Family_Size']=0data['Family_Size']=data['Parch']+data['SibSp']#family sizedata['Alone']=0data.loc[data.Family_Size==0,'Alone']=1sns.factorplot('Family_Size','Survived',data=data)&lt;seaborn.axisgrid.FacetGrid at 0x2b48f8cf790&gt;sns.factorplot('Alone','Survived',data=data)&lt;seaborn.axisgrid.FacetGrid at 0x2b48e966b80&gt;sns.factorplot('Alone','Survived',data=data,hue='Sex',col='Pclass')plt.show()Fare_Range 범주화data['Fare_Range']=pd.qcut(data['Fare'],4)data.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')                    Survived                Fare_Range                                                    (-0.001, 7.91]                        0.197309                                                (7.91, 14.454]                        0.303571                                                (14.454, 31.0]                        0.454955                                                (31.0, 512.329]                        0.581081                data['Fare_cat']=0data.loc[data['Fare']&lt;=7.91,'Fare_cat']=0data.loc[(data['Fare']&gt;7.91)&amp;(data['Fare']&lt;=14.454),'Fare_cat']=1data.loc[(data['Fare']&gt;14.454)&amp;(data['Fare']&lt;=31),'Fare_cat']=2data.loc[(data['Fare']&gt;31)&amp;(data['Fare']&lt;=513),'Fare_cat']=3sns.factorplot('Fare_cat','Survived',data=data,hue='Sex')plt.show()문자열을 숫자로 범주화하기data['Sex'].replace(['male','female'],[0,1],inplace=True)data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)data['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})fig=plt.gcf()fig.set_size_inches(18,15)plt.xticks(fontsize=14)plt.yticks(fontsize=14)plt.show()예측 모델 만들기from sklearn.linear_model import LogisticRegressionfrom sklearn import svmfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.naive_bayes import GaussianNB from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_splitfrom sklearn import metrics # 정확도 확인from sklearn.metrics import confusion_matrixtrain,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])train_X=train[train.columns[1:]]train_Y=train[train.columns[:1]]test_X=test[test.columns[1:]]test_Y=test[test.columns[:1]]X=data[data.columns[1:]]Y=data['Survived']SVMmodel=svm.SVC(kernel='rbf',C=1,gamma=0.1)model.fit(train_X,train_Y)prediction1=model.predict(test_X)print('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))Accuracy for rbf SVM is  0.835820895522388Linear Support Vector Machinemodel=svm.SVC(kernel='linear',C=0.1,gamma=0.1)model.fit(train_X,train_Y)prediction2=model.predict(test_X)print('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))Accuracy for linear SVM is 0.8171641791044776Logistic Regressionmodel = LogisticRegression()model.fit(train_X,train_Y)prediction3=model.predict(test_X)print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))The accuracy of the Logistic Regression is 0.8134328358208955Decision Treemodel=DecisionTreeClassifier()model.fit(train_X,train_Y)prediction4=model.predict(test_X)print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))The accuracy of the Decision Tree is 0.8022388059701493KNNmodel=KNeighborsClassifier() model.fit(train_X,train_Y)prediction5=model.predict(test_X)print('The accuracy of the KNN is',metrics.accuracy_score(prediction5,test_Y))The accuracy of the KNN is 0.832089552238806N의 개수에 따른 정확도 변화a_index=list(range(1,11))a=pd.Series()x=[0,1,2,3,4,5,6,7,8,9,10]for i in list(range(1,11)):    model=KNeighborsClassifier(n_neighbors=i)     model.fit(train_X,train_Y)    prediction=model.predict(test_X)    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))plt.plot(a_index, a)plt.xticks(x)fig=plt.gcf()fig.set_size_inches(12,6)plt.show()print('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())Accuracies for different values of n are: [0.75746269 0.79104478 0.80970149 0.80223881 0.83208955 0.81716418 0.82835821 0.83208955 0.8358209  0.83208955] with the max value as  0.835820895522388Gaussian Naive Bayesmodel=GaussianNB()model.fit(train_X,train_Y)prediction6=model.predict(test_X)print('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))The accuracy of the NaiveBayes is 0.8134328358208955Random Forestsmodel=RandomForestClassifier(n_estimators=100)model.fit(train_X,train_Y)prediction7=model.predict(test_X)print('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))The accuracy of the Random Forests is 0.8246268656716418Cross Validataion교차 검증을 통해서 확인을 해야합니다.K-Fold 교차분석을 사용합니다.shuffle=True를 넣어줘야합니다.from sklearn.model_selection import KFoldfrom sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import cross_val_predictkfold = KFold(n_splits=10, random_state=22, shuffle=True) xyz=[]accuracy=[]std=[]classifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']models=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]for i in models:    model = i    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")    cv_result=cv_result    xyz.append(cv_result.mean())    std.append(cv_result.std())    accuracy.append(cv_result)new_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       new_models_dataframe2                  CV Mean      Std                  Linear Svm      0.784607      0.057841              Radial Svm      0.828377      0.057096              Logistic Regression      0.799176      0.040154              KNN      0.808140      0.040287              Decision Tree      0.805868      0.045909              Naive Bayes      0.795843      0.054861              Random Forest      0.817104      0.041512      plt.subplots(figsize=(12,6))box=pd.DataFrame(accuracy,index=[classifiers])box.T.boxplot()&lt;AxesSubplot:&gt;new_models_dataframe2['CV Mean'].plot.barh(width=0.8)plt.title('Average CV Mean Accuracy')fig=plt.gcf()fig.set_size_inches(8,5)plt.show()heatmap을 통해 모델별 정확도 확인하기f,ax=plt.subplots(3,3,figsize=(12,10))y_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')ax[0,0].set_title('Matrix for rbf-SVM')y_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')ax[0,1].set_title('Matrix for Linear-SVM')y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')ax[0,2].set_title('Matrix for KNN')y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')ax[1,0].set_title('Matrix for Random-Forests')y_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')ax[1,1].set_title('Matrix for Logistic Regression')y_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')ax[1,2].set_title('Matrix for Decision Tree')y_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')ax[2,0].set_title('Matrix for Naive Bayes')plt.subplots_adjust(hspace=0.2,wspace=0.2)plt.show()하이퍼 파라미터 튜닝SVM과 Random Forest의 파라미터 튜닝하기SVMfrom sklearn.model_selection import GridSearchCVC=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]gamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]kernel=['rbf','linear']hyper={'kernel':kernel,'C':C,'gamma':gamma}gd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)gd.fit(X,Y)print(gd.best_score_)print(gd.best_estimator_)Fitting 5 folds for each of 240 candidates, totalling 1200 fits0.8282593685267716SVC(C=0.4, gamma=0.3)Random Forestn_estimators=range(100,1000,100)hyper={'n_estimators':n_estimators}gd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)gd.fit(X,Y)print(gd.best_score_)print(gd.best_estimator_)Fitting 5 folds for each of 9 candidates, totalling 45 fits0.819327098110602RandomForestClassifier(n_estimators=300, random_state=0)Ensembling1) Voting Classifier2) Bagging3) Boosting# Voting Classifierfrom sklearn.ensemble import VotingClassifierensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),                                              ('LR',LogisticRegression(C=0.05)),                                              ('DT',DecisionTreeClassifier(random_state=0)),                                              ('NB',GaussianNB()),                                              ('svm',svm.SVC(kernel='linear',probability=True))                                             ],                        voting='soft').fit(train_X,train_Y)print('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))cross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")print('The cross validated score is',cross.mean())The accuracy for ensembled model is: 0.8246268656716418The cross validated score is 0.8249188514357053# Bagging - KNNfrom sklearn.ensemble import BaggingClassifiermodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)model.fit(train_X,train_Y)prediction=model.predict(test_X)print('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')print('The cross validated score for bagged KNN is:',result.mean())The accuracy for bagged KNN is: 0.835820895522388The cross validated score for bagged KNN is: 0.8160424469413232# Bagging DecisionTreemodel=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)model.fit(train_X,train_Y)prediction=model.predict(test_X)print('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')print('The cross validated score for bagged Decision Tree is:',result.mean())The accuracy for bagged Decision Tree is: 0.8208955223880597The cross validated score for bagged Decision Tree is: 0.8171410736579275# AdaBoostfrom sklearn.ensemble import AdaBoostClassifierada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)result=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')print('The cross validated score for AdaBoost is:',result.mean())The cross validated score for AdaBoost is: 0.8249188514357055# Stochastic Gradient Boostingfrom sklearn.ensemble import GradientBoostingClassifiergrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)result=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')print('The cross validated score for Gradient Boosting is:',result.mean())The cross validated score for Gradient Boosting is: 0.8115230961298376# XGBoostimport xgboost as xgxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)result=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')print('The cross validated score for XGBoost is:',result.mean())[20:33:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.[20:33:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.The cross validated score for XGBoost is: 0.8160299625468165# Hyper-Parameter Tuning for AdaBoostn_estimators=list(range(100,1100,100))learn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]hyper={'n_estimators':n_estimators,'learning_rate':learn_rate}gd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)gd.fit(X,Y)print(gd.best_score_)print(gd.best_estimator_)Fitting 5 folds for each of 120 candidates, totalling 600 fits0.8293892411022534AdaBoostClassifier(learning_rate=0.1, n_estimators=100)# Confusion Matrix for the Best Modelada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)result=cross_val_predict(ada,X,Y,cv=10)sns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')plt.show()Feature Importancef,ax=plt.subplots(2,2,figsize=(15,12))model=RandomForestClassifier(n_estimators=500,random_state=0)model.fit(X,Y)pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])ax[0,0].set_title('Feature Importance in Random Forests')model=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)model.fit(X,Y)pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')ax[0,1].set_title('Feature Importance in AdaBoost')model=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)model.fit(X,Y)pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')ax[1,0].set_title('Feature Importance in Gradient Boosting')model=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)model.fit(X,Y)pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')ax[1,1].set_title('Feature Importance in XgBoost')plt.show()[20:39:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.",
        "url": "/programming-kaggle3"
    }
    ,
    
    "study-ml5": {
        "title": "머신러닝 정리 (5) &lt;br&gt; 데이터 표현과 특성 공학",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학  범주형 변수          원-핫 인코딩        OneHotEncoder와 ColumnTransformer: scikit-learn으로 범주형 변수 다루기  make_column_transformer로 간편하게 ColumnTransformer 만들기  구간 분할, 이산화 그리고 선형 모델, 트리 모델  상호작용과 다항식  일변량 비선형 변환  특성 자동 선택  일변량 통계(ANOVA)  모델 기반 특성 선택  반복적 특성 선택  전문가 지식 활용머신러닝 정리 (5) 데이터 표현과 특성 공학본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python범주형 변수우리가 실제 사용하는 데이터의 대부분은 범주형 변수입니다.이 데이터에서는 근로자의 수입이 50000달러를 초과하는지, 이하인지 예측하려고 합니다.사용된 데이터셋은 다음과 같습니다.# pip install mglearnimport pandas as pdfrom preamble import *import os# 이 파일은 열 이름을 나타내는 헤더가 없으므로 header=None으로 지정하고# \"names\" 매개변수로 열 이름을 제공합니다data = pd.read_csv(    os.path.join(mglearn.datasets.DATA_PATH, \"adult.data\"), header=None, index_col=False,    names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',           'marital-status', 'occupation', 'relationship', 'race', 'gender',           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',           'income'])# 예제를 위해 몇개의 열만 선택합니다data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week',             'occupation', 'income']]# IPython.display 함수는 주피터 노트북을 위해 포맷팅된 출력을 만듭니다display(data.head())                  age      workclass      education      gender      hours-per-week      occupation      income                  0      39      State-gov      Bachelors      Male      40      Adm-clerical      &lt;=50K              1      50      Self-emp-not-inc      Bachelors      Male      13      Exec-managerial      &lt;=50K              2      38      Private      HS-grad      Male      40      Handlers-cleaners      &lt;=50K              3      53      Private      11th      Male      40      Handlers-cleaners      &lt;=50K              4      28      Private      Bachelors      Female      40      Prof-specialty      &lt;=50K      원-핫 인코딩범주형 변수를 0 또는 1 값을 가진 하나 이상의 새로운 특성으로 바꾼 것입니다.0과 1로 표현된 변수가 선형 이진 분류 공식에 적용할 수 있기 때문입니다.범주형 데이터 문자열 확인하기데이터셋을 읽은 뒤 범주형 데이터가 있는지 확인해보는 것이 좋습니다.print(data.gender.value_counts()) Male      21790 Female    10771Name: gender, dtype: int64get_dummies 함수를 사용해 쉽게 인코딩할 수 있습니다.data_dummies=pd.get_dummies(data)print(list(data_dummies.columns))['age', 'hours-per-week', 'workclass_ ?', 'workclass_ Federal-gov', 'workclass_ Local-gov', 'workclass_ Never-worked', 'workclass_ Private', 'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc', 'workclass_ State-gov', 'workclass_ Without-pay', 'education_ 10th', 'education_ 11th', 'education_ 12th', 'education_ 1st-4th', 'education_ 5th-6th', 'education_ 7th-8th', 'education_ 9th', 'education_ Assoc-acdm', 'education_ Assoc-voc', 'education_ Bachelors', 'education_ Doctorate', 'education_ HS-grad', 'education_ Masters', 'education_ Preschool', 'education_ Prof-school', 'education_ Some-college', 'gender_ Female', 'gender_ Male', 'occupation_ ?', 'occupation_ Adm-clerical', 'occupation_ Armed-Forces', 'occupation_ Craft-repair', 'occupation_ Exec-managerial', 'occupation_ Farming-fishing', 'occupation_ Handlers-cleaners', 'occupation_ Machine-op-inspct', 'occupation_ Other-service', 'occupation_ Priv-house-serv', 'occupation_ Prof-specialty', 'occupation_ Protective-serv', 'occupation_ Sales', 'occupation_ Tech-support', 'occupation_ Transport-moving', 'income_ &lt;=50K', 'income_ &gt;50K']data_dummies.head()                  age      hours-per-week      workclass_ ?      workclass_ Federal-gov      ...      occupation_ Tech-support      occupation_ Transport-moving      income_ &lt;=50K      income_ &gt;50K                  0      39      40      0      0      ...      0      0      1      0              1      50      13      0      0      ...      0      0      1      0              2      38      40      0      0      ...      0      0      1      0              3      53      40      0      0      ...      0      0      1      0              4      28      40      0      0      ...      0      0      1      0      5 rows × 46 columnsNumpy 배열로 바꾸고 타깃 값을 분리하여 학습 모델에 적용해야합니다.features = data_dummies.loc[:, 'age':'occupation_ Transport-moving']# NumPy 배열 추출X = features.valuesy = data_dummies['income_ &gt;50K'].values #타깃 값print(\"X.shape: {}  y.shape: {}\".format(X.shape, y.shape))X.shape: (32561, 44)  y.shape: (32561,)# 로지스틱 회귀 분석from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)logreg = LogisticRegression(max_iter=5000)logreg.fit(X_train, y_train)print(\"테스트 점수: {:.2f}\".format(logreg.score(X_test, y_test)))테스트 점수: 0.81숫자로 표현된 범주형 특성0부터 시작하는 연속된 자연수로 이루어진 데이터셋은 연속형인지 범주형인지 알기 힘듭니다.따라서 데이터셋이 의미하는 것이 무엇인지 확인해야 합니다.숫자로 표현된 범주형 데이터셋은 get_dummies를 사용하면 숫자 특성은 바뀌지 않습니다.따라서 columns 매개변수에 인코딩하고 싶은 열을 명시해야합니다.OneHotEncoder와 ColumnTransformer: scikit-learn으로 범주형 변수 다루기scikit-learn의 OneHotEncoder을 통해 모든 열에 인코딩을 수행할 수 있습니다.sparse=False는 희소 행렬이 아닌 넘파이 배열을 반환합니다.변환된 특성에 해당하는 원본 범주형 변수 이름을 알기 위해서는 get_feature_names 메소드를 사용합니다.ColumnTransformer의 fit, transform 메소드를 사용할 수 있습니다.from sklearn.preprocessing import OneHotEncoderfrom sklearn.compose import ColumnTransformerfrom sklearn.preprocessing import StandardScalerct = ColumnTransformer(    [(\"scaling\", StandardScaler(), ['age', 'hours-per-week']),     (\"onehot\", OneHotEncoder(sparse=False), ['workclass', 'education', 'gender', 'occupation'])])이렇게 변환된 모델을 LogisticRegression에 학습합니다.from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_split# income을 제외한 모든 열을 추출합니다data_features = data.drop(\"income\", axis=1)# 데이터프레임과 income을 분할합니다X_train, X_test, y_train, y_test = train_test_split(    data_features, data.income, random_state=0)ct.fit(X_train)X_train_trans = ct.transform(X_train)print(X_train_trans.shape)(24420, 44)44개의 특성이 만들어진 것은 위의 pd.get_dummies를 사용했을 때와 마찬가지이고 스케일 조정이 된 차이가 있습니다.logreg = LogisticRegression(max_iter=1000)logreg.fit(X_train_trans, y_train)X_test_trans = ct.transform(X_test)print(\"테스트 점수: {:.2f}\".format(logreg.score(X_test_trans, y_test)))테스트 점수: 0.81make_column_transformer로 간편하게 ColumnTransformer 만들기클래스 이름을 기반으로 자동으로 각 단계에 이름을 붙여주는 make_column_transformer 함수가 있습니다.변환된 데이터는 넘파이 배열이므로 열 이름을 가지고 있지 않습니다.from sklearn.compose import make_column_transformerct = make_column_transformer(    (StandardScaler(), ['age', 'hours-per-week']),    (OneHotEncoder(sparse=False), ['workclass', 'education', 'gender', 'occupation']))print(ct)ColumnTransformer(transformers=[('standardscaler', StandardScaler(),                                 ['age', 'hours-per-week']),                                ('onehotencoder', OneHotEncoder(sparse=False),                                 ['workclass', 'education', 'gender',                                  'occupation'])])구간 분할, 이산화 그리고 선형 모델, 트리 모델wave 데이터 셋을 통해선형 회귀 모델과 결정 트리 회귀를 비교해보았습니다.from sklearn.linear_model import LinearRegressionfrom sklearn.tree import DecisionTreeRegressorX, y = mglearn.datasets.make_wave(n_samples=120)line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)reg = DecisionTreeRegressor(min_samples_leaf=3).fit(X, y)plt.plot(line, reg.predict(line), label=\"Decision Tree\")reg = LinearRegression().fit(X, y)plt.plot(line, reg.predict(line), '--', label=\"linear regression\")plt.plot(X[:, 0], y, 'o', c='k')plt.ylabel(\"output\")plt.xlabel(\"input\")plt.legend(loc=\"best\")plt.show()선형 모델은 선형 관계로만 모델링하므로 특성이 하나일 땐 직선으로 나타납니다.구간 분할을 통해 한 특성을 여러 특성으로 나누면 강력한 선형 모델을 만들 수 있습니다.균일한 너비로 나누거나 데이터의 분위를 사용할 수도 있습니다.KBinsDiscretizer 클래스를 이용하면 됩니다.KBinsDiscretizer는 한 번에 여러 개의 특성에 적용할 수 있습니다.bin_edges_는 특성별로 경계값이 저장되어 있으며 길이가 1인 넘파이 배열이 출력됩니다.기본적으로 원-핫-인코딩을 적용하며 구간마다 새로운 특성이 생기는 희소 행렬을 만듭니다.n개의 구간을 지정하면 n개의 차원이 생성됩니다.from sklearn.preprocessing import KBinsDiscretizerkb = KBinsDiscretizer(n_bins=10, strategy='uniform')kb.fit(X)print(\"bin edges: \\n\", kb.bin_edges_)bin edges:  [array([-2.967, -2.378, -1.789, -1.2  , -0.612, -0.023,  0.566,  1.155,        1.744,  2.333,  2.921])]X_binned = kb.transform(X)X_binnedprint(X[:10])X_binned.toarray()[:10][[-0.753] [ 2.704] [ 1.392] [ 0.592] [-2.064] [-2.064] [-2.651] [ 2.197] [ 0.607] [ 1.248]]array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])원-핫-인코딩된 밀집 배열을 만들어보면 다음과 같습니다.kb = KBinsDiscretizer(n_bins=10, strategy='uniform', encode='onehot-dense')kb.fit(X)X_binned = kb.transform(X)  line_binned = kb.transform(line)reg = LinearRegression().fit(X_binned, y)plt.plot(line, reg.predict(line_binned), label='linear regression')reg = DecisionTreeRegressor(min_samples_split=3).fit(X_binned, y)plt.plot(line, reg.predict(line_binned), label='decision tree')plt.plot(X[:, 0], y, 'o', c='k')plt.vlines(kb.bin_edges_[0], -3, 3, linewidth=1, alpha=.2)plt.legend(loc=\"best\")plt.ylabel(\"output\")plt.xlabel(\"input\")plt.show(두 선이 완전히 겹치는 결과가 나왔습니다.선형 모델은 훨씬 유연해졌지만 결정 트리는 덜 유연해졌습니다.상호작용과 다항식상호작용과 다항식을 추가하면 특성을 더 풍부하게 만들 수 있습니다.X_combined = np.hstack([X, X_binned])print(X_combined.shape)  reg = LinearRegression().fit(X_combined, y)line_combined = np.hstack([line, line_binned])plt.plot(line, reg.predict(line_combined), label='Linear regression with original characteristics added')plt.vlines(kb.bin_edges_[0], -3, 3, linewidth=1, alpha=.2)plt.legend(loc=\"best\")plt.ylabel(\"Regression output\")plt.xlabel(\"Input characteristics\")plt.plot(X[:, 0], y, 'o', c='k')plt.show() (120, 11)기울기가 모두 같으므로 원본 특성을 곱한 값을 더해 20개의 특성을 만들어줍니다.X_product = np.hstack([X_binned, X * X_binned])print(X_product.shape)(120, 20)reg = LinearRegression().fit(X_product, y)line_product = np.hstack([line_binned, line * line_binned])plt.plot(line, reg.predict(line_product), label='Linear regression with original characteristics added')plt.vlines(kb.bin_edges_[0], -3, 3, linewidth=1, alpha=.2)plt.plot(X[:, 0], y, 'o', c='k')plt.ylabel(\"Regression output\")plt.xlabel(\"Input characteristics\")plt.legend(loc=\"best\")plt.show()preprocessiong 모듈의 PolynomialFeatures에 구현되어 있습니다.from sklearn.preprocessing import PolynomialFeatures# x ** 10까지 고차항을 추가합니다# 기본값인 \"include_bias=True\"는 절편에 해당하는 1인 특성을 추가합니다poly = PolynomialFeatures(degree=10, include_bias=False)poly.fit(X)X_poly = poly.transform(X)print(\"X_poly.shape:\", X_poly.shape)X_poly.shape: (120, 10)print(\"항 이름:\\n\", poly.get_feature_names())항 이름: ['x0', 'x0^2', 'x0^3', 'x0^4', 'x0^5', 'x0^6', 'x0^7', 'x0^8', 'x0^9', 'x0^10']이렇게 다항 회귀 모델을 만들 수 있습니다.다항식의 특성은 1차원 데이터셋에서도 매우 부드러운 곡선을 만듭니다.하지만 고차원 다항식은 데이터가 부족한 영역에서 너무 민감하게 동작합니다.reg = LinearRegression().fit(X_poly, y)line_poly = poly.transform(line)plt.plot(line, reg.predict(line_poly), label='Multinomial linear regression')plt.plot(X[:, 0], y, 'o', c='k')plt.ylabel(\"Regression output\")plt.xlabel(\"Input characteristics\")plt.legend(loc=\"best\")plt.show()비교를 위한 커널 SVM 모델from sklearn.svm import SVRfor gamma in [1, 10]:    svr = SVR(gamma=gamma).fit(X, y)    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))plt.plot(X[:, 0], y, 'o', c='k')plt.ylabel(\"Regression output\")plt.xlabel(\"Input characteristics\")plt.legend(loc=\"best\")plt.show()커널 SVM을 사용해 특성 데이터 변환없이 다항 회귀와 비슷한 복잡도를 가진 예측을 만들 수 있었습니다.보스턴 데이터셋을 통한 비교from sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import MinMaxScalerboston = load_boston()X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,                                                    random_state=0)# 데이터 스케일 조정scaler = MinMaxScaler()X_train_scaled = scaler.fit_transform(X_train)X_test_scaled = scaler.transform(X_test)poly = PolynomialFeatures(degree=2).fit(X_train_scaled)X_train_poly = poly.transform(X_train_scaled)X_test_poly = poly.transform(X_test_scaled)print(\"X_train.shape:\", X_train.shape)print(\"X_train_poly.shape:\", X_train_poly.shape)X_train.shape: (379, 13)X_train_poly.shape: (379, 105)from sklearn.linear_model import Ridgeridge = Ridge().fit(X_train_scaled, y_train)print(\"상호작용 특성이 없을 때 점수: {:.3f}\".format(ridge.score(X_test_scaled, y_test)))ridge = Ridge().fit(X_train_poly, y_train)print(\"상호작용 특성이 있을 때 점수: {:.3f}\".format(ridge.score(X_test_poly, y_test)))상호작용 특성이 없을 때 점수: 0.621상호작용 특성이 있을 때 점수: 0.753from sklearn.ensemble import RandomForestRegressorrf = RandomForestRegressor(n_estimators=100, random_state=0).fit(X_train_scaled, y_train)print(\"상호작용 특성이 없을 때 점수: {:.3f}\".format(rf.score(X_test_scaled, y_test)))rf = RandomForestRegressor(n_estimators=100, random_state=0).fit(X_train_poly, y_train)print(\"상호작용 특성이 있을 때 점수: {:.3f}\".format(rf.score(X_test_poly, y_test)))상호작용 특성이 없을 때 점수: 0.795상호작용 특성이 있을 때 점수: 0.775Ridge의 성능은 크게 높였지만 RandomForest는 상호작용 특성이 있는 Ridge만큼의 성능과 맞먹었습니다.일변량 다항식log, exp, sin과 같은 함수들도 특성 변환에 사용됩니다.log, exp 함수는 데이터 스케일을 변경하는 것에 사용되며, sin과 cos은 주기적인 패턴이 들어간 데이터를 변경하는 것에 유용하게 사용됩니다.일변량 비선형 변환을 하는 이유는 특성과 타깃 값 사이에 비선형성이 있다면 모델을 만들기 어렵기 때문입니다.또한 대부분의 모델은 특성의 분포가 정규분포와 비슷할 때 데이터 간 편차를 줄여 성능이 상승합니다.![그림1](https://user-images.githubusercontent.com/54880474/141170385-38f2a372-e76e-44f8-8f05-5ffcd01b743c.png)![그림2](https://user-images.githubusercontent.com/54880474/141170390-e416f90a-e5d0-433a-9c24-09f89bcad250.png)다음 분포처럼 적은 값 쪽에 분포가 몰리지만 값의 범위가 매우 넓을 때 log함수를 사용하게 된다면 log함수는 x가 커질수록 증가율이 작아지기 때문에 멀리 떨어진 값을 중간값과 가깝게 이동시켜 정규분포와 유사한 형태를 띄게 만들 수 있습니다.  트리 모델처럼 스스로 중요한 상호작용을 찾을 수 있는 모델은 괜찮지만 선형 모델과 같이 스케일과 분포에 민감한 모델은 비선형 변환을 통해 좋은 효과를 얻을 수 있습니다.  ## 특성 자동 선택특성이 많으면 모델은 복잡해지고 과대적합 가능성이 상승하는 것을 앞에서 배웠습니다.  따라서 불필요한 특성을 줄이는 것이 모델 학습에 좋은 영향을 미칩니다.  특성 선택을 사용할 때는 test 데이터셋에 영향이 가지않도록 train 데이터셋에만 적용해야하는 주의점이 있습니다.  ### 일변량 통계 (ANOVA)일변량 통계는 특성과 타깃 사이에 중요한 통계적 관계를 계산합니다.  각 특성을 독립적으로 평가하여 다른 특성과 깊게 연관된 특성은 선택하지 않습니다.  SelectKBest는 K개의 특성을, SelectPercentile은 지정한 비율만큼의 특성을 선택해줍니다.  또한 분류 모델에서는 f_classif, 회귀에서는 f_regression을 사용합니다.  일변량 통계는 Y값과 특성의 통계적 유의미를 분석합니다.   귀무가설을 기각하기 위해서는 p-value가 작아야합니다.  일변량 통계의 귀무가설은 '집단간 평균은 같다'입니다.  집단간 분산을 집단 내 분산으로 나눈 것을 F-value라고 합니다.  F-value가 작으면 p-value가 커지므로 이는 집단간 분산이 분모에 있으므로 집단간 분산이 작다는 의미와 같습니다.  이것은 집단간 차이가 적다는 의미이므로 타깃에 미치는 영향이 적다고 할 수 있습니다.  ![그림3](https://user-images.githubusercontent.com/54880474/141170392-caac02a7-62e7-4091-a621-6705b3eb68c9.png)따라서 타깃에 미치는 영향이 적은 특성을 제외시킬 수 있게 됩니다.  ### 모델 기반 특성 선택특성의 중요도를 측정하여 순서를 매깁니다.  결정 트리에서 중요도는 feature_importances_, 선형 모델에서는 계수의 절대값을 통해 중요도를 결정합니다.  임계치를 threshold를 통해 지정하는데 중앙값, 평균값, 1.2*평균값과 같이 지정해주면 됩니다.  ### 반복적 특성 선택  특성 수가 다른 일련의 모델을 생성하여 최선의 특성을 선택합니다.  여러 개의 모델을 만들어야하기 때문에 계산비용이 증가하게 됩니다.  1. 전진(후진) 선택법특성을 하나도 선택하지 않은 모델부터 회귀에서 결정계수(R^2)나 분류에서 정확도를 통해서 scoring 매개변수를 기준으로 종료조건을 충족시킬 때까지 하나씩 추가하는 방법입니다.  2. 재귀적 특성 제거  모든 특성을 가지고 시작해서 어떤 종료 조건이 될 때까지 특성을 하나씩 제거하는 방법이니다.  마찬가지로 중요도가 낮은 특성을 먼저 제거합니다.  제거한 다음 새로운 모델을 만들어서 정의한 특성 개수가 남을 때까지 계속합니다.  특성 자동 선택을 이용하면 특성 개수가 적어지기 때문에 모델의 예측 속도가 빨라지며 해석력이 좋아집니다.  ### 전문가 지식 활용  모델에 따라 외삽 문제가 발생할 수도 있고 그렇게 된다면 결정계수가 전혀 상관없는 예측을 할 수도 있습니다.  따라서 모델을 예측할 때는 서로간의 연관관계를 확인하고 특성을 추가, 제거하여 더 좋은 모델을 만들 수 있습니다.  ",
        "url": "/study-ML5"
    }
    ,
    
    "license-adsp8": {
        "title": "ADSP (8) 정형 데이터 마이닝",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP (7) 통계 분석 (2)ADSP를 준비하면서 공부한 내용을 정리한 글입니다.3과목 3장 정형 데이터 마이닝에 대한 부분을 정리한 글입니다.",
        "url": "/license-adsp8"
    }
    ,
    
    "license-adsp7": {
        "title": "ADSP (7) 통계 분석(2)",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP (7) 통계 분석 (2)ADSP를 준비하면서 공부한 내용을 정리한 글입니다.3과목 2장 통계분석에 대한 부분을 정리한 글입니다.",
        "url": "/license-adsp7"
    }
    ,
    
    "license-adsp6": {
        "title": "ADSP (6) 통계 분석(1)",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP (7) 통계 분석 (2)ADSP를 준비하면서 공부한 내용을 정리한 글입니다.3과목 2장 통계 분석에 대한 부분을 정리한 글입니다.  통계의 이해          통계와 표본 조사      표본 추출 방법      측정과 척도      기술통계와 추리통계        확률과 확률분포          이산확률분포      연속확률분포      여러가지 통계값        추정과 가설검정          추정      가설검정      통계의 이해통계와 표본 조사통계분석하고자 하는 집단에 대해 조사하거나 실험을 통해 얻는 자료표본조사전수조사가 불가능하기 때문에 표본의 대표성을 신뢰할 수 있는 표본 조사를 진행신뢰수준신뢰수준 95%라는 말은 95% 신뢰할 수 있다는 말이 아니라 100번 조사했을 때 오차범위 내에서 동일한 결과가 95번 나온다는 말오차범위 오차범위는 결과값에 대한 오차범위로 오차범위 3%라면 n-3~n+3의 값을 갖는다는 의미표본 추출 방법단순 랜덤 추출법표본 추출 방법에서 N개의 모집단에서 n개의 데이터를 무작위로 추출하는 방법계통 추출법모집단의 원소에 차례대로 번호를 부여한 뒤 일정한 간격을 두고 데이터를 추출하는 방법.집락(군집) 추출법각각 군집으로 구분한 뒤 단순 랜덤 추출법에 의하여 선택된 군집의 데이터를 표본으로 사용한다.집락은 서로 동질적이지만, 집락 내 데이터는 서로 이질적이다.층화 추출법층화 추출법은 집락 추출법과 유사하나 각 집락은 서로 이질적이고 내부 데이터는 서로 독립적이다.  비례 층화 추출법비례 층화 추출법은 전체 데이터의 분포를 반영하여 각 군집별 데이터를 추출하는 방법이다.  불비례 층화 추출법불비례 층화 추출법은 전체 데이터의 분포를 반영하지 않고 각 군집에서 원하는 데이터의 개수를 추출한다.측정과 척도측정표본조사를 실시하는 경우 추출된 원소들이나 실험 단위로부터 주어진 목적에 적합하게 관측해 자료를 얻는 것척도관측 대상의 속성을 측정하여 그 값이 숫자로 나타나도록 일정한 규칙을 정하여 바꾸는 도구척도의 종류  질적 척도          명목 척도 측정 대상이 어느 집단에 속하는지 나타내는 자료 (성별, 지역 등)      순서 척도 측정 대상이 명목척도이면서 서열 관계를 갖는 자료 (선호도, 학년 등)        양적 척도          구간 척도 측정 대상이 가지고 있는 속성의 양을 측정할 수 있으며 두 구간 사이에 의미가 있는 자료  (온도, 지수 등)      비율 척도 측정 대상이 구간척도이면서도 절대적 기준 0이 존재하여 사칙연산이 가능한 척도  (신장, 무게, 점수, 등)      기술통계와 추리통계기술통계기술 통계는 표본 자체의 속성이나 특징을 파악하는 데 중점을 두고 자료를 요약하고 조직화, 단순화하는데 그 목적이 있다.추리통계추리통계는 수집한 데이터를 바탕으로 ‘추론 및 예측’하는 통계 기법이다.표본에서 얻은 통계치를 바탕으로 오차를 고려하면서 모수를 확률적으로 추정하는 통계 기법이다.모집단의 특성을 추정하는 데 초점을 두고 가설을 검증하거나 확률적인 가능성을 파악한다.통계 기초 개졈편차평균과의 차이분산 Var(X)평균으로부터의 분포분산이 크면 퍼짐의 정도가 크다.표준편차 sd(X)분산은 기존과 다른 단위를 갖게 되기 때문에 단위를 일치시키기 위해 분산에 루트를 씌워서 구하는 값확률과 확률분포확률발생 가능한 모든 사건들의 집합 표본공간에서 표본공간의 부분집합인 특정 사건 A가 발생할 수 있는 비율조건부 확률특정 사건 A가 발생했다는 사실 아래 또 다른 사건 B가 발생할 확률독립사건서로에게 영향을 주지 않는 두 개의 사건배반사건공통된 부분이 없는 사건확률변수무작위 실험을 했을 때 특정 확률로 발생하는 각각의 결과를 수치적 값으로 표현하는 변수확률분포확률변수의 모든 값과 그 대응하는 확률이 어떻게 분포하고 있는 지를 보여주는 분포이산확률분포의 확률함수를 ‘확률질량함수’연속확률분포의 확률함수를 ‘확률밀도함수’이산확률분포베르누이 분포확률변수 X가 취할 수 있는 값이 두 개인 경우로 일반적으로 한 번의 시행을 할 때 성공과 실패로 나눌 수 있는 성공할 확률이 p인 분포ex) 하나의 동전을 던져 앞면이 나올 확률이항 분포이항 분포는 n번의 베루누이 시행에서 k번 성공할 확률의 분포ex) 하나의 동전을 3번 던져 앞면이 2번 나올 확률기하 분포성공할 확률이 p인 베르누이 시행에서 처음으로 성공이 나올 때까지 k번 실패할 확률의 분포ex) 동전을 던져서 3번째에 앞면이 나올 확률다항 분포이항 분포를 확장한 개념으로, n번의 시행에서 각 시행이 3개 이상의 결과를 가질 수 있는 확률의 분포ex) 주사위를 n번 던졌을 때 Pn의 확률로 1이 x번, 2가 y번 3이 z번 나올 확률포아송 분포단위 시간 또는 단위 공간 내에서 발생할 수 있는 사건의 발생 횟수에 대한 확률분포ex) 8시간 동안 3명의 손님이 왔을 때 1시간 동안 1명의 손님이 올 확률이산확률변수확률변수가 취할 수 있는 실수 값의 수를 셀 수 있는 변수연속확률분포균일분포균일 분포는 연속형 확률변수인 X가 취할 수 있는 모든 값에 대하여 같은 확률을 갖고 있는 분포균일 분포 그래프 아래 면적의 넓이는 확률의 총합인 1정규분포정규분포는 평균이 μ, 표준편차가 σ인 분포표준정규분포는 평균이 0, 표준편차가 1인 정규분포t-분포자유도가 n인 t분포는 평균이 0이고 종 모양이지만 정규분포보다 두꺼운 꼬리를 갖는 분포표준정규분포를 활용하여 모수를 추정하기 위해서는 모표준편차를 사전에 알고있어야 하지만, 현실적으로 모르기 때문에 t분포를 이용하여 모평균 검정 또는 두 집단의 평균이 동일한지 계산하기 위한 검정통계량으로 활용된다.자유도가 커질수록 t분포는 표준정규분포와 가깝다.자유도는 표본자료들이 모집단에 정보를 주는 독립적인 자료의 개수ex) 의자 4개와 사람 4명이 있을 때 마지막 사람에겐 선택권이 없으므로 자유도는 3카이제곱 분포표준정규분포를 따르는 확률변수 Z1,Z2,···,Zn의 제곱의 합 X는 자유도가 n인 카이제곱 분포를 따른다.모평균과 모분산을 모르는 두 개 이상의 집단 간 동질성 검정 또는 모분산 검정을 위해 활용된다.  동질성 검정두 집단의 내부 구성비를 비교하는 것F분포서로 독립인 두 카이제곱 분포를 따르는 확률변수 V1~x2(k1),V2~x2(k2)를 각각의 자유도로 나누었을 때 서로의 비율 X는 자유도가 k1, k2인 F분포를 따른다.F분포는 등분산 검정 및 분산분석을 위해 활용된다.  등분산 검정가설 검정을 수행하는 환경에 따라 두 모집단에 대한 평균을 비교할 때 사용연속확률변수확률변수가 취할 수 있는 실수 값이 어떤 특정 구간 전체에 해당하여 그 수를 셀 수 없는 변수를 연속확률번수확률밀도함수의 아래 면적이 확률여러가지 통계값기댓값특정 사건이 시행되었을 때 확률변수 X가 취할 수 있는 값의 평균 값분산분산데이터들이 중심에서 얼마나 떨어져 있는지를 알아보기 위한 측도확률변수의 분산확률변수가 취할 수 있는 값들이 모평균에서 얼마나 떨어져 있는지를 측정하는 측도표준편차자료의 산포도를 나타내는 수치첨도확률분포의 뾰족한 정도를 나타내는 측도로 값이 3에 가까울수록 정규분포 모양왜도확률분포의 비대칭 정도를 나타내는 측도0일 경우 평균, 중앙값, 최빈값이 같음공분산확률변수 X, Y의 상관 정도를 나타내는 값하나의 확률변수가 증가할 때 다른 확률변수의 증감소 여부를 확인양수면 X가 증가할 때 Y도 증가, 음수면 X가 증가할 때 Y는 감소공분산 값이 100이면 두 확률 분포가 어느 정도 선형성을 갖는지 알 수 없음상관계수공분산의 문제를 해결한 값으로 -1과 1 사이의 값 중에서 공분산을 X의 표준편차와 Y의 표준편차로 모두 나눈 값추정과 가설검정추정모수의 추정우리가 궁극적으로 알고싶은 값은 모수지만, 전수조사를 해야 알 수 있기 때문에 모수를 추정한다.점추정모집단의 모수, 특히 모평균을 추정할 때 모평균을 하나의 특정한 값이라고 예측한 것불편추정량이란 모수를 추정할 때 추정하는 값과 실제 모수 값의 차이의 기댓값이 0이므로 어느 한쪽으로 편향되지 않아 모수를 추정하기에 이상적인 값구간추정구간추정은 모수가 특정한 구간 안에 존재할 것이라 예상하는 것가설검정통계적 가설검정은 모집단의 특성에 대한 주장 또는 가설을 세우고 표본에서 얻은 정보를 이용해 가설이 옳은지 판정하는 과정으로 귀무가설과 대립가설로 구분귀무가설 (H0)모집단이 어떤 특징을 지닐것으로 여겨지는 가설대립가설 (H1)귀무가설이 틀렸다고 판단될 경우 채택되는 가설연구를 통해 증명하고자 하는 새로운 가설제1종 오류와 제2종 오류제 1종 오류귀무가설이 사실인데 귀무가설이 틀렸다고 결정하는 오류제 2종 오류귀무가설이 틀렸음에도 귀무가설이 옳다고 결정하는 오류검정통계량귀무가설의 옳고 그름을 판단할 수 있는 값기각역귀무가설을 기각하게 될 검정통계량의 영역검정통계량이 기각역 내에 있으면 귀무가설을 기각한다.기각역의 경계값을 임계값이라고 한다.유의수준 (α)귀무가설이 참인데도 이를 잘못 기각하는 오류를 범할 확률의 최대 허용 한계제 1종 오류를 줄이기위해 사용된다.유의확률 (p-value)귀무가설을 지지하는 정도를 나타낸 확률p-value &lt; α귀무가설을 기각할 수 있다.p-value &gt; α귀무가설을 기각할 수 없다.  기존의 귀무가설을 수립하고 이를 기각하는 증거를 찾는 대립가설을 채택하는 것이 가설 검증의 횟수를 줄일 수 있기 때문에 귀무가설을 검증한다.모수검정모수검정은 표본이 정규성을 갖는다는 모수적 특성을 이용등간척도, 비율척도평균피어슨 상관계수one sample t-test, two sample t-test, paired t-test, one way anova비모수검정비모수검정은 정규성 검정에서 정규분포를 따르지 않는다고 증명되거나 소규모 실험에서와 같이 정규분포를 가정할 수 없는 경우에 사용명목척도, 서열척도중앙값스피어만 상관계수부호검정, Wilcoxon 부호순위검정, Mann-Whitney 검정",
        "url": "/license-adsp6"
    }
    ,
    
    "license-adsp5": {
        "title": "ADSP (5) R 기초와 데이터 마트",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP (7) 통계 분석 (2)ADSP를 준비하면서 공부한 내용을 정리한 글입니다.3과목 1장 R 기초와 데이터 마트에 대한 부분을 정리한 글입니다.  R 기초          데이터 타입      연산자      R 데이터 구조      R 내장 함수      R 데이터 핸들링      제어문      통계분석에 사용되는 R 함수        데이터 마트          데이터 전처리      R 패키지 활용        데이터 탐색          탐색적 데이터 분석      결측값      이상값      R 기초데이터 타입문자형 타입””, ‘‘안에 입력하여 사용한다.class('abc')class(\"abc\")class('1')class(\"True\")‘character’‘character’‘character’‘character’숫자형 타입numeric, double(실수), integer(정수), complex(복소수)와 같은 타입이 있다.double은 숫자로만 표현이 가능하다.Inf는 Infinite의 약자로 무한대를 의미한다.class(Inf)class(1)class(-3)‘numeric’‘numeric’‘numeric’논리형 타입참, 거짓을 의미한다.class(TRUE)class(FALSE)‘logical’‘logical’NaN. NA, NULLNaN은 ‘Not of Number’의 약자로 음수의 제곱근을 구하려고 시도하는 것과 같은 경우에 오류와 함께 숫자가 아님을 반환한다.‘Not Available’의 약자인 Na와 NULL은 결측값을 의미한다.NA는 공간을 차지하는 방면, NULL은 공간을 차지하지 않는 존재하지 않는 값을 의미한다.sqrt(-3)class(NA)class(NULL)Warning message in sqrt(-3):\"NaN이 생성되었습니다\"NaN‘logical’‘NULL’연산자대입 연산자&lt;-,«-,= : 오른쪽 값을 왼쪽에 대입-&gt;,-» : 왼쪽 값을 오른쪽에 대입string1 &lt;- 'abc''data'-&gt;string2number1&lt;&lt;-15Inf-&gt;&gt;number2logical=NAstring1string2number1number2logical‘abc’‘data’15Inf&lt;NA&gt;비교 연산자== : 두 값이 같은지 비교&lt; ,&gt; : 초과, 미만을 비교&lt;= , =&gt; : 이상, 이하를 비교is.character : 문자형인지 아닌지를 비교is.numeric : 숫자형인지 아닌지를 비교is.logical : 논리형인지 아닌지를 비교is.na : NA인지 아닌지를 비교is.null : NULL인지 아닌지를 비교NA는 비교할 값이 존재하지 않으므로 어떤 것과 비교를 하더라도 NA를 반환한다.string1=='abc'string2&gt;'DATA'number1&lt;=15is.na(logical)is.null(NULL)TRUEFALSETRUETRUETRUE산술 연산자두 숫자형 타입의 계산을 위한 연산자로서 다양한 연산이 가능하다.+, -, *, / : 두 숫자간의 사칙연산%/% : 두 숫자의 나눗셈의 몫%% : 두 숫자의 나눗셈의 나머지^, ** : 거듭제곱exp() : 자연상수의 거듭제곱기타 연산자논리값을 계싼하기 위한 연산자로는 부정연산자 (!), AND 연산자 (&amp;), OR 연산자 (|)가 있다.부정 연산자는 현재의 논리값에 반대되는 값, AND 연산자는 두 값이 모두 참일 때만 참, OR 연산자는 두 값 중 하나의 값만 참이여도 참이다.!TRUETRUE&amp;TRUETRUE&amp;FALSE!(TRUE)TRUE | FALSEFALSETRUEFALSEFALSETRUER 데이터 구조벡터벡터는 타입이 같은 여러 데이터를 하나의 행으로 저장하는 1차원 데이터 구조다.‘연결한다’라는 의미의 ‘concatenate’의 c를 써서 데이터를 묶을 수 있다.v4&lt;-c(3, TRUE,FALSE)v4v5&lt;-c('a',1,TRUE)v5&lt;ol class=list-inline&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;0&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;‘a’&lt;/li&gt;\t&lt;li&gt;‘1’&lt;/li&gt;\t&lt;li&gt;‘TRUE’&lt;/li&gt;&lt;/ol&gt;벡터를 생성할 때 c 안에 콤마를 구분자로 써서 성분을 직접 입력할 수 있지만 콜론:을 사용하여 시작과 끝값을 지정해 벡터를 생성할 수도 있다.v1&lt;-c(1:6)v1&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;6&lt;/li&gt;&lt;/ol&gt;행렬행렬은 2차원 구조를 가진 벡터다.벡터의 성질을 가지고 있으므로 행렬에 저장된 모든 데이터는 같은 타입이어야 한다.matrix를 사용하여 행렬을 만들 경우 nrow를 사용하여 행의 수를 결정하거나 ncol을 사용하여 열의 수를 결정할 수 있다.m1&lt;-matrix(c(1:6),nrow=2)m1m2&lt;-matrix(c(1:6),ncol=2)m2\t135\t246\t14\t25\t36matrix를 사용하여 행렬을 만들 경우 행렬의 값들이 열로 저장된다.  byrow=T를 지정하면 값들이 열이 아닌 행으로 나온다.m3&lt;-matrix(c(1:6),nrow=2, byrow=T)m3\t123\t456벡터에 차원을 주는 방법도 있다.dim함수는 벡터를 행렬로 변환할 뿐만 아니라 주어진 행렬이 몇 개의 행과 열로 구성되어 있는지 행렬의 크기를 나타내기도 한다.v1&lt;-c(1:6)v1dim(v1)&lt;-c(2,3)v1&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;6&lt;/li&gt;&lt;/ol&gt;\t135\t246배열3차원 이상의 구조를 갖는 벡터이다.array를 사용하여 만들 수 있으나 몇 차원의 구조를 갖는지 dim 옵션에 명시해야 한다.주피터 노트북 R을 사용하면 해당 기능이 작동하지 않는다.a1&lt;-array(c(1:12), dim=c(2,3,2))a1a2&lt;-c(1:12)dim(a2)&lt;-c(2,3,2)a2&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;6&lt;/li&gt;\t&lt;li&gt;7&lt;/li&gt;\t&lt;li&gt;8&lt;/li&gt;\t&lt;li&gt;9&lt;/li&gt;\t&lt;li&gt;10&lt;/li&gt;\t&lt;li&gt;11&lt;/li&gt;\t&lt;li&gt;12&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;6&lt;/li&gt;\t&lt;li&gt;7&lt;/li&gt;\t&lt;li&gt;8&lt;/li&gt;\t&lt;li&gt;9&lt;/li&gt;\t&lt;li&gt;10&lt;/li&gt;\t&lt;li&gt;11&lt;/li&gt;\t&lt;li&gt;12&lt;/li&gt;&lt;/ol&gt;리스트리스트는 성분들이 서로 이질적이다.L&lt;-list()L[[1]]&lt;-5L[[2]]&lt;-c(1:6)L[[3]]&lt;-matrix(c(1:6),nrow=2)L[[4]]&lt;-array(c(1:12),dim=c(2,3,2))L\t5\t&lt;ol class=list-inline&gt;\t1\t2\t3\t4\t5\t6&lt;/ol&gt;\t\t135\t246\t&lt;ol class=list-inline&gt;\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12&lt;/ol&gt;데이터프레임2차원 구조를 갖는 관계형 데이터 구조행렬과 같은 모양이지만 각 열은 서로 다른 타입의 데이터를 가질 수 있다.R의 벡터는 시작 인덱스 값을 1로 갖는다.v1&lt;-c(1,2,3)v2&lt;-c('a','b','c')df1&lt;-data.frame(v1,v2)df1v1v2\t1a\t2b\t3cR의 벡터는 시작 인덱스 값을 1로 갖는다.df1[0]df1[1]v1\t1\t2\t3R 내장 함수기본 함수help() or ? : 함수의 도움말paste() : 문자열 이어 붙이기seq() : 시작값, 끝값, 간격으로 수열을 생성rep() : 주어진 데이터를 일정 횟수만큼 반복rm() : 대입 연산자에 의해 생성된 변수 삭제ls() : 현재 생성된 변수들의 리스트 표현print() : 값을 콘솔창에 출력# help와 ?는 R스튜디오에서는 도움창이 있지만 주피터 노트북에서는 하단에 창으로 나온다.help(paste)?paste paste('This is','a pen')seq(1,10,by=2)rep(1,5)a&lt;-1arm(a)# a : errorls()print(10)‘This is a pen’&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;7&lt;/li&gt;\t&lt;li&gt;9&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;1&lt;/li&gt;&lt;/ol&gt;1&lt;ol class=list-inline&gt;\t&lt;li&gt;‘a1’&lt;/li&gt;\t&lt;li&gt;‘a2’&lt;/li&gt;\t&lt;li&gt;‘df1’&lt;/li&gt;\t&lt;li&gt;‘L’&lt;/li&gt;\t&lt;li&gt;‘logical’&lt;/li&gt;\t&lt;li&gt;‘m1’&lt;/li&gt;\t&lt;li&gt;‘m2’&lt;/li&gt;\t&lt;li&gt;‘m3’&lt;/li&gt;\t&lt;li&gt;‘number1’&lt;/li&gt;\t&lt;li&gt;‘number2’&lt;/li&gt;\t&lt;li&gt;‘string1’&lt;/li&gt;\t&lt;li&gt;‘string2’&lt;/li&gt;\t&lt;li&gt;‘strint1’&lt;/li&gt;\t&lt;li&gt;‘v1’&lt;/li&gt;\t&lt;li&gt;‘v2’&lt;/li&gt;\t&lt;li&gt;‘v4’&lt;/li&gt;\t&lt;li&gt;‘v5’&lt;/li&gt;&lt;/ol&gt;[1] 10통계 함수sum : 입력된 값의 합mean : 입력된 값의 평균median : 입력된 값의 중앙값var : 입력된 값의 표본 분산sd : 입력된 값의 표본 표준편차max : 입력된 값의 최댓값min : 입력된 값의 최솟값range : 입력된 값의 최댓값과 최솟값summary : 입력된 값의 요약값NA가 있는 경우에 na.rm=T 사용하면 NA 제외하고 계산v1&lt;-c(1:9)sum(v1)mean(v1)median(v1)var(v1)sd(v1)max(v1)min(v1)range(v1)summary(v1)45557.52.7386127875258391&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;9&lt;/li&gt;&lt;/ol&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.       1       3       5       5       7       9 R 데이터 핸들링데이터 이름 변경2차원 이상의 데이터 구조는 colnames와 rownames 함수를 사용하여 행과 열의 이름을 표현m1&lt;-matrix(c(1:6),nrow=2)colnames(m1)&lt;-c('c1','c2','c3')rownames(m1)&lt;-c('r1','r2')m1colnames(m1)rownames(m1)df&lt;-data.frame(x=c(1,2,3),y=c(4,5,6))colnames(df1)&lt;-c('c1','c2')rownames(df1)&lt;-c('r1','r2','r3')df1colnames(df1)rownames(df1)c1c2c3\tr1135\tr2246&lt;ol class=list-inline&gt;\t&lt;li&gt;‘c1’&lt;/li&gt;\t&lt;li&gt;‘c2’&lt;/li&gt;\t&lt;li&gt;‘c3’&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;‘r1’&lt;/li&gt;\t&lt;li&gt;‘r2’&lt;/li&gt;&lt;/ol&gt;c1c2\tr11a\tr22b\tr33c&lt;ol class=list-inline&gt;\t&lt;li&gt;‘c1’&lt;/li&gt;\t&lt;li&gt;‘c2’&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;‘r1’&lt;/li&gt;\t&lt;li&gt;‘r2’&lt;/li&gt;\t&lt;li&gt;‘r3’&lt;/li&gt;&lt;/ol&gt;데이터 추출대괄호([])를 이용하여 원하는 위치의 데이터를 얻을 수 있으며 행과 열의 이름으로 데이터를 얻을 수 있다.v1&lt;-c(3,6,9,12)v1[2]m1&lt;-matrix(c(1:6),nrow=3)m1m1[2,2]colnames(m1)&lt;-c('c1','c2')m1[,'c1']rownames(m1)&lt;-c('r1','r2','r3')m1['r3','c2']6\t14\t25\t365&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;&lt;/ol&gt;6데이터프레임에서는 $ 기호를 사용하여 원하는 열의 데이터를 구할 수 있으며, $와 []의 혼용이 가능하다.v1&lt;-c(1:6)v2&lt;-c(7:12)df1&lt;-data.frame(v1,v2)df1$v1df1$v2[3]&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;6&lt;/li&gt;&lt;/ol&gt;9데이터 결합행으로 결합하는 rbind와 열로 결합하는 cbind가 대표적이다.v1&lt;-c(1,2,3)v2&lt;-c(4,5,6)rbind(v1,v2)cbind(v1,v2)\tv1123\tv2456v1v2\t14\t25\t36행의 수 혹은 열의 수가 같아야 결합이 가능하다.하지만 벡터와 벡터의 결합에서는 부족한 데이터를 앞에서부터 다시 재활용하여 사용한 뒤, 오류와 함께 결과를 반환한다.v1&lt;-c(1,2,3)v2&lt;-c(4,5,6,7,8)rbind(v1,v2)Warning message in rbind(v1, v2):\"number of columns of result is not a multiple of vector length (arg 1)\"\tv112312\tv245678제어문반복문for과 while이 있다.for(i in 1:3){    print(i)}data&lt;-c('a','b','c')for (i in data){    print(i)}i&lt;-0while(i&lt;5){    print(i)    i&lt;-i+1}[1] 1[1] 2[1] 3[1] \"a\"[1] \"b\"[1] \"c\"[1] 0[1] 1[1] 2[1] 3[1] 4조건문참과 거짓에 따라 특정 코드가 수행될지 혹은 수행되지 않을지를 결정한다.number&lt;-5if(number&lt;5){    print('number은 5보다 작다')}else if (number&gt;5){    print('number은 5보다 크다')}else{    print('number은 5와 같다.')}number&lt;-3if(number&lt;5){    print('number은 5보다 작다')}else if (number&gt;5){    print('number은 5보다 크다')}else{    print('number은 5와 같다.')}number&lt;-7if(number&lt;5){    print('number은 5보다 작다')}else if (number&gt;5){    print('number은 5보다 크다')}else{    print('number은 5와 같다.')}[1] \"number은 5와 같다.\"[1] \"number은 5보다 작다\"[1] \"number은 5보다 크다\"사용자 정의 함수사용자가 원하는 변수로 정의 함수를 만들 수 있다.comparedTo5&lt;-function(number){    if(number&lt;5){    print('number은 5보다 작다')    }else if (number&gt;5){    print('number은 5보다 크다')    }else{    print('number은 5와 같다.')    }}comparedTo5(10)comparedTo5(3)comparedTo5(5)[1] \"number은 5보다 크다\"[1] \"number은 5보다 작다\"[1] \"number은 5와 같다.\"주석주석은 #를 사용하여 표시한다.# 1+1 계산 방법1+12통계분석에 사용되는 R 함수숫자 연산sqrt : 제곱근abs : 절댓값exp : e의 제곱수log : 밑이 e인 로그 값log10 : 밑이 10인 로그 값pi : 원주율round : 반올림 값ceiling : 올림floor : 내림문자 연산tolower : 소문자 변환toupper : 대문자 변환nchar : 문자열의 길이substr : 문자 일부분 추출strsplit : 구분자로 나누어 분할grepl : 문자열에 존재 확인gsub : 일부 문자를 대체data&lt;-'This is a pen'tolower(data)toupper(data)nchar(data)substr(data,9,13)strsplit(data,'is')grepl('pen',data)gsub('pen','banana',data)‘this is a pen’‘THIS IS A PEN’13‘a pen’\t&lt;ol class=list-inline&gt;\t'Th'\t' '\t' a pen'&lt;/ol&gt;TRUE‘This is a banana’벡터 연산length : 벡터의 길이paste : 구분자를 기준으로 결합cov : 공분산cor : 상관계수table : 데이터의 개수order : 벡터의 순서행렬 연산t : 전치행렬diag : 대각행렬%*% : 행렬의 곱데이터 탐색head : 데이터 앞의 일부분tail : 데이터 뒤의 일부분quantile : 4분위수x&lt;-c(1:12)head(x,5)tail(x)quantile(x)&lt;ol class=list-inline&gt;\t&lt;li&gt;1&lt;/li&gt;\t&lt;li&gt;2&lt;/li&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;7&lt;/li&gt;\t&lt;li&gt;8&lt;/li&gt;\t&lt;li&gt;9&lt;/li&gt;\t&lt;li&gt;10&lt;/li&gt;\t&lt;li&gt;11&lt;/li&gt;\t&lt;li&gt;12&lt;/li&gt;&lt;/ol&gt;&lt;dl class=dl-horizontal&gt;\t&lt;dt&gt;0%&lt;/dt&gt;\t\t&lt;dd&gt;1&lt;/dd&gt;\t&lt;dt&gt;25%&lt;/dt&gt;\t\t&lt;dd&gt;3.75&lt;/dd&gt;\t&lt;dt&gt;50%&lt;/dt&gt;\t\t&lt;dd&gt;6.5&lt;/dd&gt;\t&lt;dt&gt;75%&lt;/dt&gt;\t\t&lt;dd&gt;9.25&lt;/dd&gt;\t&lt;dt&gt;100%&lt;/dt&gt;\t\t&lt;dd&gt;12&lt;/dd&gt;&lt;/dl&gt;데이터 전처리subset : 조건식에 맞는 데이터 추출merge : 특정 공통된 열을 기준으로 병합apply : 열 또는 행별로 주어진 함수 적용df1&lt;-data.frame(x=c(1,1,1,2,2),y=c(2,3,4,3,3))df2&lt;-data.frame(x=c(1,2,3,4),z=c(5,6,7,8))subset(df1, x==1)merge(df1,df2,by=c('x'))apply(df1,1,sum) # 각 행에 함수 적용apply(df1,2,sum) # 각 열에 함수 적용xy\t12\t13\t14xyz\t125\t135\t145\t236\t236&lt;ol class=list-inline&gt;\t&lt;li&gt;3&lt;/li&gt;\t&lt;li&gt;4&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;&lt;/ol&gt;&lt;dl class=dl-horizontal&gt;\t&lt;dt&gt;x&lt;/dt&gt;\t\t&lt;dd&gt;7&lt;/dd&gt;\t&lt;dt&gt;y&lt;/dt&gt;\t\t&lt;dd&gt;15&lt;/dd&gt;&lt;/dl&gt;정규분포기본값은 표준 정규분포로 mean=0,sd=1이다.dnorm : 주어진 값에서 함수 값rnorm : 주어진 개수만큼 표본 추출pnorm : 주어진 값보다 작은 확률 값qnorm : 주어진 넓이 값을 갖는 x표본추출runif : 균일 분포에서 주어진 개수만큼 표본 추출sample : 주어진 데이터에서 주어진 개수만큼 표본을 추출날짜Sys.Date : 연, 월, 일 출력Sys.time : 연, 월, 일, 시간 출력as.Date : 주어진 값을 날짜 형식으로 변환format : 원하는 날짜 형식으로 변경as.POSIXct : 타임스탬프를 날짜 및 시간으로 변환타임 스탬프란 1970/1/1 UTC부터 날짜가 몇 초가 흘렀는지 나타내는 값Sys.Date()Sys.time()as.Date(\"2020-01-01\")format(Sys.Date(),'%Y/%m/%d')format(Sys.Date(),'%A') #A는 요일#시간 데이터의 unclass 값이 타임스탬프  unclass(Sys.time())as.POSIXct(1577804401,origin='1970-01-01')2021-11-04[1] \"2021-11-04 01:01:04 KST\"2020-01-01‘2021/11/04’‘목요일’1635955264.19852[1] \"2020-01-01 00:00:01 KST\"산점도plot : 산점도  type : p는 점, l은 직선, b는 점과 직선, n은 아무것도 표시하지 않음xlim : x축의 범위, ylim : y축의 범위xlab : x축의 이름, ylab : y축의 이름main : 산점도의 이름abline : 추가 직선  v : 수직선, h : 수평선col : 매개변수의 색상x&lt;-c(1:10)y&lt;-rnorm(10)plot(x,y,type='b',xlim=c(-2,12),ylim=c(-3,3),xlab='X axis',ylab='Y axis',main='Test plot')abline(v=c(1,10),col='blue')파일 읽기 쓰기read.csv : CSV 파일 불러오기write.csv : CSV 파일로 저장saveRDS : 분석 모델 및 R 파일 저장readRDS : 분석 모델 및 R 파일 읽기기타install.packages : 패키지 설치library : 패키지 호출getwd : 작업 디렉토리 확인setwd : 작업 디렉토리 설정데이터 마트데이터 마트란 데이터 웨어하우스로부터 특정 사용가가 관심을 갖는 데이터들을 주제별, 부서별로 추출하여 모은 비교적 작은 데이터 웨어하우스데이터 전처리데이터 정제와 분석 변수 처리 과정데이터 정제 : 결측값과 이상값을 처리분석 변수 처리 : 변수 선택, 차원 축소, 파생변수 생성, 변수 변환, 클래스 불균형파생변수 : 목적과 조건에 따라 생성한 변수요약변수 : 기본적인 통계 자료를 추출한 변수R 패키지 활용reshape 패키지https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/reshapemelt : 특정 변수를 기준으로 나머지 변수에 대한 세분화된 데이터 생성install.packages ('reshape')package 'reshape' successfully unpacked and MD5 sums checkedThe downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packagesstudent_number&lt;-c(1,2,1,2)semester&lt;-c(1,1,2,2)math_score&lt;-c(60,90,70,90)english_score&lt;-c(80,70,40,60)score&lt;-data.frame(student_number,semester,math_score,english_score)rownames(score)&lt;-c(1,2,3,4)scorelibrary(reshape)melt(score,id=c('student_number','semester'))student_numbersemestermath_scoreenglish_score\t1 1 6080\t2 1 9070\t1 2 7040\t2 2 9060Warning message:\"package 'reshape' was built under R version 3.6.3\"student_numbersemestervariablevalue\t1            1            math_score   60           \t2            1            math_score   90           \t1            2            math_score   70           \t2            2            math_score   90           \t1            1            english_score80           \t2            1            english_score70           \t1            2            english_score40           \t2            2            english_score60           cast : melt로 변환한 데이터를 요약을 위해 새롭게 가공melted_score&lt;-melt(score,id=c('student_number','semester'))# 학생의 과목별 평균 구하기cast(melted_score,student_number~variable,mean)# 학생의 학기별 평균 점수cast(melted_score,student_number~semester,mean)# 학생의 과목별 최대 점수cast(melted_score,student_number~variable,max)student_numbermath_scoreenglish_score\t1 6560\t2 9065student_number12\t1 7055\t2 8075student_numbermath_scoreenglish_score\t1 7080\t2 9070sqldf 패키지sqldf는 표준 SQL 문장을 활용하여 R에서 데이터프레임을 다루게 한다.install.packages('sqldf')also installing the dependencies 'ellipsis', 'fastmap', 'bit', 'vctrs', 'rlang', 'cachem', 'bit64', 'blob', 'DBI', 'memoise', 'Rcpp', 'gsubfn', 'proto', 'RSQLite', 'chron'  There are binary versions available but the source versions are later:        binary source needs_compilationrlang   0.4.11 0.4.12              TRUEcachem   1.0.4  1.0.6              TRUEblob     1.2.1  1.2.2             FALSERcpp     1.0.6  1.0.7              TRUERSQLite  2.2.7  2.2.8              TRUE  Binaries will be installedpackage 'ellipsis' successfully unpacked and MD5 sums checkedpackage 'fastmap' successfully unpacked and MD5 sums checkedpackage 'bit' successfully unpacked and MD5 sums checkedpackage 'vctrs' successfully unpacked and MD5 sums checkedpackage 'rlang' successfully unpacked and MD5 sums checkedpackage 'cachem' successfully unpacked and MD5 sums checkedpackage 'bit64' successfully unpacked and MD5 sums checkedpackage 'DBI' successfully unpacked and MD5 sums checkedpackage 'memoise' successfully unpacked and MD5 sums checkedpackage 'Rcpp' successfully unpacked and MD5 sums checkedWarning message:\"cannot remove prior installation of package 'Rcpp'\"Warning message in file.copy(savedcopy, lib, recursive = TRUE):\"C:\\Users\\keonj\\anaconda3\\Lib\\R\\library\\00LOCK\\Rcpp\\libs\\x64\\Rcpp.dll를 C:\\Users\\keonj\\anaconda3\\Lib\\R\\library\\Rcpp\\libs\\x64\\Rcpp.dll로 복사하는데 문제가 발생했습니다: Permission denied\"Warning message:\"restored 'Rcpp'\"package 'gsubfn' successfully unpacked and MD5 sums checkedpackage 'proto' successfully unpacked and MD5 sums checkedpackage 'RSQLite' successfully unpacked and MD5 sums checkedpackage 'chron' successfully unpacked and MD5 sums checkedpackage 'sqldf' successfully unpacked and MD5 sums checkedThe downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packagesinstalling the source package 'blob'library(sqldf)sqldf('select * from score')sqldf('select * from score where student_number=1')sqldf('select avg(math_score),avg(english_score) from score group by student_number')student_numbersemestermath_scoreenglish_score\t1 1 6080\t2 1 9070\t1 2 7040\t2 2 9060student_numbersemestermath_scoreenglish_score\t1 1 6080\t1 2 7040avg(math_score)avg(english_score)\t6560\t9065plyr 패키지apply함수를 기반으로 데이터를 분리하고 다시 결합하는 기능을 제공한다.https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html입력 데이터 구조, 출력 데이터 구조, plyex) ddply-데이터프레임, 데이터프레임, plyinstall.packages('plyr')package 'plyr' successfully unpacked and MD5 sums checkedWarning message:\"cannot remove prior installation of package 'plyr'\"Warning message in file.copy(savedcopy, lib, recursive = TRUE):\"C:\\Users\\keonj\\anaconda3\\Lib\\R\\library\\00LOCK\\plyr\\libs\\x64\\plyr.dll를 C:\\Users\\keonj\\anaconda3\\Lib\\R\\library\\plyr\\libs\\x64\\plyr.dll로 복사하는데 문제가 발생했습니다: Permission denied\"Warning message:\"restored 'plyr'\"The downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packagesclass&lt;-c('A','A','B','B')math&lt;-c(50,70,60,90)english&lt;-c(70,80,60,80)score&lt;-data.frame(class,math,english)scorelibrary(plyr)#summarise는 데이터 요약ddply(score,'class',summarise,math_avg=mean(math),eng_avg=mean(english))#transform은 데이터 추가ddply(score,'class',transform,math_avg=mean(math),eng_avg=mean(english))classmathenglish\tA 5070\tA 7080\tB 6060\tB 9080Attaching package: 'plyr'The following objects are masked from 'package:reshape':    rename, round_anyclassmath_avgeng_avg\tA 6075\tB 7570classmathenglishmath_avgeng_avg\tA 50706075\tA 70806075\tB 60607570\tB 90807570year&lt;-c(2012,2012,2012,2012,2013,2013,2013,2013)month&lt;-c(1,1,2,2,1,1,2,2)value&lt;-c(3,5,7,9,1,5,4,6)data&lt;-data.frame(year,month,value)dataddply(data,c('year','month'),summarise,value_avg=mean(value))ddply(data,c('year','month'),function(x){    value_avg=mean(x$value)    value_sd=sd(x$value)    data.frame(avg_sd=value_avg/value_sd)})yearmonthvalue\t20121   3   \t20121   5   \t20122   7   \t20122   9   \t20131   1   \t20131   5   \t20132   4   \t20132   6   yearmonthvalue_avg\t20121   4   \t20122   8   \t20131   3   \t20132   5   yearmonthavg_sd\t2012    1       2.828427\t2012    2       5.656854\t2013    1       1.060660\t2013    2       3.535534data.table 패키지특정 칼럼별 주소값을 갖는 인덱스를 생성하여 연산과 검색을 빠르게 수행할 수 있다.install.packages('data.table')  There is a binary version available but the source version is later:           binary source needs_compilationdata.table 1.14.0 1.14.2              TRUE  Binaries will be installedpackage 'data.table' successfully unpacked and MD5 sums checkedThe downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packageslibrary('data.table')year&lt;-rep(c(2012:2015),each=12000000)month&lt;-rep(rep(c(1:12),each=1000000),4)value&lt;-runif(48000000)# 데이터 프레임과 데이터 테이블 생성DataFrame&lt;-data.frame(year,month,value)DataTable&lt;-as.data.table(DataFrame)# 데이터 프레임의 검색 시간 측정system.time(DataFrame[DataFrame$year==2012,])# 데이터 테이블의 검색 시간 측정system.time(DataTable[DataTable$year==2012,])# 칼럼이 키 값으로 설정될 경우 자동 오름차순 정렬  setkey(DataTable,year)# 키 값으로 설정된 칼럼과 J 표현식을 사용한 검색시간 측정 system.time(DataTable[J(2012)])Warning message:\"package 'data.table' was built under R version 3.6.3\"Attaching package: 'data.table'The following object is masked from 'package:reshape':    melt   user  system elapsed    0.87    0.24    1.11    user  system elapsed    0.47    0.21    0.29    user  system elapsed    0.32    0.06    0.12 데이터 탐색탐색적 데이터 분석데이터를 이해하고 의미있는 관계를 찾기 위해 데이터의 통계값과 분포 등을 시각화하고 분석하는 것head(iris,3)summary(iris)str(iris)Sepal.LengthSepal.WidthPetal.LengthPetal.WidthSpecies\t5.1   3.5   1.4   0.2   setosa\t4.9   3.0   1.4   0.2   setosa\t4.7   3.2   1.3   0.2   setosa  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width    Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100   1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300   Median :5.800   Median :3.000   Median :4.350   Median :1.300   Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199   3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800   Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500         Species   setosa    :50   versicolor:50   virginica :50                                                  'data.frame':\t150 obs. of  5 variables: $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...결측값결측값은 존재하지 않는 데이터를 의미하며 NA로 표현하지만 null, 공백, -1과 같은 표현을 할 때도 있다.결측치를 처리하는 것은 가장 중요한 과정이다.Amelia와 DMwR2가 대표적이다.Ameliamissmap 함수로 결측값을 시각화하기 좋다.install.packages('Amelia')also installing the dependency 'RcppArmadillo'  There are binary versions available but the source versions are later:                  binary     source needs_compilationRcppArmadillo 0.10.4.0.0 0.10.7.0.0              TRUEAmelia             1.7.6      1.8.0              TRUE  Binaries will be installedpackage 'RcppArmadillo' successfully unpacked and MD5 sums checkedpackage 'Amelia' successfully unpacked and MD5 sums checkedThe downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packagescopy_iris&lt;-iriscopy_iris[sample(1:150,30),1]&lt;-NA #결측값 생성 library(Amelia)missmap(copy_iris)Warning message:\"package 'Amelia' was built under R version 3.6.3\"Loading required package: Rcpp## ## Amelia II: Multiple Imputation## (Version 1.7.6, built: 2019-11-24)## Copyright (C) 2005-2021 James Honaker, Gary King and Matthew Blackwell## Refer to http://gking.harvard.edu/amelia/ for more information## 결측값 대치 방법단순 대치법데이터를 삭제하는 방법이다.대량의 데이터 손실이 발생할 수 있다.complete.cases 함수가 있다.하나의 열에 결측값이 존재하면 FALSE, 존재하지 않으면 TRUE를 반환한다.copy_iris&lt;-irisdim(copy_iris) # 기존 데이터copy_iris[sample(1:150,30),1]&lt;-NA # 결측값 생성copy_iris&lt;-copy_iris[complete.cases(copy_iris),] # 단순 대치법dim(copy_iris) # 결측값 처리 후&lt;ol class=list-inline&gt;\t&lt;li&gt;150&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;&lt;/ol&gt;&lt;ol class=list-inline&gt;\t&lt;li&gt;120&lt;/li&gt;\t&lt;li&gt;5&lt;/li&gt;&lt;/ol&gt;평균 대치법평균 혹은 중앙값으로 결측값을 대치하여 완전한 자료로 만드는 방법이다.비조건부 평균 대치법 : 데이터의 평균값으로 결측값을 대치하는 방법조건부 평균 대치법 : 실제 값들을 분석하여 회귀분석을 사용하는 방법DMwR2 패키지의 central Imputation 함수가 있다.install.packages('DMwR2')package 'DMwR2' successfully unpacked and MD5 sums checkedThe downloaded binary packages are in\tC:\\Users\\keonj\\AppData\\Local\\Temp\\Rtmp2RneIh\\downloaded_packagescopy_iris&lt;-iriscopy_iris[sample(1:150,30),1]&lt;-NA # 결측값 생성meanValue&lt;-mean(copy_iris$Sepal.Length,na.rm=T) #결측값을 제외한 평균  copy_iris$Sepal.Length[is.na(copy_iris$Sepal.Length)] &lt;-meanValue # 평균 대치  # central Imputation 사용library(DMwR2)copy_iris[sample(1:150,30),1]&lt;-NA # 결측값 생성copy_iris&lt;-centralImputation(copy_iris)단순 확률 대치법평균 대치법에서 추정량 표준 오차의 과소 추정 문제를 보완하고자 고안된 방법으로 KNN 방법이 있다.KNN 방법 : KNN 알고리즘으로 주변 K개의 데이터 중 가장 많은 데이터로 대치하는 방법copy_iris&lt;-iriscopy_iris[sample(1:150,30),1]&lt;-NA # 결측값 생성copy_iris&lt;-knnImputation(copy_iris,k=10)다중 대치법여러 번의 대치를 통해 n개의 임의 완전자료를 만드는 방법으로 대치, 분석, 결합의 세 단계로 구성된다.copy_iris&lt;-iriscopy_iris[sample(1:150,30),1]&lt;-NA # 결측값 생성iris_imp&lt;-amelia(copy_iris,m=3,cs='Species') #cs는 cross-sectional로 분석에 포함될 정보copy_iris$Sepal.Length&lt;-iris_imp$imputation[[3]]$Sepal.Length-- Imputation 1 --  1  2  3-- Imputation 2 --  1  2  3-- Imputation 3 --  1  2  3이상값이상값이란 값이 존재하지 않는 결측값과 달리 다른 데이터와 비교했을 때 극단적으로 크거나 작은값ESD(Extreme Studentized Deviation)ESD는 평균으로부터 ‘표준편차 3’만큼 떨어진 값들을 이상값으로 인식하는 방법이다.전체 데이터의 약 0.3퍼센트를 이상값으로 구분한다.사분위수사분위수를 이용하여 25%(Q1), 75%(Q3)에 해당하는 값을 활용하여 이상치를 판단한다.IQR이란 Q1~Q3 사이를 의미하며 사분범위라고 한다.사분범위에서 1.5분위수 벗어나는 경우를 이상치로 판단한다.Q1-1.5IQR 미만이거나 Q3+1.5IQR 초과면 이상값으로 판단한다.",
        "url": "/license-adsp5"
    }
    ,
    
    "license-adsp4": {
        "title": "ADSP (4) 분석 마스터플랜",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP (7) 통계 분석 (2)ADSP를 준비하면서 공부한 내용을 정리한 글입니다.2과목 2장 분석 마스터플랜에 대한 부분을 정리한 글입니다.  마스터플랜 수립          수행 과제 도출 및 우선순위 평가      로드맵 수립        분석 거버넌스 체계 수립  데이터 거버넌스 체계 수립마스터플랜 수립분석 마스터플랜이란 어떤 하나의 분석 프로젝트를 위한 전체 설계도와 같다.분석 마스터플랜 수립 프레임워크수행 과제 도출 및 우선순위 평가전략적 중요도  전략적 필요성  시급성실행 용이성  투자 용이성  기술 용이성ROI 요소투자비용 요소 (Investment)Volume, Variety, Velocity비즈니스 효과 (Return)Value우선 평가 기준전략적 중요도에 따른 시급성을 먼저 판단 - Value난이도는 데이터 분석의 적합성 여부를 따져본다. - Volume, Variety, Velocity포트폴리오 사분면 분석을 활용한 우선순위 평가 기준난이도와 시급성에 따라 포트폴리오 분석을 진행한다.로드맵 수립분석 체계 도입 - 분석 유효성 검증 - 분석 확산 및 고도화세부 이행계획 수립분석 데이터 수집/확보 - 분석 데이터 준비 - 모델링 및 평가분석 거버넌스 체계 수립분석 거버넌스 체계 구성요소조직, 과제 기획 및 운영 프로세스, 분석 관련 시스템, 데이터, 분석 관련 교육 및 마인드 육성 체계분석 준비도분석 업무 파악, 분석 인력 및 조직, 분석 기법, 분석 데이터, 분석 문화, IT 인프라분석 성숙도  도입분석 시작, 환경과 시스템 구축  활용분석 결과를 업무에 적용  확산전사 차원에서 분석 관리, 공유  최적화분석을 진화시켜 혁신 및 성과 향상에 기여분석 수준 진단 결과데이터 거버넌스 체계 수립데이터 거버넌스 구성 요소원칙, 조직, 프로세스데이터 거버넌스 체계  데이터 표준화표준 용어 설정, 명명 규칙, 메타데이터 구축, 데이터 사전 구축  데이터 관리 체계 메타데이터와 데이터 사전의 관리 원칙 수립  데이터 저장소 관리전사 차원의 저장소 구성  표준화 활동표준 준수 여부를 주기적으로 점검하고 모니터링 실시데이터 분석 조직 유형  집중형 조직 구조조직 내 별도의 독립적인 분석 전담 조직 구성  기능 중심의 조직 구조일반적으로 분석을 수행하는 형태로 별도 조직을 구성하지 않고 해당 업무 부서에서 직접 분석  분산형 조직 구조분석 조직의 인력을 현업 부서에 배치해 분석 업무를 수행분석 과제 관리 프로세스과제 발굴  분석 idea 발굴  분석 과제 후보 제안  분석 과제 확정과제 수행  팀 구성  분석 과제 실행  분석 과제 진행 관리  결과 공유/개선",
        "url": "/license-adsp4"
    }
    ,
    
    "license-adsp3": {
        "title": "ADSP (3) 데이터 분석 기회의 이해",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP (7) 통계 분석 (2)ADSP를 준비하면서 공부한 내용을 정리한 글입니다.2과목 1장 데이터 분석 기회의 이해에 대한 부분을 정리한 글입니다.  분석 기획  분석 방법론 개요          전통적인 분석 방법론      빅데이터 분석 방법론        분석 과제 발굴  분석 프로세스 관리 방안분석 기획분석 대상이 무엇인지 알고 있고 그 분석 방법도 알고 있다면 ‘최적화’분석 대상이 무엇인지 알고 있지만, 그 분석 방법을 모른다면 ‘솔루션’분석 대상이 무엇인지 모르고 그 분석 방법도 모른다면 ‘발견’분석 대상이 무엇인지는 모르지만, 그 분석 방법은 알고 있다면 ‘통찰’과제 중심적인 접근 방식빠르게 해결해야 하는 경우, 빠른 수행과 문제 해결이 목적장기적인 마스터플랜 방식지속적인 분석 내재화를 위한 경우, 정확도와 무엇이 문제인가에 대한 문제 정의가 목적분석 기획 시 고려사항가용 데이터 고려적절한 활용 방안과 유스케이스의 탐색장애 요소에 대한 사전 계획 수립분석 방법론 개요합리적 의사결정 방해요소고정 관념, 편향된 생각, 프레이밍 효과분석 방법론의 생성 과정방법론 -(내재화)-암묵지-(형식화)-형식지-(체계화)-방법론분석 방법론이 적용되는 업무 특성에 따른 모델      폭포수 모델단계적으로 진행되는 방식으로 하향식 방향        프로토타입 모델사용자 중심의 개발 방법        나선형 모델반복을 통해 점증적으로 개발하지만 위험 요소를 사전에 제거        계층적 프로세스 모델최상의 계층인 몇 개의 단계로 구성되어 있고 하나의 단계는 여러 개의 태스크로 구성되고 하나의 태스크는 여러개의 스텝으로 구성된다.단계: 프로세스 그룹을 통해 완성된 단계별 산출물 생성태스크: 단계를 구성하는 단위 활동스텝: WBS의 워크패이지에 해당  전통적인 분석 방법론      KDD 분석 방법론데이터로부터 통계적 패턴이나 지식을 찾기 위해 체계적으로 정리한 데이터 마이닝 프로세스데이터셋 선택 - 데이터 전처리 - 데이터 변환 - 데이터 마이닝 - 해석과 평가        CRISP-DM 분석 방법론KDD 분석 방법론보다 더 세분화업무 이해 - 데이터 이해 - 데이터 준비 - 모델링 - 평가 - 전개  빅데이터 분석 방법론분석 기획, 데이터 준비, 데이터 분석, 시스템 구현, 평가 및 전개의 5개 단계와 각각의 태스크와 스텝이 순차적으로 진행5단계 빅데이터 분석 방법론 플로우단계별분석 기획 - 데이터 준비 - 데이터 분석 - 시스템 구현 - 평가 및 전개단계별 수행 태스크분석 기획      비즈니스 이해 및 범위 설정비즈니스 이해 - 프로젝트 범위 설정        프로젝트 정의 및 계획 수립데이터 분석 프로젝트 정의 - 프로젝트 수행 계획 수립        프로젝트 위험계획 수립데이터 분석 위험 식별 - 위험 대응 계획 수립  데이터 준비      필요 데이터 정의데이터 정의 - 데이터 획득 방안 수립        데이터 스토어 설계정형 데이터 스토어 설계 - 비정형 데이터 스토어 설계        데이터 수집 및 정합성 검정데이터 수집 및 저장 - 데이터 정합성 점검  데이터 분석      분석용 데이터 준비비즈니스 룰 확인 - 분석용 데이터셋 준비        텍스트 분석텍스트 데이터 확인 및 추출 - 텍스트 데이터 분석        탐색적 분석탐색적 데이터 분석 - 데이터 시각화        모델링데이터 분할 - 데이터 모델링 - 모델 적용 및 운영 방안        모델 평가 및 검증모델 평가 - 모델 검증  시스템 구현      설계 및 구현시스템 분석 및 설계 - 시스템 구현        시스템 테스트 및 운영시스템 테스트 - 시스템 운영 계획  평가 및 전개      모델 발전 계획 수립모델 발전 계획        프로젝트 평가 및 보고프로젝트 성과 평가 - 프로젝트 종료  분석 과제 발굴하향식 접근법분석 대상을 알고 있을 때 문제 탐색 단계 - 문제 정의 단계 - 해결방안 탐색 단계 - 타당성 컴토 단계문제 탐색 단계      비즈니스 모델 탐색 기법업무, 제품, 고객, 규제와 감사, 지원 인프라        분석 기회 발굴 범위의 확장거시적 관점, 경쟁자 확대 관점, 시장의 니즈 탐색, 역량의 재해석        외부 참조 모델 기반 문제 탐색유사 동종 업계에서의 문제 탐색        분석 유스케이스유사 및 동종 사례 탐색  문제 정의 단계해결 방안 탐색 단계타당성 검토 단계  경제적 타당성  데이터 및 기술적 타당성상향식 접근법분석 대상을 모르고 있을 때분석 후 가치를 찾음지도, 비지도 학습 - 프로토타입지도학습정답이 있는 데이터를 활용하여 분석 모델 학습ex) 머신 러닝, DT, 인공신경망 모형, 분류, 회귀비지도학습정답이 없는 데이터를 활용하여 학습연관성, 유사성, 결합을 중심으로 데이터의 상태를 표현장바구니 분석, 기술통계, 프로파일링, 군집 분석, 주성분 분석, 다차원 척도프로토타이핑 접근법먼저 분석을 시도하고 결과를 확인하면서 개선가설의 생성 - 디자인에 대한 실험 - 실제 환경에서의 테스트 - 결과로부터 인사이트 도출 및 가설 확인분석 프로세스 관리 방안데이터의 양, 데이터의 복잡도, 분석의 속도, 분석 복잡도, 정확도와 정밀도",
        "url": "/license-adsp3"
    }
    ,
    
    "license-adsp2": {
        "title": "ADSP (2) 데이터의 가치와 미래",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP (7) 통계 분석 (2)ADSP를 준비하면서 공부한 내용을 정리한 글입니다.1과목 2장 데이터의 가치와 미래에 대한 부분을 정리한 글입니다.  빅데이터의 이해          빅데이터의 특징      빅데이터의 출현 배경      빅데이터의 기능과 변화        데이터의 가치와 미래          빅데이터의 가치      빅데이터 활용 기술      빅데이터의 위기 요인과 통제 방안        가치창조를 위한 데이터 사이언스와 전략 인사이트          빅데이터 분석과 전략 인사이트      데이터 사이언스에 대한 이해      빅데이터의 이해빅데이터란 큰 용량과 복잡성으로 기존 애플리케이션이나 툴로는 다루기 어려운 데이터셋의 집합빅데이터의 특징3VVolume데이터 양의 증가Variety데이터 유형 증가Velocity데이터 수집 및 처리 속도의 증가4V4V는 3V에 추가된 특징이다.Value데이터 가치의 중요성Veracity예측 분석 결과에 대한 신뢰성의 중요성밑에 두개는 의견이 갈린다Visualization데이터의 시각화Variability데이터의 가변성Validility데이터의 정확성Volarility데이터의 휘발성빅데이터 출현 배경  데이터의 양적 증가과학기술의 발달로 인한 데이터의 양적 증가  산업계의 변화정보의 축적과 기술이 만나 새로운 가치를 창출할 수 있는 변화의 상태  학계의 변화다양한 분야에서의 데이터 이용으로 필요한 기술 아키텍처 및 통계 도구의 발전  관련 기술의 발전디지털화, 저장 기술의 발전과 가격 하락, 인터넷의 발전과 클라우드 컴퓨팅와 같은 빅데이터와 연관된 기술의 발전빅데이터의 기능과 변화빅데이터는 석탄, 철, 원유, 렌즈, 플랫폼과 같은 역할을 한다.빅데이터로 인한 변화사전처리 -&gt; 사후처리표본조사 -&gt; 전수조사질 -&gt; 양인과관계 -&gt; 상관관계데이터 처리, 저장, 분석, 아키텍처, 클라우드 컴퓨팅과 같은 기술 변화데이터의 양, 유형, 수집 및 처리 기술과 같은 데이터의 변화데이터 사이언티스트, 데이터 중심 조직과 같은 인재 조직 변화데이터의 가치와 미래빅데이터의 가치빅데이터의 가치는 어떻게 활용할 것인지에 달렸다.데이터의 활용 방식, 가치 창출 방식, 분석 기술의 발전과 같은 이유로 가치 산정은 어렵다.빅데이터 활용 기술      연관규칙 학습 (Association rule learning)변인간의 상관 관계를 찾는 방법        유형분석 (Classification tree analysis)새로운 사건이 속할 범주를 찾는 방법        유전 알고리즘 (Genetic algorithms)최적화가 필요한 문제의 해결책의 진화 방법        기계학습 (Machine learning)훈련 데이터로부터 학습한 알려진 특성을 활용해 예측하는 방법        회귀분석 (Regression analysis)독립변수를 조작하면서 종속변수가 어떻게 변하는지 보며 관계를 파악하는 방법        감정분석 (Sentiment analysis)특정 주제에 대한 말이나 글의 감정을 분석하는 방법        소셜 네트워크 분석 (Social network analysis)사회 관계망 분석으로 사람 사이의 관계를 분석하는 방법  빅데이터의 위기 요인과 통제 방안위기 요인      사생활 침해개인의 사생활 침해 및 정보의 오용 위험        책임 원칙 훼손알고리즘으로 인한 피해 발생 위험        데이터 오용데이터 과신 및 잘못된 지표 사용으로 피해 발생 위험  통제 방안      사생활 침해의 통제 방안제공자의 ‘동의’에서 사용자의 ‘책임’으로개인정보 비식별 기술 (데이터 마스킹, 가명 처리, 총계 처리, 값 삭제, 범주화)        책임 원칙 훼손의 통제 방안결과 기반 책임 원칙 고수        알고리즘 접근 허용알고리즘으로 인한 피해 발생 시 알고리즘 접근을 허용하여 피해자 구제  가치창조를 위한 데이터 사이언스와 전략 인사이트빅데이터 분석과 전략 인사이트빅데이터에서 중요한 것은 ‘크기’가 아니라 ‘인사이트’이다.데이터 분석을 많이 사용하는 것이 아닌 전략적으로 사용해야 효과적인 운영이 가능하다.일차원적 분석에서 시작하여 전략 도출을 위한 가치 기반 분석까지 확장되어야한다.데이터 사이언스에 대한 이해데이터 사이언스는 데이터로부터 의미 있는 정보를 추출해내는 학문데이터 마이닝은 분석에 포커스를 둔다면 데이터 사이언스는 분석뿐 아니라 효과적으로 구현하고 전달하는 과정까지 포괄하는 개념수학, 확률 모델, 머신러닝, 분석학, 패턴 인식과 같은 Analytics,프로그래밍, 데이터 엔지니어링, 데이터 웨어하우징과 같은 Data Management,커뮤니케이션, 시각화, 프레젠테이션, 스토리텔링과 같은 비즈니스 분석으로 구성된다.기술적 능력으로 이루어진 하드 스킬과 분석, 전달, 협력으로 이루어진 소프트 스킬이 합쳐져야한다.따라서 인문학적 사고 특성도 길러야한다.",
        "url": "/license-adsp2"
    }
    ,
    
    "license-adsp1": {
        "title": "ADSP (1) 데이터의 이해",
            "author": "keonju",
            "category": "",
            "content": "ADSP 관련 글    ADSP (1) 데이터의 이해    ADSP (2) 데이터의 가치와 미래    ADSP (3) 데이터 분석 기회의 이해    ADSP (4) 분석 마스터플랜    ADSP (5) R 기초와 데이터 마트    ADSP (6) 통계 분석 (1)    ADSP (7) 통계 분석 (2)ADSP를 준비하면서 공부한 내용을 정리한 글입니다.1과목 1장 데이터 이해에 대한 부분을 정리한 글입니다.  데이터의 정의          데이터의 유형        데이터와 정보          DIKW 피라미드      데이터의 단위        데이터베이스 개요          데이터베이스의 특징      데이터베이스의 활용      데이터베이스의 종류      SQL의 이해      데이터베이스 구성요소      데이터의 정의데이터란?기술적이고 사실적인 의미의 자료. 객관적 사실정보는 데이터로 부터 얻은 것으로 가공된 자료존재적 특성있는 그대로의 객관적 사실당위적 특성데이터는 추론, 예측, 전망, 추정을 위한 근거데이터의 유형정성적 데이터 (언어, 문자)집합으로 표현할 수 없는 기준이 명확하지 않은 데이터정량적 데이터 (수치, 모형, 기호)집합으로 표현할 수 있는 기준이 명확한 데이터정형 데이터 (CSV, 엑셀)고정된 틀을 가지고 있으며 연산이 가능한 데이터로 관계형 DB에 저장하며 수집과 관리가 용이비정형 데이터 (소셜 데이터, 댓글, 음성, 영상)고정된 틀이 존재하지 않고 연산이 불가능관계형 DB가 아닌 NoSQL DB에 저장반정형 데이터 (XML, JSON, 센서 데이터)고정된 형태는 있지만 연산이 불가능테이블 형태보다는 파일 형태로 저장하여 가공을 거쳐 정형 데이터로 변환 가능암묵지와 형식지암묵지학습과 체험을 통해 개인에게 습득되어 있지만 겉으로 드러나지 않는 상태의 지식형식지암묵지를 여러 사람이 공유할 수 있게 형상화된 지식개인에 내면화된 암묵지가 출화하고 이를 개인의 지식으로 연결화 되는 과정을 거치면 조직의 지식으로 공통화되어 형식지가 된다.데이터와 정보DIKW 피라미드데이터 (Data)개별 데이터 자체는 의미가 중요하지 않은 객관적 사실정보 (Information)데이터의 가공, 처리와 데이터 간 연관 관계 속에서 의미 도출정보가 내포하는 의미는 유용하지 않을 수 있음지식 (Knowledge)데이터를 통해 얻은 정보를 구조화하여 유의미한 정보를 분류하고 경험과 결합해 고유의 지식으로 내재화지혜 (Wisdom)지식의 축적과 아이디어가 결합된 창의적 산물데이터 단위비트‘0’과 ‘1’의 두 가지 값으로 신호를 나타내는 최소단위바이트8개의 비트로 구성된 데이터의 양을 나타내는 단위숫자와 영어는 1바이트, 한글은 2바이트킬로-메가-기가-테라-페타-엑사-제타-요타 (각 단위는 1024배)데이터베이스 개요DB체계적으로 수집, 축적하여 다양한 용도와 방법으로 이용할 수 있게 정리한 정보의 집합체DBMS이용자가 쉽게 데이터베이스를 구축, 유지할 수 있게 하는 관리 소프트웨어데이터베이스의 특징통합된 데이터동일한 내용의 데이터가 중복되어 있지 않다.저장된 데이터컴퓨터가 매체에 접근할 수 있는 저장 매체에 저장되어 있다.공용 데이터여러 사용자가 공유할 수 있다.변화하는 데이터삽입, 수정, 삭제를 통해 항상 현재의 정확한 데이터를 유지해야 한다.정보의 축적 및 전달 측면기계의 가독성대량의 정보를 일정한 형식에 따라 정보처리기기가 읽고 쓸 수 있다.검색 가능성다양한 방법으로 필요한 정보를 검색할 수 있다.원격 조작성정보통신망을 통해 원거리에서도 즉시 온라인으로 이용 가능하다.트랜잭션 특성트랜잭션이란 데이터 베이스에서 명령을 수행하는 하나의 논리적 기능 단위원자성데이터베이스에 모두 적용되거나 모두 적용되지 않아야 한다일관성트랜잭션의 결과는 항상 일관성을 띠어야 한다고립성하나의 트랜잭션이 다른 트랜잭션에 영향을 주지 않아야 한다지속성트랜잭션이 성공적으로 수행된 경우 그 결과는 영구적이어야 한다데이터베이스 활용기업 내부의 데이터베이스 활용인하우스 DB, OLTP, OLAP, CRM, SCM, ERP, BI, RTE 등이 있다.사회 기반 구조 데이터베이스물류 부문CALS, PORT-MIS, KROIS지리부문GIS, LBS, SIM교통부문ITS의료부문PACS, U-Health교육부문NEIS데이터베이스의 종류관계형 데이터베이스데이터를 테이블에 저장되고 하나의 열은 하나의 속성을 나타내고 같은 속성 값만 가진다. 정형 데이터를 다루는 데 좋다.Oracle, MySQL, MS-SQL, SQLiteNoSQL비관계형을 의미하며 대용량의 데이터 분석 및 분산 처리에 용이하다.MongoDB, Dynamo, BigtableSQL의 이해SQL은 DBMS에서 데이터베이스에 내리는 명령이다.DB마다 문법이 다르지만 기본적인 데이터 추출과 분석에 사용되는 문법은 거의 동일하다.데이터 정의 언어 (DDL)CREATE, ALERT, RENAME, DROP데이터 조작 언어 (DML)SELECT, INSERT, UPDATE, DELETE데이터 제어 언어 (DCL)GRANT, REVOKE트랜잭션 제어 언어 (TCL)COMMIT, SAVEPOINT, ROLLBACK데이터베이스 구성요소인스턴스하나의 객체를 의미 (홍길동, 남자,000-0000-0000)속성객체를 표현하기 위해 사용되는 값 (이름, 성별, 주민번호)엔터티데이터의 집합, 테이블과 달리 개념적인 존재메타데이터데이터를 설명하는 데이터인덱스데이터베이스에 저장할 때 지정되는 데이터의 이름",
        "url": "/license-adsp1"
    }
    ,
    
    "study-ml4": {
        "title": "머신러닝 정리 (4) &lt;br&gt; 비지도학습 (2)",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학머신러닝 정리 (4) - 비지도학습 (2)본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python비지도학습 (2)  군집          계층적 군집분석      비계층적 군집분석        K-평균 군집  병합 군집  DBSCAN  군집 알고리즘의 비교와 평가          타깃 값으로 군집 평가하기      타깃 값 없이 군집 평가하기      군집군집(clusterling)은 데이터셋을 클러스터라는 그룹으로 나누는 작업입니다.기본적으로 거리에 관련된 측정 방법을 활용합니다.한 클러스터 안의 데이터 포인트끼리는 매우 비슷하고 다른 클러스터의 데이터 포인트와는 구분되도록 데이터를 나누는 것이 목표입니다.유사성을 측정하는 것에는 군집 간 분산이 최대화 되거나, 군집 내 분산을 최소화하게 됩니다.종류계층적 군집분석각 요소들로부터 시작한 클러스터들이 계층 구조를 이루도록 군집분석을 수행합니다.이 때 만들어진 계층구조를 덴드로그램이라고 합니다.비계층적 군집분석각 클러스터의 계층을 고려하지 않고 평면적으로 군집분석을 수행합니다.K-평균 군집k-평균(k-means) 군집은 데이터의 어떤 영역을 대표하는 클러스터 중심을 찾습니다.알고리즘은 먼저 데이터 포인트를 가장 가까운 클러스터 중심에 할당하고 클러스터에 할당된 데이터 포인트의 평균으로 클러스터 중심을 다시 지정합니다.클러스터에 할당되는 데이터 포인트에 변화가 없을 때 알고리즘이 종료됩니다.  데이터 포인트를 무작위로 초기화합니다.  각 데이터 포인트를 가장 가까운 클러스터 중심에 할당합니다.  할당한 포인트의 평균값으로 클러스터 중심을 갱신합니다.  더이상 포인트에 변화가 없다면 알고리즘이 멈춥니다.K 값을 설정하는 방법으로는 elbow method, silhouette method와 같은 방법이 있습니다.K-means 사용 방법from sklearn.datasets import make_blobsfrom sklearn.cluster import KMeans# 인위적으로 2차원 데이터를 생성합니다X, y = make_blobs(random_state=1)# 군집 모델을 만듭니다kmeans = KMeans(n_clusters=3)kmeans.fit(X)# 라벨을 확인합니다print(kmeans.labels_)k-means는 지정해준 cluster 값만큼 데이터를 분류해줍니다.또한 cluster 개수만큼 label은 0부터 값을 순서대로 가집니다.하지만 cluster 값을 잘 지정해준다고 하더라도 분류가 실패하는 경우가 생깁니다.‘모든 클러스터의 반경이 똑같다’, ‘클러스터에서 모든 방향이 똑같이 중요하다’ 라는 두 가지 가정을 가지고 있기 때문에 중심에서 멀리 떨어진 경우에 데이터를 잘 처리하지 못합니다.즉, 서로 원형으로 잘 모여있는 데이터에 대해서는 잘 구분하지만 모양이 복잡할수록 더 성능이 나빠집니다.K-means는 클러스터 중심, 하나의 성분으로 표현된다고 볼 수 있습니다.하나의 성분으로 분해되는 관점으로 보는 것을 벡터 양자화라고 합니다.벡터 양자화는 입력 데이터의 차원보다 더 많은 클러스터를 사용해 데이터를 인코딩할 수 있습니다.즉, 2차원 데이터에서도 10개의 클러스터를 사용해 10개의 특성을 가지는 모델을 만들 수 있습니다.장점  이해하기 쉽다.구현하기 쉽다.비교적 빠르다.유연하고 효율적이다.단점  난수 초깃값에 따라 달라진다.활용 범위가 제한적이다.클러스터의 개수를 지정해야한다.최적의 군집을 찾기 어렵다.군집 개수 파악에 대한 합리적 추측이 필요하다.이상치나 노이즈에 민감하다.병합 군집병합 군집은 알고리즘은 시작할 때 각 포인트를 하나의 클러스터로 지정하고 어떤 종료 조건을 만족할 때 까지 가장 비슷한 두 클러스터를 합쳐나갑니다.종료 조건은 클러스터 개수로 하여 지정된 개수의 클러스터가 남을 때까지 비슷한 클러스터를 합칩니다.wardward 연결은 모든 클러스터 내의 분산을 가장 작게 증가시키는 두 클러스터를 합칩니다.따라서 크기가 비슷한 클러스터가 만들어집니다.averageaverage 연결은 클러스터 포인트 사이의 평균 거리가 가장 짧은 두 클러스터를 합칩니다.completecomplete 연결은 클러스터 포인트 사이의 최대 거리가 가장 짧은 두 클러스터를 합칩니다.singlesigle 연결은 클러스터 포인트 사이의 최소 거리가 가장 짧은 두 클러스터를 합칩니다.클러스터는 다음과 같은 모습으로 합쳐집니다.클러스터의 사용 방법은 다음과 같습니다.from sklearn.cluster import AgglomerativeClusteringX, y = make_blobs(random_state=1)agg = AgglomerativeClustering(n_clusters=3)assignment = agg.fit_predict(X)mglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)plt.legend([\"클러스터 0\", \"클러스터 1\", \"클러스터 2\"], loc=\"best\")plt.xlabel(\"특성 0\")plt.ylabel(\"특성 1\")병합 군집은 계층적 군집을 만듭니다.계층적 군집은 Scipy에서 덴드로그램을 통해 다차원 데이터셋을 처리하여 시각화 할 수 있습니다.from scipy.cluster.hierarchy import dendrogram, wardX, y = make_blobs(random_state=0, n_samples=12)# 데이터 배열 X 에 ward 함수를 적용합니다# SciPy의 ward 함수는 병합 군집을 수행할 때 생성된# 거리 정보가 담긴 배열을 리턴합니다linkage_array = ward(X)# 클러스터 간의 거리 정보가 담긴 linkage_array를 사용해 덴드로그램을 그립니다dendrogram(linkage_array)# 두 개와 세 개의 클러스터를 구분하는 커트라인을 표시합니다ax = plt.gca()bounds = ax.get_xbound()ax.plot(bounds, [7.25, 7.25], '--', c='k')ax.plot(bounds, [4, 4], '--', c='k')ax.text(bounds[1], 7.25, ' 두 개 클러스터', va='center', fontdict={'size': 15})ax.text(bounds[1], 4, ' 세 개 클러스터', va='center', fontdict={'size': 15})plt.xlabel(\"샘플 번호\")plt.ylabel(\"클러스터 거리\")DBSCANDBSCAN은 클러스터의 개수를 미리 지정할 필요가 없습니다.복잡한 형상도 찾을 수 있으며 어떤 클래스에 속하지 않는 포인트도 구분할 수 있습니다.병합 군집이나 k-means보다 다소 느리지만 큰 데이터 셋에도 적용할 수 있습니다.DBSCAN은 특성 공간에서 가까이 있는 데이터가 많아 붐비는 지역을 포인트로 찾습니다.밀집 지역이 한 클러스터를 구성하며 비어있는 지역을 경계로 다른 클러스터와 구분된다는 것입니다.밀도: 자기를 중심으로 반지름 안에 있는 다른 좌표점의 개수최소 거리: 이웃을 정의하기 위한 거리, 밀도 측정 반지름최소 데이터 개수: 밀집 지역을 정의하기 위해 필요한 이웃의 개수, 반지름 내에 있는 최소 데이터의 개수  무작위 포인트를 선택합니다.  포인트에서 eps 거리 안의 모든 포인트를 찾습니다.  거리 안의 포인트 수가 min_samples보다 적다면 노이즈로 레이블 합니다.  거리 안의 min_samples보다 포인트 수가 많다면 그 포인트는 핵심 샘플로 레이블하고 새로운 클러스터 레이블을 할당합니다.  포인트의 eps 거리 안의 모든 이웃을 확인하여 어떤 클러스터에도 할당되지 않았다면 방금 만든 클러스터 레이블을 할당합니다.  이 과정을 반복하여 클러스터는 eps 거리 안에 더 이상 핵심 샘플이 없을 때까지 커집니다.  아직 선택되지 못한 포인트를 기준으로 위 과정을 다시 반복합니다.DBSCAN은 다음과 같이 사용합니다.from sklearn.cluster import DBSCANX, y = make_blobs(random_state=0, n_samples=12)dbscan = DBSCAN()clusters = dbscan.fit_predict(X)print(\"클러스터 레이블:\\n\", clusters)다음 그림에서 보는 것과 같이 min_samples와 eps을 통해 모양이 많이 달라집니다.eps를 증가시키면 하나의 클러스터에 더 많은 포인트가 포함되고, min_samples를 키우면 노이즈가 증가합니다.장점  K-means와 다르게 군집의 수를 설정할 필요가 없습니다.다양한 모양의 군집이 형성될 수 있으며, 군집끼리 겹치는 경우가 없습니다.노이즈 개념 덕분에 이상치에 대응할 수 있습니다.eps, min_samples를 잘 설정하면 좋은 성능을 낼 수 있습니다.단점  한 데이터는 하나의 군집에 속하게 되므로 시작점에 따라 다른 모양의 군집이 형성됩니다.eps 값에 따라 성능이 크게 좌우됩니다.군집별로 밀도가 다른 경우 군집화가 제대로 이루어지지 않습니다.군집 알고리즘의 비교와 평가타깃 값으로 군집 평가하기1(최적일 때)과 0(무작위로 분류) 사이의 값을 제공하는 ARI, NMI가 가장 널리 사용하는 지표입니다.adjusted_rand_score과 normalized_mutual_info_score과 같은 군집용 측정 도구가 따로 존재하므로 accuracy_score을 사용하지 않아야 합니다.하지만 ARI와 NMI와 같이 정확한 클러스터를 알고 있어야 평가가 가능하다면 지도 학습 모델을 만드는 곳에만 사용되고 실제 애플리케이션 성능 평가는 사용할 수 없습니다.타깃 값 없이 군집 평가하기따라서 타깃 값 없이 실루엣 계수라는 것을 이용하여 군집용 지표가 존재합니다.실루엣 점수는 클러스터의 밀집 정도를 계산하는 것으로 높을수록 좋으며 최대 점수는 1입니다.하지만 K-평균에서 나왔던 문제와 비슷하게 모양이 복잡할 때는 밀집도를 활용한 평가는 좋지 않습니다.",
        "url": "/study-ML4"
    }
    ,
    
    "programming-baekjoon7": {
        "title": "백준 (7) &lt;br&gt; (10828,10773,1874,10799, &lt;br&gt; 4949,1406,2493)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)백준 7문제를 풀어보았다.중복되는 문제도 있습니다스택에 관련된 7문제를 풀어보았다.https://www.acmicpc.net/problemset?sort=ac_desc&amp;algo=7110828번 스택https://www.acmicpc.net/problem/10828input()을 이용하면 시간 초과가 발생한다. (sys.stdin.readline() 사용)입력받은 값에 각 명령어들이 있으면 명령어에 따라 작동하도록 해주면 된다.push는 뒤에 숫자가 따라오기 때문에 split() 함수를 통해 뒤에 숫자를 분리해줘서 스택에 넣어준다.pop은 스택의 길이가 0 보다 클 때와 작을 때를 구분하여 pop 함수를 실행하거나 -1을 출력해주면 된다. size는 스택의 길이와 같다.empty는 스택의 길이가 0 인가 아닌가를 판별해주면 된다.top은 pop과 유사하지만 pop 함수를 사용하면 가장 위에 숫자가 스택에서 사라지기 때문에 stack[-1]을 출력해주면 된다.# 처음 문제만 읽고 풀었을 때 작성한 코드import sysstack=[]for i in range(int(input())):    command=input()    if 'push' in command:        a,b=command.split()        b=int(b)        stack.append(b)    if 'top' in command:        if len(stack)&gt;0:            print(stack[-1])        else:            print('-1')    if 'size' in command:        print(len(stack))    if 'empty' in command:        if len(stack)==0:            print('1')        else: print('0')    if 'pop' in command:        if len(stack)&gt;0:            print(stack.pop())        else:            print('-1')14push 1push 2top2size2empty0pop2pop1pop-1size0empty1pop-1push 3empty0top3# 클래스와 함수를 이용하는 것이 훨씬 보기 좋을 것 같아서 새로 작성한 코드# 클래스와 함수에 익숙하지 않아서 생각보다 오래 걸렸다.# 마찬가지로 input 대신 sys.stdin.readline()를 사용하여 제출해야한다.class stack:    def __init__(self):         self.stack_list = []    def push(self, num):        self.stack_list.append(num)    def pop(self):        if len(self.stack_list) == 0:            print(\"-1\")        else:            print(self.stack_list.pop())    def size(self):        print(len(self.stack_list))    def empty(self):        if len(self.stack_list) == 0:            print(\"1\")        else :            print(\"0\")    def top(self):        if len(self.stack_list) == 0:            print(\"-1\")        else :            print(self.stack_list[-1])import sysnumber = int(input())stack=stack()for _ in range(number):    command = input().split()    if command[0] == \"push\":        stack.push(command[1])    if 'pop' in command:        stack.pop()    if 'size' in command:        stack.size()    if 'empty' in command:        stack.empty()    if 'top' in command:        stack.top()14push 1push 2top2size2empty0pop2pop1pop-1size0empty1pop-1push 3empty0top310773번 제로https://www.acmicpc.net/problem/10773스택에서 pop과 append를 이용하여 해결할 수 있는 간단한 문제였다.재현이가 0 을 외치면 pop을 이용해서 최근의 숫자를 지워주면 된다.반대로 0이 아닌 숫자를 외치면 스택에 append 해주면 된다.출력하는 값은 stack에 남아있는 숫자이기 때문에 sum을 이용해서 출력해준다.import sysstack=[]for i in range(int(input())):    money=int(input())    if money==0:        stack.pop()    else:        stack.append(money)print(sum(stack))4304001874번 스택 수열https://www.acmicpc.net/problem/1874스택과 푸쉬, 팝 이해하기push를 세 번 하면 [1,2,3] 스택이 쌓이게 되고 여기서 pop을 하면 3이 출력된다.n을 통해 입력할 숫자의 갯수를 입력받고 num을 통해 숫자를 입력받는다.count는 입력받을 숫자가 stack에 입력되도록 해준다. 0으로 두면 0부터 시작이다.따라서 1로 한다. result를 통해 +와 -를 입력받고 stack에는 count에 생긴 숫자들을 쌓아둔다.while문을 통해 stack을 완성하고 if문을 통해 해당 숫자가 나오면 -를 입력한 뒤 pop해서 숫자를 제거한다.n=int(input())count=1result=[]stack=[]temp=Truefor i in range(n):    num=int(input())    while count&lt;=num:        stack.append(count)        result.append('+')        count=count+1    if stack[-1]==num:        stack.pop()        result.append('-')    else:        temp=False        if temp==False:    print('NO')else:    for j in result:        print(j)843687521++++--++-++-----10799번 쇠막대기https://www.acmicpc.net/problem/10799()가 쌍을 이룰 때는 레이저이므로 L로 대체했다.stack=[]을 이용하여 막대가 몇 개 있는지 저장하는 용도로 사용하였다.(가 되면 막대가 계속 쌓이는 데, 레이저를 만나면 레이저 왼쪽으로 막대의 개수만큼 잘린다.그 다음 )를 만나면 막대가 끝난 경우이므로 또 잘린 막대가 생긴다.따라서 if 문으로 (가 나오면 막대 개수를 추가해주고L은 막대의 개수만큼 답을 더해주고 )는 잘린 막대 1개를 추가해준다.word=input()()(((()())(())()))(())word=word.replace('()','L')stack=[]answer=0for i in word:    if i=='(':        stack.append(0)    elif i==')':        stack.pop()        answer+=1    else:        answer+=len(stack)print(answer)        L 0( 0( 0( 0L 3L 6) 7( 7L 10) 11L 13) 14) 15( 15L 16) 17174949번 균형잡힌 세상https://www.acmicpc.net/problem/4949하나는 replace와 re.sub을 이용하여 괄호들을 제외한 모든 문자를 지워주는 문제로 풀었다.하지만 주제가 스택이었으므로 스택으로 다시 한 번 풀었다.# replace와 sub을 이용한 풀이import reword='a'while word != '.':    word=input()    if word=='.':        break    word=word.replace('.','')    word=re.sub('[a-zA-Z]','',word)    word=re.sub(' ','',word)    while ('[]' in word) or ('()' in word):        word=word.replace('[]','')        word=word.replace('()','')    if word =='':        print('yes')    else:        print('no')A rope may form )( a trail in a maze.noHelp( I[m being held prisoner in a fortune cookie factory)].noSo when I die (the [first] I will see in (heaven) is a score list).yes[ first in ] ( first out ).yes([ (([( [ ] ) ( ) (( ))] )) ]).yes .yes.# stack을 이용한 풀이while True: # 계속 입력받기 위한 while    word=input()    stack=[] # 괄호 저장을 위한 stack    temp=True # 나중에 stack에 있는지 없는지 판단을 위한 temp    if word=='.': # .만 입력하면 while이 끝난다.          break            for i in word:        if i =='[' or i=='(': # 괄호 여는 것은 모두 스택에 append한다.            stack.append(i)        elif i==']': # 닫힌 괄호가 나왔을 때,            if len(stack) ==0 or stack[-1]=='(': # 스택에 아무것도 없거나 소괄호만 있다면 no를 출력한다.                                print('no')                temp=False #temp를 false로 두어서 나중에 stack에 짝이 맞아서 아무것도 없을 때를 대비한다.                break            else:                stack.pop() # 괄호의 짝이 맞게 있다면 '['를 없애서 스택에서 비워준다.        elif i==')':             if len(stack) ==0 or stack[-1]=='[':                print('no')                temp=False                break            else:                stack.pop()    if temp==True: #괄호의 짝이 모두 맞았다면 stack은 0이 될 것이다.        if len(stack)==0:            print('yes')        else:            print('no')So when I die (the [first] I will see in (heaven) is a score list).yes[ first in ] ( first out ).yesHalf Moon tonight (At least it is better than no Moon at all].noA rope may form )( a trail in a maze.noHelp( I[m being held prisoner in a fortune cookie factory)].no([ (([( [ ] ) ( ) (( ))] )) ]).yes .yes.1406번 에디터https://www.acmicpc.net/problem/1406처음에 짠 코드는 답은 맞았지만 시간초과가 발생한다.찾아보니까 insert와 del은 시간 복잡도가 O(n)이라고 한다.따라서 O(1)인 pop과 append를 사용해서 문제를 해결해야 한다.pop과 append를 이용하여 빈 스택에 ‘L’,’D’로 변화된 커서에 대한 정보를 저장한다.B와 P는 처음 푼 것과 다르게 커서 변경없이 pop과 append로 값만 추가, 삭제해주면 된다.word_list에는 새로 입력받은 값이 저장될 것고 새로 만든 스택에 커서 변경된 정보가 순서대로 저장될 것이다. 실제 입력받은 문자에서의 변화와 반대로 저장되기 때문에 [::-1]로 새로운 스택은 합쳐줘야한다.당연히 sys.stin.readline()으로 입력받아야한다.# 시간 초과된 insert와 defword_list=list(input())point=int(input())j=len(word_list)for i in range(point):    command=input()    if command[0]=='L' and j!=0:        j=j-1             elif command[0]=='D' and j!=len(word_list):        j=j+1        elif command[0]=='B' and j != 0:        del word_list[j-1]        j=j-1            elif command[0]=='P':        if j==len(word_list):            word_list.insert(len(word_list)+1,command[2])            j=j+1            print(j)        elif j==0:            word_list.insert(j,command[2])            print(j)        else:            word_list.insert(j,command[2])            print(''.join(word_list))dmih11BBP x3LBBBP y0DDP z3yxzword_list=list(input())point=int(input())j=len(word_list)stack=[]for i in range(point):    command=input()    if command[0]=='L' and len(word_list) !=0:        stack.append(word_list.pop())    elif command[0]=='D' and len(stack) !=0:        word_list.append(stack.pop())    elif command[0]=='B'and len(word_list) !=0:        word_list.pop()    elif command[0]=='P':        word_list.append(command[2])print(word_list)print(stack)word_list.extend(stack[::-1])    print(''.join(word_list))abc9LLLLLP xLBP y['y']['c', 'b', 'a', 'x']yxabc2493번 탑https://www.acmicpc.net/problem/2493answer은 정답을 기록하는 용도로, stack은 비교하는 탑과 인덱스를 저장한다.즉, for문으로 레이저가 출발하는 송전탑을 고르고, stack에는 이 전에 지나온 송전탑들이 저장된다.i값은 인덱스보다 1 작기 때문에 답에는 1을 추가해줘야한다.n = int(input()) #5top = list(map(int, input().split())) #[6 9 5 7]stack = [] #[]answer = [0 for i in range(n)] #[0 0 0 0 4] for i in range(n):    while stack:        if stack[-1][1] &gt; top[i]:            answer[i] = stack[-1][0] + 1            break        else:            stack.pop()    stack.append([i, top[i]]) print(*answer)56 9 5 7 40 0 2 2 4a=[1,2,3]print(*a)1 2 3",
        "url": "/programming-baekjoon7"
    }
    ,
    
    "study-ml3": {
        "title": "머신러닝 정리 (3) &lt;br&gt; 비지도학습 (1)",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학머신러닝 정리 (3) - 비지도학습 (1)본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python비지도학습 (1)  비지도 학습의 종류  비지도 학습의 도전과제  데이터 전처리와 스케일 조정  여러 가지 전처리 방법          StandardScaler      RobustScaler      MinMaxScaler      Nomarlizer        데이터 변환 적용하기  Quantile Transformer 와 Power Transformer          Quantile Transformer      Power Transformer        훈련 데이터와 테스트 데이터의 스케일을 같은 방법으로 조정하기  지도 학습에서 데이터 전처리 효과  차원 축소, 특성 추출, 매니폴드 학습          주성분 분석(PCA)                  유방암 데이터 셋 시각화          고유얼굴 특성 추출                    비지도 학습의 종류책에서는 두 가지 비지도 학습을 공부합니다.비지도 변환과 군집입니다.비지도 변환은 데이터를 새롭게 표현하여 사람이나 다른 머신러닝 알고리즘이 원래 데이터보다 쉽게 해석할 수 있도록 만드는 알고리즘입니다.특히 고차원 데이터를 특성의 수를 줄이면서 꼭 필요한 특징을 포함한 데이터로 표현하는 방법인 차원축소가 대표적인 예입니다.비지도 변환으로 데이터를 구성하는 단위나 성분을 찾기도 합니다.텍스트 문서에서 주제를 추출하는 것이 예입니다.소셜 미디어에서 선거, 총기 규제, 팝스타 같은 주제로 일어나는 토론을 추적할 때 사용한다고 합니다.군집 알고리즘은 데이터를 서로 비슷하게 그룹으로 묶는 것 입니다.같은 사람이 찍힌 사진을 같은 그룹으로 묶게 추천해주는 것을 생각하면 이해하기 쉽습니다.비지도 학습의 도전과제비지도 학습에서 가장 어려운 일은 알고리즘이 유용한 가를 평가하는 것 입니다.비지도 학습은 보통 레이블이 없어서 어떤 것이 올바른 것인지 모릅니다.그래서 비지도 학습의 결과를 평가하기 위해서는 직접 확인하는 것이 유일한 방법일 때도 있다고 합니다.비지도 학습 알고리즘은 데이터를 잘 이해하고 싶을 때 탐색적 분석 단계에서도 많이 사용합니다.전처리 단계에서도 사용되는데 비지도 학습 결과를 사용하여 학습하면 지도 학습의 정확도가 좋아지기도 하고 메모리나 시간 절약도 할 수 있습니다.데이터 전처리와 스케일 조정신경망이나 SVM과 같은 스케일에 민감한 알고리즘은 데이터 특성 값을 조정해야합니다.여러 가지 전처리 방법StandardSclaer각 특성의 평균을 0, 분산을 1로 변경하여 모든 특성이 같은 크기를 가지게 합니다.특성의 최솟값과 최댓값의 크기를 제한하지는 않습니다.\\[z= \\frac {x−μ} σ ( z점수= \\frac{자료값-평균} {표준편차})\\]RobustScaler특성이 같은 스케일을 갖게되지만 평균과 분산 대신 중간 값과 사분위 값을 사용합니다.따라서 이상치에 영향을 받지 않습니다.MinMaxScaler모든 특성이 정확하게 0과 1 사이에 위치하도록 변경합니다.2차원 데이터셋일 경우에는 모든 데이터가 x 축의 0과 1, y축의 0과 1 사이의 사각 영역에 담기게 됩니다.(q는 각 사분위값을 뜻합니다.)\\[\\frac {x-q~2} {q~3-q~1}\\]Nomarlizer특성 벡터의 유클리디안 길이가 1이 되도록 데이터 포인트를 조정합니다.다른 말로 하면 지름이 1인 원(3차원에서는 구)에 들어옵니다.각 데이터 포인트가 다른 비율로 조정됩니다.  특성 벡터의 길이는 상관 없고 데이터의 방향이 중요할 때 많이 사용합니다.데이터 변환 적용하기from sklearn.preprocessing import MinMaxScalerscaler = MinMaxScaler()scaler.fit(X_train)X_train_scaled = scaler.transform(X_train)print(\"변환된 후 크기:\", X_train_scaled.shape)print(\"스케일 조정 전 특성별 최소값:\\n\", X_train.min(axis=0))print(\"스케일 조정 전 특성별 최대값:\\n\", X_train.max(axis=0))print(\"스케일 조정 후 특성별 최소값:\\n\", X_train_scaled.min(axis=0))print(\"스케일 조정 후 특성별 최대값:\\n\", X_train_scaled.max(axis=0))변환된 후 크기: (426, 30)스케일 조정 전 특성별 최소값:[  6.981   9.71   43.79  143.5     0.053   0.019   0.      0.      0.1060.05    0.115   0.36    0.757   6.802   0.002   0.002   0.      0.0.01    0.001   7.93   12.02   50.41  185.2     0.071   0.027   0.0.      0.157   0.055]스케일 조정 전 특성별 최대값:[  28.11    39.28   188.5   2501.       0.163    0.287    0.427    0.201    0.304    0.096    2.873    4.885   21.98   542.2      0.031    0.135    0.396    0.053    0.061    0.03    36.04    49.54   251.2   4254.    0.223    0.938    1.17     0.291    0.577    0.149]스케일 조정 후 특성별 최소값:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.0. 0. 0. 0. 0. 0.]스케일 조정 후 특성별 최대값:[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.1. 1. 1. 1. 1. 1.]변환 된 값들이 모두 0과 1 사이가 된 것을 알 수 있습니다.X_test_scaled = scaler.transform(X_test)print(\"스케일 조정 후 특성별 최소값:\\n\", X_test_scaled.min(axis=0))print(\"스케일 조정 후 특성별 최대값:\\n\", X_test_scaled.max(axis=0))스케일 조정 후 특성별 최소값:[ 0.034  0.023  0.031  0.011  0.141  0.044  0.     0.     0.154 -0.006-0.001  0.006  0.004  0.001  0.039  0.011  0.     0.    -0.032  0.0070.027  0.058  0.02   0.009  0.109  0.026  0.     0.    -0.    -0.002]스케일 조정 후 특성별 최대값:[0.958 0.815 0.956 0.894 0.811 1.22  0.88  0.933 0.932 1.037 0.427 0.4980.441 0.284 0.487 0.739 0.767 0.629 1.337 0.391 0.896 0.793 0.849 0.7450.915 1.132 1.07  0.924 1.205 1.631]테스트 세트의 최솟값과 최댓값은 0과 1이 아닐 수 있다.테스트 세트의 최솟값과 범위를 사용하지 않고 훈련 세트의 최솟값을 빼고 훈련 세트의 범위로 나누기 때문이다.MinMaxScaler을 사용하려면 항상 테스트와 훈련 세트 모두 같은 변환을 해야한다.Quantile Transformer 와 Power TransformerQuantile TransformerQuantile Transformer은 1000개의 분위를 사용하여 데이터를 균등하게 분포시킵니다.RobustScaler과 비슷하게 이상치에 민감하지 않으며 전체 데이터를 0과 1 사이로 압축합니다.분위 수는 n_quantiles 매개 변수로 설정할 수 있으며 속성의 크기는 (n_quantiles,n_features) 입니다.output_distribution 매개변수를 통해 균등 분포에서 정규 분포로 출력을 바꿀 수도 있습니다.Power TransformerPower Transformer는 method 매개변수에 ‘yeo-johnson’과 ‘box-cox’ 알고리즘을 지정할 수 있습니다.어떤 변환이 정규 분포에 가깝게 변환할지 사전에 알기 힘들기 때문에 히스토그램으로 확인해보는 것이 좋습니다.훈련 데이터와 테스트 데이터의 스케일을 같은 방법으로 조정하기지도 학습에서는 훈련 세트와 테스트 세트에 같은 변환을 적용하는 것이 중요합니다.잘 조정된 데이터는 같은 비율로 데이터를 바꿔 원본 데이터와 비율만 다른 그래프를 보여주지만 잘못 조정된 데이터는 배열이 엉망이 되어서 결과에 영향을 미칩니다.지도 학습에서 데이터 전처리 효과from sklearn.svm import SVCX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)svm = SVC(gamma='auto')svm.fit(X_train, y_train)print(\"테스트 세트 정확도: {:.2f}\".format(svm.score(X_test, y_test)))테스트 세트 정확도: 0.63# 0~1 사이로 스케일 조정scaler = MinMaxScaler()scaler.fit(X_train)X_train_scaled = scaler.transform(X_train)X_test_scaled = scaler.transform(X_test)svm.fit(X_train_scaled, y_train)print(\"스케일 조정된 테스트 세트의 정확도: {:.2f}\".format(svm.score(X_test_scaled, y_test)))스케일 조정된 테스트 세트의 정확도: 0.95# 평균 0, 분산 1을 갖도록 스케일 조정from sklearn.preprocessing import StandardScalerscaler = StandardScaler()scaler.fit(X_train)X_train_scaled = scaler.transform(X_train)X_test_scaled = scaler.transform(X_test)svm.fit(X_train_scaled, y_train)print(\"SVM 테스트 정확도: {:.2f}\".format(svm.score(X_test_scaled, y_test)))SVM 테스트 정확도: 0.97차원 축소, 특성 추출, 매니폴드 학습주성분 분석은 가장 많이 사용되는 데이터 변환 방법입니다.특성 추출에서는 비음수 행렬 분해가 많이 사용됩니다.2차원 산점도를 이용한 시각화 용도로 사용되는 t-SNE 알고리즘도 있습니다.차원 축소  차원 축소의 필요성관측 치의 수는 한정되어 있습니다.차원이 커질 수록 한정된 자료는 커진 차원의 패턴을 잘 설명하지 못하고 복잡도가 기하급수적으로 늘어나게 됩니다.상관계수가 높은 변수 중 일부만 분석하게 된다면 정보의 손실이 발생하게 됩니다.  차원 축소의 여러 방법    &gt; principal component    &gt; 변수 선택법    &gt; penalty 기반 regression    &gt; convolutional neural network    &gt; drop out &amp; bagging  차원 축소의 활용    &gt; 차원 축소를 통해 데이터를 잘 설명할 수 있는 잠재적 요소 추출    &gt; 이미지 분류, 시맨틱, 토픽 분류 등주성분 분석(PCA)공분산 행렬 개념은 다음과 같습니다.Principal Components의 개념은 차원을 줄이면서 정보 손실을 최소화하는 방법입니다.더 적은 개수로 데이터를 충분히 잘 설명할 수 있는 새로운 축을 찾아냅니다.공분산이 데이터의 형태를 변형시키는 방향의 축과 그것에 직교하는 축을 찾습니다.2차원의 경우 공분산이 나타내는 타원의 장축과 단축입니다.PC score은 새로 찾아낸 축에서의 좌표값을 의미하는데 기존 값을 새로운 축에 내린 정사영입니다.즉, PCA는 데이터의 분산을 최대한 보존하면서 서로 직교하는 새 기저(축)을 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법입니다.데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있고, 이렇게 분해된 고유벡터를 이용해 입력데이터를 선형변환하는 것이 PCA입니다.PCA의 수학적개념_Singular Value Decomposition(SVD)SVD와 eigen value, eigen vector의 연관성주성분 분석은 특성들이 통계적으로 상관관계가 없도록 데이터셋을 회전시키는 기술입니다.회전한 뒤에 데이터를 설명하는 데 중요한 새로운 특성 중 일부만 선택합니다.주성분 찾기  분산이 가장 큰 가장 많은 정보를 가지고 있는 방향을 찾는다.  첫 번째 방향과 직각인 방향 중에서 가장 많은 정보를 가지고 있는 방향을 찾는다.두 번째 그래프는 주성분 1과 2를 x 축과 y 축에 나란히 회전한 것으로 변환된 데이터의 상관관계 행렬이 대각선 방향을 제외하고 0이 됩니다.세 번째 그래프는 차원 축소 용도로 사용될 수 있습니다.  첫 번째 주성분만 유지하므로 2차원 데이터 셋이 1차원 데이터 셋으로 차원 감소 합니다.단순히 원본 특성 중 하나만 남기는 것이 아닌 가장 유용한 방향을 찾아 그 성분을 유지하는 것입니다.마지막 그래프는 데이터에 다시 평균을 더하고 반대로 회전시킨 그래프 입니다.원래 특성 공간에 있지만 첫 번째 주성분의 정보만 가지고 있습니다.보통 노이즈 제거나 주성분에서 유지되는 정보의 시각화를 위해 사용됩니다.유방암 데이터 셋 시각화유방암 데이터는 특성이 30개를 가지고 있기 때문에 너무 많은 산점도를 요구합니다.따라서 히스토그램을 그리는 방법이 있지만 히스토그램은 특성 간의 상호작용이나 클래스와의 곤계를 알려주지 못합니다.따라서 PCA를 통해 데이터를 회전시키고 차원을 축소합니다.그러면 다음과 같이 2차원 공간에서도 잘 구분됩니다.분류를 할 때 좋은 결과를 얻을 수 있을 것 같은 그래프입니다.하지만 두 축을 해석하기 어렵다는 단점을 가지고 있습니다.따라서 pca.components_를 통해서 중요도를 확인할 수 있습니다.고유얼굴 특성 추출PCA는 특성 추출에도 이용됩니다.RGB 강도로 기록된 픽셀들을 분류하는데 유용합니다.픽셀을 사용해서 두 이미지를 비교할 때, 각 픽셀의 회색톤 값을 다른 이미지에서 동일한 위치에 있는 픽셀 값과 비교합니다.하지만 이는 사람이 얼굴을 인식하는 것과 많이 다르고 특징을 잡기 어렵습니다.따라서 주성분으로 변환하여 거리를 계산하면 정확도가 높아집니다.PCA의 화이트닝 옵션을 사용하면 주성분의 스케일이 같습니다.화이트닝 옵션은 StandardScaler과 동일한 방식입니다.PCA 모델은 픽셀을 기반으로 하므로 얼굴의 배치와 조명이 비슷한 이미지를 판단하는 데 큰 영향을 줍니다.따라서 테스트 포인트를 주성분의 가중치 합으로 나타내는 것에 PCA 변환을 사용하는 방법도 해석 중 한 가지 방법입니다.원본 데이터를 재구성하는 방법도 PCA 모델의 해석 방법 중 한가지입니다.",
        "url": "/study-ML3"
    }
    ,
    
    "programming-kaggle2": {
        "title": "캐글 (2) &lt;br&gt; 타이타닉 튜토리얼 1,2 공부하기",
            "author": "keonju",
            "category": "",
            "content": "kaggle 관련 글    캐글 (1) Simple Matplotlib &amp; Visualization Tips 공부하기    캐글 (2) 타이타닉 튜토리얼 1,2 공부하기    캐글 (3) 타이타닉 생존자 EDA 부터 분류까지타이타닉 튜토리얼 1,2 공부하기kaggle 타이타닉 튜토리얼을 필사하였다.해당 유튜브를 따라서 필사하였고 블로그에 자세한 설명도 나와있었다.https://www.youtube.com/watch?v=_iqz7tFhox0&amp;list=PLC_wC_PMBL5MnqmgTLqDgu4tO8mrQakuFhttps://kaggle-kr.tistory.com/17?category=868316https://kaggle-kr.tistory.com/18?category=868316# 분석에 필요한 패키지import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns# plt의 스타일 지정plt.style.use('seaborn')sns.set(font_scale=2.5) # 결측치를 알기 쉽게 하는 패키지import missingno as msno# warning무시import warningswarnings.filterwarnings('ignore')# notebook에서 바로 그림 확인하는 코드%matplotlib inline# 데이터 불러오기df_train=pd.read_csv('train.csv')df_test=pd.read_csv('test.csv')df_test.head()                  PassengerId      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked                  0      892      3      Kelly, Mr. James      male      34.5      0      0      330911      7.8292      NaN      Q              1      893      3      Wilkes, Mrs. James (Ellen Needs)      female      47.0      1      0      363272      7.0000      NaN      S              2      894      2      Myles, Mr. Thomas Francis      male      62.0      0      0      240276      9.6875      NaN      Q              3      895      3      Wirz, Mr. Albert      male      27.0      0      0      315154      8.6625      NaN      S              4      896      3      Hirvonen, Mrs. Alexander (Helga E Lindqvist)      female      22.0      1      1      3101298      12.2875      NaN      S      # 데이터 확인df_train.head()                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      7.2500      NaN      S              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      71.2833      C85      C              2      3      1      3      Heikkinen, Miss. Laina      female      26.0      0      0      STON/O2. 3101282      7.9250      NaN      S              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      35.0      1      0      113803      53.1000      C123      S              4      5      0      3      Allen, Mr. William Henry      male      35.0      0      0      373450      8.0500      NaN      S      # 데이터 행렬 확인df_train.shape(891, 12)# 데이터 셋 특징 확인df_train.describe()                  PassengerId      Survived      Pclass      Age      SibSp      Parch      Fare                  count      891.000000      891.000000      891.000000      714.000000      891.000000      891.000000      891.000000              mean      446.000000      0.383838      2.308642      29.699118      0.523008      0.381594      32.204208              std      257.353842      0.486592      0.836071      14.526497      1.102743      0.806057      49.693429              min      1.000000      0.000000      1.000000      0.420000      0.000000      0.000000      0.000000              25%      223.500000      0.000000      2.000000      20.125000      0.000000      0.000000      7.910400              50%      446.000000      0.000000      3.000000      28.000000      0.000000      0.000000      14.454200              75%      668.500000      1.000000      3.000000      38.000000      1.000000      0.000000      31.000000              max      891.000000      1.000000      3.000000      80.000000      8.000000      6.000000      512.329200      # max 값만 확인df_train.max()PassengerId                            891Survived                                 1Pclass                                   3Name           van Melkebeke, Mr. PhilemonSex                                   maleAge                                     80SibSp                                    8Parch                                    6Ticket                           WE/P 5735Fare                               512.329dtype: objectdf_test.describe()                  PassengerId      Pclass      Age      SibSp      Parch      Fare                  count      418.000000      418.000000      332.000000      418.000000      418.000000      417.000000              mean      1100.500000      2.265550      30.272590      0.447368      0.392344      35.627188              std      120.810458      0.841838      14.181209      0.896760      0.981429      55.907576              min      892.000000      1.000000      0.170000      0.000000      0.000000      0.000000              25%      996.250000      1.000000      21.000000      0.000000      0.000000      7.895800              50%      1100.500000      3.000000      27.000000      0.000000      0.000000      14.454200              75%      1204.750000      3.000000      39.000000      1.000000      0.000000      31.500000              max      1309.000000      3.000000      76.000000      8.000000      9.000000      512.329200      # column값 확인df_train.columnsIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],      dtype='object')# 각 column의 null 데이터 비율 확인 {:&gt;10}:오른쪽 정렬for col in df_train.columns:    msg='column: {:&gt;10}\\t Percent of NaN value: {:.2f}%'.format(col,100*(df_train[col].isnull().sum()/df_train[col].shape[0]))    print(msg)column: PassengerId\t Percent of NaN value: 0.00%column:   Survived\t Percent of NaN value: 0.00%column:     Pclass\t Percent of NaN value: 0.00%column:       Name\t Percent of NaN value: 0.00%column:        Sex\t Percent of NaN value: 0.00%column:        Age\t Percent of NaN value: 19.87%column:      SibSp\t Percent of NaN value: 0.00%column:      Parch\t Percent of NaN value: 0.00%column:     Ticket\t Percent of NaN value: 0.00%column:       Fare\t Percent of NaN value: 0.00%column:      Cabin\t Percent of NaN value: 77.10%column:   Embarked\t Percent of NaN value: 0.22%# 값 확인df_train[col]0      S1      C2      S3      S4      S      ..886    S887    S888    S889    C890    QName: Embarked, Length: 891, dtype: object# null 값 확인df_train[col].isnull()0      False1      False2      False3      False4      False       ...  886    False887    False888    False889    False890    FalseName: Embarked, Length: 891, dtype: bool# null 값의 합df_train[col].isnull().sum()2# shape를 통해 총 데이터 갯수 확인하기df_train[col].isnull().sum()/df_train[col].shape[0]0.002244668911335578for col in df_test.columns:    msg='column: {:&gt;10}\\t Percent of NaN value: {:.2f}%'.format(col,100*(df_test[col].isnull().sum()/df_test[col].shape[0]))    print(msg)column: PassengerId\t Percent of NaN value: 0.00%column:     Pclass\t Percent of NaN value: 0.00%column:       Name\t Percent of NaN value: 0.00%column:        Sex\t Percent of NaN value: 0.00%column:        Age\t Percent of NaN value: 20.57%column:      SibSp\t Percent of NaN value: 0.00%column:      Parch\t Percent of NaN value: 0.00%column:     Ticket\t Percent of NaN value: 0.00%column:       Fare\t Percent of NaN value: 0.24%column:      Cabin\t Percent of NaN value: 78.23%column:   Embarked\t Percent of NaN value: 0.00%# missingno를 통해 확인하기msno.matrix(df=df_train.iloc[:,:],figsize=(8,8),color=(0.8,0.5,0.2))&lt;AxesSubplot:&gt;# iloc으로 가져오고 싶은 위치 찾기df_train.iloc[:,-1]0      S1      C2      S3      S4      S      ..886    S887    S888    S889    C890    QName: Embarked, Length: 891, dtype: objectmsno.bar(df=df_train.iloc[:,:],figsize=(8,8),color=(0.8,0.5,0.2))&lt;AxesSubplot:&gt;# pie plot과 count-plot 그래프 그리기# 도화지를 준비하는 과정 (1,2): 행렬 f, ax=plt.subplots(1,2,figsize=(18,8)) #'Survived'에 있는 값 count하기, 떨어뜨리기, 글자 규칙, 그리는 위치, 그림자df_train['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)ax[0].set_title('Pie plot-Survived') # 제목ax[0].set_ylabel('')sns.countplot('Survived',data=df_train,ax=ax[1]) #countplot을 [1] 위치에 그리기ax[1].set_title('Count plot-Survived')plt.show()2.1 PClass# class 별 생존자 수 count는 객체가 몇명인가df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=True).count()                  Survived              Pclass                        1      216              2      184              3      491      # sum은 숫자 자체의 데이터의 갯수 [0,1]에서 1을 다 더한 값df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=True).sum()                  Survived              Pclass                        1      136              2      87              3      119      # crosstab을 통해 비교 (margin은 All 표현,style.background_gradient를 통해 색상 조절)pd.crosstab(df_train['Pclass'],df_train['Survived'],margins=True).style.background_gradient(cmap='summer_r')            Survived        0        1        All                Pclass                                                                    1                        80                        136                        216                                                2                        97                        87                        184                                                3                        372                        119                        491                                                All                        549                        342                        891                # 평균 알아보기 (as_index를 통해 그래프 그리기 설정, sort_values를 통한 오름차순, ascending=False는 내림차순)df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=True).mean().sort_values(by='Survived',ascending=False)                  Survived              Pclass                        1      0.629630              2      0.472826              3      0.242363      # 그래프 기리기df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=True).mean().sort_values(by='Survived',ascending=False).plot()&lt;AxesSubplot:xlabel='Pclass'&gt;# as_index=False일때는 Pclass도 같이 그린다df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived',ascending=False).plot()&lt;AxesSubplot:&gt;# 막대그래프 그리기df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=True).mean().sort_values(by='Survived',ascending=False).plot.bar()&lt;AxesSubplot:xlabel='Pclass'&gt;y_position=1.02f,ax=plt.subplots(1,2,figsize=(18,8))# Class별 탑승자 수df_train['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])ax[0].set_title('Number of passenger By Pclass',y=y_position)ax[0].set_ylabel('Count')# Class별 Survived와 Dead 구분 (hue를 통해 색깔 구분)sns.countplot('Pclass',hue='Survived',data=df_train,ax=ax[1])ax[1].set_title('Pclass:Survived vs Dead',y=y_position)plt.show()2.2 Sexf,ax=plt.subplots(1,2,figsize=(18,8))df_train[['Sex','Survived']].groupby(['Sex'],as_index=True).mean().plot.bar(ax=ax[0])ax[0].set_title('Survived vs Sex')sns.countplot('Sex',hue='Survived',data=df_train,ax=ax[1])ax[1].set_title('Sex:Survived vs Dead')plt.show()df_train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean()                  Sex      Survived                  0      female      0.742038              1      male      0.188908      pd.crosstab(df_train['Sex'],df_train['Survived'],margins=True).style.background_gradient(cmap='summer_r')            Survived        0        1        All                Sex                                                                    female                        81                        233                        314                                                male                        468                        109                        577                                                All                        549                        342                        891                2.2 Both Sex and Pclass# factorplot 그래프 그리기# 선은 error barsns.factorplot('Pclass','Survived',hue='Sex',data=df_train,size=6,aspect=1.5)&lt;seaborn.axisgrid.FacetGrid at 0x2248b7c1c10&gt;# 축과 보는 방향을 바꾼 것sns.factorplot(x='Sex',y='Survived',col='Pclass',data=df_train,saturation=.5,size=9,aspect=1)&lt;seaborn.axisgrid.FacetGrid at 0x2248b8ad310&gt;sns.factorplot(x='Sex',y='Survived',hue='Pclass',data=df_train,saturation=.5,size=9,aspect=1)&lt;seaborn.axisgrid.FacetGrid at 0x2248ba293a0&gt;Ageprint('제일 나이 많은 탑승객: {:.1f} years'.format(df_train['Age'].max()))print('제일 나이 어린 탑승객: {:.1f} years'.format(df_train['Age'].min()))print('탑승객 평균 나이: {:.1f} years'.format(df_train['Age'].mean()))제일 나이 많은 탑승객: 80.0 years제일 나이 어린 탑승객: 0.4 years탑승객 평균 나이: 29.7 years# kdeplot(커널 밀도 함수) 그리기 (히스토그램과 유사)fig,ax=plt.subplots(1,1,figsize=(9,5))sns.kdeplot(df_train[df_train['Survived']==1]['Age'],ax=ax)sns.kdeplot(df_train[df_train['Survived']==0]['Age'],ax=ax)plt.legend(['Survived'==1,'Survived'==0])plt.show()# 히스토그램df_train[df_train['Survived']==1]['Age'].hist()&lt;AxesSubplot:&gt;그래프 그리는 다양한 방법f=plt.figure(figsize=(10,10))a=np.arange(100)b=np.sin(a)plt.plot(b)[&lt;matplotlib.lines.Line2D at 0x2248d260070&gt;]f,ax=plt.subplots(1,1,figsize=(10,10))a=np.arange(100)b=np.sin(a)plt.plot(b)[&lt;matplotlib.lines.Line2D at 0x2248d2c4040&gt;]plt.figure(figsize=(10,10))a=np.arange(100)b=np.sin(a)plt.plot(b)[&lt;matplotlib.lines.Line2D at 0x2248bd55280&gt;]# 탑승객의 연령별 분포plt.figure(figsize=(8,6))df_train['Age'][df_train['Pclass']==1].plot(kind='kde')df_train['Age'][df_train['Pclass']==2].plot(kind='kde')df_train['Age'][df_train['Pclass']==3].plot(kind='kde')plt.xlabel('Age')plt.title('Age Distribution within classes')plt.legend(['1st Class','2nd Class','3rd Class'])&lt;matplotlib.legend.Legend at 0x2248d2f8a00&gt;# 히스토그램은 겹치면 보이지 않음plt.figure(figsize=(8,6))df_train['Age'][df_train['Pclass']==1].plot(kind='hist')df_train['Age'][df_train['Pclass']==2].plot(kind='hist')df_train['Age'][df_train['Pclass']==3].plot(kind='hist')plt.xlabel('Age')plt.title('Age Distribution within classes')plt.legend(['1st Class','2nd Class','3rd Class'])&lt;matplotlib.legend.Legend at 0x2248b74fd30&gt;fig,ax=plt.subplots(1,1,figsize=(9,5))sns.kdeplot(df_train[(df_train['Survived']==0)&amp;(df_train['Pclass']==1)]['Age'],ax=ax)sns.kdeplot(df_train[(df_train['Survived']==1)&amp;(df_train['Pclass']==1)]['Age'],ax=ax)plt.legend(['Survived==1','Survived==0'])plt.title('1st class')plt.show()# 히스토그램은 겹치면 보이지 않음plt.figure(figsize=(8,6))df_train['Age'][(df_train['Pclass']==1)&amp;(df_train['Survived']==0)].plot(kind='hist')df_train['Age'][(df_train['Pclass']==1)&amp;(df_train['Survived']==1)].plot(kind='hist')plt.xlabel('Age')plt.title('Age Distribution within classes')Text(0.5, 1.0, 'Age Distribution within classes')fig,ax=plt.subplots(1,1,figsize=(9,5))sns.kdeplot(df_train[(df_train['Survived']==0)&amp;(df_train['Pclass']==2)]['Age'],ax=ax)sns.kdeplot(df_train[(df_train['Survived']==1)&amp;(df_train['Pclass']==2)]['Age'],ax=ax)plt.legend(['Survived==1','Survived==0'])plt.title('2nd class')plt.show()# 히스토그램은 겹치면 보이지 않음plt.figure(figsize=(8,6))df_train['Age'][(df_train['Pclass']==2)&amp;(df_train['Survived']==0)].plot(kind='hist')df_train['Age'][(df_train['Pclass']==2)&amp;(df_train['Survived']==1)].plot(kind='hist')plt.xlabel('Age')plt.title('Age Distribution within classes')Text(0.5, 1.0, 'Age Distribution within classes')fig,ax=plt.subplots(1,1,figsize=(9,5))sns.kdeplot(df_train[(df_train['Survived']==0)&amp;(df_train['Pclass']==3)]['Age'],ax=ax)sns.kdeplot(df_train[(df_train['Survived']==1)&amp;(df_train['Pclass']==3)]['Age'],ax=ax)plt.legend(['Survived==1','Survived==0'])plt.title('3rd class')plt.show()# 히스토그램은 겹치면 보이지 않음plt.figure(figsize=(8,6))df_train['Age'][(df_train['Pclass']==3)&amp;(df_train['Survived']==0)].plot(kind='hist')df_train['Age'][(df_train['Pclass']==3)&amp;(df_train['Survived']==1)].plot(kind='hist')plt.xlabel('Age')plt.title('Age Distribution within classes')Text(0.5, 1.0, 'Age Distribution within classes')change_age_range_survival_ratio=[]for i in range(1,80):    change_age_range_survival_ratio.append(df_train[df_train['Age']&lt;i]['Survived'].sum()/len(df_train[df_train['Age']&lt;i]['Survived']))    plt.figure(figsize=(7,7))plt.plot(change_age_range_survival_ratio)plt.title('Survial rate change depending on range of Age',y=1.02)plt.ylabel=('Survival rate')plt.xlabel('Range of Age(0~x)')plt.show()i=10df_train[df_train['Age']&lt;i]['Survived'].sum() / len(df_train[df_train['Age']&lt;i]['Survived'])0.6129032258064516Pclass, Sex, Agef,ax=plt.subplots(1,2,figsize=(18,8))sns.violinplot('Pclass','Age',hue='Survived',data=df_train,scale='count',split=True,ax=ax[0])ax[0].set_title('Pclass and Age vs Survived')ax[0].set_yticks(range(0,110,10))sns.violinplot('Sex','Age',hue='Survived',data=df_train,scale='count',split=True,ax=ax[1])ax[1].set_title('Sex and Age vs Survived')ax[1].set_yticks(range(0,110,10))plt.show()# split=Falsef,ax=plt.subplots(1,2,figsize=(18,8))sns.violinplot('Pclass','Age',hue='Survived',data=df_train,scale='count',split=False,ax=ax[0])ax[0].set_title('Pclass and Age vs Survived')ax[0].set_yticks(range(0,110,10))sns.violinplot('Sex','Age',hue='Survived',data=df_train,scale='count',split=False,ax=ax[1])ax[1].set_title('Sex and Age vs Survived')ax[1].set_yticks(range(0,110,10))plt.show()# scale 차이 같은 면적이기 때문에 count보다 숫자의 개념이 보기 힘듬f,ax=plt.subplots(1,2,figsize=(18,8))sns.violinplot('Pclass','Age',hue='Survived',data=df_train,scale='area',split=False,ax=ax[0])ax[0].set_title('Pclass and Age vs Survived')ax[0].set_yticks(range(0,110,10))sns.violinplot('Sex','Age',hue='Survived',data=df_train,scale='area',split=False,ax=ax[1])ax[1].set_title('Sex and Age vs Survived')ax[1].set_yticks(range(0,110,10))plt.show()Embarked# Embarked 비율f, ax= plt.subplots(1,1, figsize=(7,7))df_train[['Embarked','Survived']].groupby(['Embarked'],as_index=True).mean().sort_values(by='Survived',ascending=False).plot.bar(ax=ax)&lt;AxesSubplot:xlabel='Embarked'&gt;# sort_valuesdf_train[['Embarked','Survived']].groupby(['Embarked'],as_index=True).mean().sort_values(by='Survived')                  Survived              Embarked                        S      0.336957              Q      0.389610              C      0.553571      # 내림차순df_train[['Embarked','Survived']].groupby(['Embarked'],as_index=True).mean().sort_values(by='Survived',ascending=False)                  Survived              Embarked                        C      0.553571              Q      0.389610              S      0.336957      # sort_indexdf_train[['Embarked','Survived']].groupby(['Embarked'],as_index=True).mean().sort_index()                  Survived              Embarked                        C      0.553571              Q      0.389610              S      0.336957      f, ax=plt.subplots(2,2,figsize=(20,15))sns.countplot('Embarked',data=df_train,ax=ax[0,0])ax[0,0].set_title('(1) No. Of Passengers Boarded')sns.countplot('Embarked', hue='Sex',data=df_train,ax=ax[0,1])ax[0,1].set_title('(2) Male-Feamle split for embarked')sns.countplot('Embarked', hue='Survived',data=df_train,ax=ax[1,0])ax[1,0].set_title('(3) Embarked vs Survived')sns.countplot('Embarked', hue='Pclass',data=df_train,ax=ax[1,1])ax[1,1].set_title('(4) Embarked vs Pclass')# 좌우간격, 상하간격 맞추기plt.subplots_adjust(wspace=0.2,hspace=0.5)plt.show()Family - Sibsp + Parchdf_train['FamilySize']=df_train['SibSp']+df_train['Parch']+1print('Maximum size of Family:',df_train['FamilySize'].max())print('Minimum size of Family:',df_train['FamilySize'].min())Maximum size of Family: 11Minimum size of Family: 1f, ax=plt.subplots(1,3,figsize=(40,10))sns.countplot('FamilySize',data=df_train,ax=ax[0])ax[0].set_title('(1) No. Of Passenger Boarded',y=1.02)sns.countplot('FamilySize',hue='Survived',data=df_train,ax=ax[1])ax[1].set_title('(2) Survived countplot depending on FamilSize',y=1.02)df_train[['FamilySize','Survived']].groupby(['FamilySize'],as_index=True).mean().sort_values(by='Survived',ascending=False).plot.bar(ax=ax[2])ax[2].set_title('(3) Survived rate depending on FamilySize',y=1.02)plt.subplots_adjust(wspace=0.2,hspace=0.5)plt.show()Faredf_test.loc[df_test.Fare.isnull(), 'Fare'] = df_test['Fare'].mean()df_train['Fare'] = df_train['Fare'].map(lambda i: np.log(i) if i &gt; 0 else 0)df_test['Fare'] = df_test['Fare'].map(lambda i: np.log(i) if i &gt; 0 else 0)fig, ax=plt.subplots(1,1,figsize=(8,8))g=sns.distplot(df_train['Fare'],color='b',label='Skweness {:.2f}'.format(df_train['Fare'].skew()),ax=ax)g=g.legend(loc='best')df_train['Fare']=df_train['Fare'].map(lambda i:np.log(i) if i&gt;0 else 0)df_train['Ticket'].value_counts()1601        7347082      7CA. 2343    7347088      63101295     6           ..PC 17318    131418       1345765      1244270      1244278      1Name: Ticket, Length: 681, dtype: int64Fill Null in Agedf_train['Age'].isnull().sum()177df_train['Age'].mean()29.69911764705882# str로 변환한 뒤 extract와 정규표현식을 통해 추출df_train['Initial']= df_train.Name.str.extract('([A-Za-z]+)\\.')df_test['Initial']= df_test.Name.str.extract('([A-Za-z]+)\\.') pd.crosstab(df_train['Initial'],df_train['Sex']).T.style.background_gradient(cmap='summer_r')            Initial        Capt        Col        Countess        Don        Dr        Jonkheer        Lady        Major        Master        Miss        Mlle        Mme        Mr        Mrs        Ms        Rev        Sir                Sex                                                                                                                                                                                    female                        0                        0                        1                        0                        1                        0                        1                        0                        0                        182                        2                        1                        0                        125                        1                        0                        0                                                male                        1                        2                        0                        1                        6                        1                        0                        2                        40                        0                        0                        0                        517                        0                        0                        6                        1                df_train['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)df_test['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)df_train.groupby('Initial').mean()                  PassengerId      Survived      Pclass      Age      SibSp      Parch      Fare      FamilySize              Initial                                                                  Master      414.975000      0.575000      2.625000      4.574167      2.300000      1.375000      1.190112      4.675000              Miss      411.741935      0.704301      2.284946      21.860000      0.698925      0.537634      1.085686      2.236559              Mr      455.880907      0.162571      2.381853      32.739609      0.293006      0.151229      0.932798      1.444234              Mrs      456.393701      0.795276      1.984252      35.981818      0.692913      0.818898      1.207905      2.511811              Other      564.444444      0.111111      1.666667      45.888889      0.111111      0.111111      0.958425      1.222222      df_train.groupby('Initial')['Survived'].mean().plot.bar()&lt;AxesSubplot:xlabel='Initial'&gt;df_train.loc[(df_train['Age'].isnull())&amp;(df_train['Initial']=='Mr'),'Age']=33df_train.loc[(df_train['Age'].isnull())&amp;(df_train['Initial']=='Mrs'),'Age']=36df_train.loc[(df_train['Age'].isnull())&amp;(df_train['Initial']=='Master'),'Age']=5df_train.loc[(df_train['Age'].isnull())&amp;(df_train['Initial']=='Miss'),'Age']=22df_train.loc[(df_train['Age'].isnull())&amp;(df_train['Initial']=='Other'),'Age']=46df_test.loc[(df_test['Age'].isnull())&amp;(df_test['Initial']=='Mr'),'Age']=33df_test.loc[(df_test['Age'].isnull())&amp;(df_test['Initial']=='Mrs'),'Age']=36df_test.loc[(df_test['Age'].isnull())&amp;(df_test['Initial']=='Master'),'Age']=5df_test.loc[(df_test['Age'].isnull())&amp;(df_test['Initial']=='Miss'),'Age']=22df_test.loc[(df_test['Age'].isnull())&amp;(df_test['Initial']=='Other'),'Age']=46df_train.loc[(df_train['Initial']=='Mr'),'Age'].isnull().sum&lt;bound method Series.sum of 0      False4      False5      False6      False12     False       ...  881    False883    False884    False889    False890    FalseName: Age, Length: 529, dtype: bool&gt;Fill Null in Embarked and categorize Agedf_train['Embarked'].isnull().sum()2df_train['Embarked'].fillna('S',inplace=True)df_train['Embarked'].isnull().sum()0df_train.head()                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked      FamilySize      Initial                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      0.683603      NaN      S      2      Mr              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      1.450832      C85      C      2      Mrs              2      3      1      3      Heikkinen, Miss. Laina      female      26.0      0      0      STON/O2. 3101282      0.727559      NaN      S      1      Miss              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      35.0      1      0      113803      1.379314      C123      S      2      Mrs              4      5      0      3      Allen, Mr. William Henry      male      35.0      0      0      373450      0.735091      NaN      S      1      Mr      df_train.loc[df_train['Age']&lt;10,'Age_cat']=0df_train.loc[(df_train['Age']&gt;=10)&amp;(df_train['Age']&lt;20),'Age_cat']=1df_train.loc[(df_train['Age']&gt;=20)&amp;(df_train['Age']&lt;30),'Age_cat']=2df_train.loc[(df_train['Age']&gt;=30)&amp;(df_train['Age']&lt;40),'Age_cat']=3df_train.loc[(df_train['Age']&gt;=40)&amp;(df_train['Age']&lt;50),'Age_cat']=4df_train.loc[(df_train['Age']&gt;=50)&amp;(df_train['Age']&lt;60),'Age_cat']=5df_train.loc[(df_train['Age']&gt;=60)&amp;(df_train['Age']&lt;70),'Age_cat']=6df_train.loc[df_train['Age']&gt;=70,'Age_cat']=7df_test.loc[df_test['Age']&lt;10,'Age_cat']=0df_test.loc[(df_test['Age']&gt;=10)&amp;(df_test['Age']&lt;20),'Age_cat']=1df_test.loc[(df_test['Age']&gt;=20)&amp;(df_test['Age']&lt;30),'Age_cat']=2df_test.loc[(df_test['Age']&gt;=30)&amp;(df_test['Age']&lt;40),'Age_cat']=3df_test.loc[(df_test['Age']&gt;=40)&amp;(df_test['Age']&lt;50),'Age_cat']=4df_test.loc[(df_test['Age']&gt;=50)&amp;(df_test['Age']&lt;60),'Age_cat']=5df_test.loc[(df_test['Age']&gt;=60)&amp;(df_test['Age']&lt;70),'Age_cat']=6df_test.loc[df_test['Age']&gt;=70,'Age_cat']=7df_train.head()                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked      FamilySize      Initial      Age_cat                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      0.683603      NaN      S      2      Mr      2.0              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      1.450832      C85      C      2      Mrs      3.0              2      3      1      3      Heikkinen, Miss. Laina      female      26.0      0      0      STON/O2. 3101282      0.727559      NaN      S      1      Miss      2.0              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      35.0      1      0      113803      1.379314      C123      S      2      Mrs      3.0              4      5      0      3      Allen, Mr. William Henry      male      35.0      0      0      373450      0.735091      NaN      S      1      Mr      3.0      df_test.head()                  PassengerId      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked      Initial      Age_cat                  0      892      3      Kelly, Mr. James      male      34.5      0      0      330911      2.057860      NaN      Q      Mr      3.0              1      893      3      Wilkes, Mrs. James (Ellen Needs)      female      47.0      1      0      363272      1.945910      NaN      S      Mrs      4.0              2      894      2      Myles, Mr. Thomas Francis      male      62.0      0      0      240276      2.270836      NaN      Q      Mr      6.0              3      895      3      Wirz, Mr. Albert      male      27.0      0      0      315154      2.159003      NaN      S      Mr      2.0              4      896      3      Hirvonen, Mrs. Alexander (Helga E Lindqvist)      female      22.0      1      1      3101298      2.508582      NaN      S      Mrs      2.0      def category_age(x):    if x&lt;10:        return 0    elif x&lt;20:        return 1    elif x&lt;30:        return 2    elif x&lt;40:        return 3    elif x&lt;50:        return 4    elif x&lt;60:        return 5    elif x&lt;70:        return 6    else:        return 7df_train['Age_cat_2']=df_train['Age'].apply(category_age)df_train.head()                  PassengerId      Survived      Pclass      Name      Sex      Age      SibSp      Parch      Ticket      Fare      Cabin      Embarked      FamilySize      Initial      Age_cat      Age_cat_2                  0      1      0      3      Braund, Mr. Owen Harris      male      22.0      1      0      A/5 21171      0.683603      NaN      S      2      Mr      2.0      2              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      38.0      1      0      PC 17599      1.450832      C85      C      2      Mrs      3.0      3              2      3      1      3      Heikkinen, Miss. Laina      female      26.0      0      0      STON/O2. 3101282      0.727559      NaN      S      1      Miss      2.0      2              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      35.0      1      0      113803      1.379314      C123      S      2      Mrs      3.0      3              4      5      0      3      Allen, Mr. William Henry      male      35.0      0      0      373450      0.735091      NaN      S      1      Mr      3.0      3      (df_train['Age_cat']==df_train['Age_cat_2']).all()Truedf_train.drop(['Age','Age_cat_2'],axis=1,inplace=True)df_test.drop(['Age'],axis=1,inplace=True)Change string to categorical and Pearson coefficientdf_train.Initial.unique()array(['Mr', 'Mrs', 'Miss', 'Master', 'Other'], dtype=object)df_train.loc[df_train['Initial']=='Master','Initial']7      Master16     Master50     Master59     Master63     Master65     Master78     Master125    Master159    Master164    Master165    Master171    Master176    Master182    Master183    Master193    Master261    Master278    Master305    Master340    Master348    Master386    Master407    Master445    Master480    Master489    Master549    Master709    Master751    Master755    Master787    Master788    Master802    Master803    Master819    Master824    Master827    Master831    Master850    Master869    MasterName: Initial, dtype: objectdf_train['Initial'] = df_train['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})df_test['Initial'] = df_test['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})df_train.Embarked.unique()array(['S', 'C', 'Q'], dtype=object)df_train['Embarked'].value_counts()S    646C    168Q     77Name: Embarked, dtype: int64df_train['Embarked']=df_train['Embarked'].map({'C':0,'Q':1,'S':2})df_test['Embarked']=df_test['Embarked'].map({'C':0,'Q':1,'S':2})df_train.head()                  PassengerId      Survived      Pclass      Name      Sex      SibSp      Parch      Ticket      Fare      Cabin      Embarked      FamilySize      Initial      Age_cat                  0      1      0      3      Braund, Mr. Owen Harris      male      1      0      A/5 21171      0.683603      NaN      2      2      2      2.0              1      2      1      1      Cumings, Mrs. John Bradley (Florence Briggs Th...      female      1      0      PC 17599      1.450832      C85      0      2      3      3.0              2      3      1      3      Heikkinen, Miss. Laina      female      0      0      STON/O2. 3101282      0.727559      NaN      2      1      1      2.0              3      4      1      1      Futrelle, Mrs. Jacques Heath (Lily May Peel)      female      1      0      113803      1.379314      C123      2      2      3      3.0              4      5      0      3      Allen, Mr. William Henry      male      0      0      373450      0.735091      NaN      2      1      2      3.0      df_train.Embarked.isnull().any()Falsedf_train['Sex'].unique()array(['male', 'female'], dtype=object)df_train['Sex']=df_train['Sex'].map({'female':0,'male':1})df_test['Sex']=df_test['Sex'].map({'female':0,'male':1})heatmap_data=df_train[['Survived','Pclass','Sex','Fare','Embarked','FamilySize','Initial','Age_cat']]heatmap_data.corr()                  Survived      Pclass      Sex      Fare      Embarked      FamilySize      Initial      Age_cat                  Survived      1.000000      -0.338481      -0.543351      0.332593      -0.167675      0.016639      -0.085529      -0.095002              Pclass      -0.338481      1.000000      0.131900      -0.659932      0.162098      0.065997      -0.133054      -0.314809              Sex      -0.543351      0.131900      1.000000      -0.271514      0.108262      -0.200988      0.051687      0.122917              Fare      0.332593      -0.659932      -0.271514      1.000000      -0.177469      0.410847      -0.016650      0.068385              Embarked      -0.167675      0.162098      0.108262      -0.177469      1.000000      0.066516      0.026550      -0.033173              FamilySize      0.016639      0.065997      -0.200988      0.410847      0.066516      1.000000      -0.204574      -0.280537              Initial      -0.085529      -0.133054      0.051687      -0.016650      0.026550      -0.204574      1.000000      0.481309              Age_cat      -0.095002      -0.314809      0.122917      0.068385      -0.033173      -0.280537      0.481309      1.000000      colormap=plt.cm.BuGnplt.figure(figsize=(12,10))plt.title('Pearson Correlation of Features',y=1.05,size=15)sns.heatmap(heatmap_data.astype(float).corr(),linewidths=0.1,vmax=2,square=True,cmap=colormap,linecolor='white',annot=True,annot_kws={'size':16},fmt='.2f')&lt;AxesSubplot:title={'center':'Pearson Correlation of Features'}&gt;One-hot encoding on the Initial and Embarkeddf_test.head()                  PassengerId      Pclass      Name      Sex      SibSp      Parch      Ticket      Fare      Cabin      Embarked      Initial      Age_cat                  0      892      3      Kelly, Mr. James      1      0      0      330911      2.057860      NaN      1      2      3.0              1      893      3      Wilkes, Mrs. James (Ellen Needs)      0      1      0      363272      1.945910      NaN      2      3      4.0              2      894      2      Myles, Mr. Thomas Francis      1      0      0      240276      2.270836      NaN      1      2      6.0              3      895      3      Wirz, Mr. Albert      1      0      0      315154      2.159003      NaN      2      2      2.0              4      896      3      Hirvonen, Mrs. Alexander (Helga E Lindqvist)      0      1      1      3101298      2.508582      NaN      2      3      2.0      df_train = pd.get_dummies(df_train, columns=['Initial'], prefix='Initial')df_test = pd.get_dummies(df_test, columns=['Initial'], prefix='Initial')df_train = pd.get_dummies(df_train, columns=['Embarked'], prefix='Embarked')df_test = pd.get_dummies(df_test, columns=['Embarked'], prefix='Embarked')df_train.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)df_test.drop(['PassengerId', 'Name',  'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)df_test.head()                  Pclass      Sex      Fare      Age_cat      Initial_0      Initial_1      Initial_2      Initial_3      Initial_4      Embarked_0      Embarked_1      Embarked_2                  0      3      1      2.057860      3.0      0      0      1      0      0      0      1      0              1      3      0      1.945910      4.0      0      0      0      1      0      0      0      1              2      2      1      2.270836      6.0      0      0      1      0      0      0      1      0              3      3      1      2.159003      2.0      0      0      1      0      0      0      0      1              4      3      0      2.508582      2.0      0      0      0      1      0      0      0      1      df_train.head()                  Survived      Pclass      Sex      Fare      FamilySize      Age_cat      Initial_0      Initial_1      Initial_2      Initial_3      Initial_4      Embarked_0      Embarked_1      Embarked_2                  0      0      3      1      0.683603      2      2.0      0      0      1      0      0      0      0      1              1      1      1      0      1.450832      2      3.0      0      0      0      1      0      1      0      0              2      1      3      0      0.727559      1      2.0      0      1      0      0      0      0      0      1              3      1      1      0      1.379314      2      3.0      0      0      0      1      0      0      0      1              4      0      3      1      0.735091      1      3.0      0      0      1      0      0      0      0      1      Machine learningl(Randomforest)from sklearn.ensemble import RandomForestClassifierfrom sklearn import metricsfrom sklearn.model_selection import train_test_splitdf_train.head()                  Survived      Pclass      Sex      Fare      FamilySize      Age_cat      Initial_0      Initial_1      Initial_2      Initial_3      Initial_4      Embarked_0      Embarked_1      Embarked_2                  0      0      3      1      0.683603      2      2.0      0      0      1      0      0      0      0      1              1      1      1      0      1.450832      2      3.0      0      0      0      1      0      1      0      0              2      1      3      0      0.727559      1      2.0      0      1      0      0      0      0      0      1              3      1      1      0      1.379314      2      3.0      0      0      0      1      0      0      0      1              4      0      3      1      0.735091      1      3.0      0      0      1      0      0      0      0      1      df_test.head()                  Pclass      Sex      Fare      Age_cat      Initial_0      Initial_1      Initial_2      Initial_3      Initial_4      Embarked_0      Embarked_1      Embarked_2                  0      3      1      2.057860      3.0      0      0      1      0      0      0      1      0              1      3      0      1.945910      4.0      0      0      0      1      0      0      0      1              2      2      1      2.270836      6.0      0      0      1      0      0      0      1      0              3      3      1      2.159003      2.0      0      0      1      0      0      0      0      1              4      3      0      2.508582      2.0      0      0      0      1      0      0      0      1      X_train=df_train.drop('Survived',axis=1).valuestarget_label=df_train['Survived'].valuesX_test=df_test.valuesX_tr, X_vld, y_tr, y_vld = train_test_split(X_train, target_label, test_size=0.3, random_state=2018)model = RandomForestClassifier()model.fit(X_tr, y_tr)prediction = model.predict(X_vld)print('총 {}명 중 {:.2f}% 정확도로 생존 맞춤'.format(y_vld.shape[0], 100 * metrics.accuracy_score(prediction, y_vld)))총 268명 중 82.09% 정확도로 생존 맞춤feature importance and prediction on test setmodel.feature_importances_array([0.09818595, 0.10792619, 0.32805606, 0.09146926, 0.1232598 ,       0.01214762, 0.04206886, 0.11624336, 0.02958443, 0.0041871 ,       0.01512252, 0.01363153, 0.01811731])from pandas import Seriesfeature_importance = model.feature_importances_Series_feat_imp = Series(feature_importance, index=df_test.columns)plt.figure(figsize=(8, 8))Series_feat_imp.sort_values(ascending=True).plot.barh()plt.xlabel('Feature importance')plt.ylabel('Feature')plt.show()submission = pd.read_csv('gender_submission.csv')submission.head()                   PassengerId      Survived                  0      892      0              1      893      1              2      894      0              3      895      0              4      896      1      prediction = model.predict(X_test)submission['Survived'] = prediction",
        "url": "/programming-kaggle2"
    }
    ,
    
    "programming-baekjoon6": {
        "title": "백준 (6) &lt;br&gt; (3085, 2563, 4673, 5635, 11170)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)백준 5문제를 풀어보았다.중복되는 문제도 있습니다3085번 사탕 게임https://www.acmicpc.net/problem/3085먼저 보드는 한 글자당 리스트 한 요소를 차지하게끔 이중으로 만든다.check 함수를 통해 보드에 연속된 사탕이 몇개인지 만들어준다.그 다음 for문을 통해 값들을 하나씩 변경해보고 가장 많은 값을 찾는다.num=int(input())board=[]answer=0for i in range(num):    candy=list(input())    board.append(candy)3CCPCCPPCCdef check(board):    n=len(board)    answer=1 #연속된 사탕의 결과        for i in range(n):        count=1        for j in range(1, n):            if board[i][j] == board[i][j-1]:  #열에서 같다면 +1 해주기                count += 1            else:                count=1    # 같지 않다면 1로 초기화            if count &gt; answer:                answer = count # 가장 큰 값을 answer로 반환        count=1 # 행 결과를 찾기 위한 초기화        for j in range(1, n):            if board[j][i] == board[j-1][i]: # 행에서 최대값 찾기                count += 1            else:                count=1            if count &gt; answer:                answer = count    return answeranswer=0for i in range(num):    for j in range(num):        if j+1 &lt; num:            board[i][j],board[i][j+1] =board[i][j+1],board[i][j] # 열에서 값 바꾸기            temp=check(board) # 최대 개수 확인하기            if temp &gt; answer:                answer = temp # 가장 많은 값으로 저장            board[i][j], board[i][j+1] = board[i][j+1], board[i][j] # 값 초기화하기        if i+1 &lt; num: #행에서 마찬가지로 진행            board[i][j], board[i+1][j] = board[i+1][j], board[i][j]            temp=check(board)            if temp &gt; answer:                answer = temp                        board[i][j], board[i+1][j] = board[i+1][j], board[i][j]            print(answer)32563번 색종이https://www.acmicpc.net/problem/2563100 * 100의 흰 색종이를 먼저 만들어준다.다음 입력받은 색종이만큼 0을 1로 바꿔주면 중복도 해결하면서 검은색을 표시할 수 있다.1이 된 숫자의 부분만 세면 된다.paper=[[0 for i in range(101)] for j in range(101)]for i in range(int(input())):    x,y=map(int,input().split())    for j in range(x,x+10):        for k in range(y,y+10):            paper[j][k]=1result=0for i in paper:    result += i.count(1)print(result)33 715 75 22604673번 색종이https://www.acmicpc.net/problem/4673for문을 통해 숫자들의 셀프 넘버를 구해서 초기 [1:10000]의 리스트에서 셀프 넘버가 나오면 제거해주었다.self_num=[i for i in range(1,10001)]for i in range(1,10001):    total=i    num=str(i)    for j in range(len(num)):        total=total+int(num[j])    if total in self_num:        self_num.remove(total)for i in range(len(self_num)):    print(self_num[i])135792031425364758697108110121132143154165176187198209211222233244255266277288299310312323334345356367378389400411413424435446457468479490501512514525536547558569580591602613615626637648659670681692703714716727738749760771782793804815817828839850861872883894905916918929940951962973984995100610211032104310541065107610871098110911111122113311441155116611771188119912101212122312341245125612671278128913001311131313241335134613571368137913901401141214141425143614471458146914801491150215131515152615371548155915701581159216031614161616271638164916601671168216931704171517171728173917501761177217831794180518161818182918401851186218731884189519061917191919301941195219631974198519962007202220332044205520662077208820992110211221232134214521562167217821892200221122132224223522462257226822792290230123122314232523362347235823692380239124022413241524262437244824592470248124922503251425162527253825492560257125822593260426152617262826392650266126722683269427052716271827292740275127622773278427952806281728192830284128522863287428852896290729182920293129422953296429752986299730083023303430453056306730783089310031113113312431353146315731683179319032013212321432253236324732583269328032913302331333153326333733483359337033813392340334143416342734383449346034713482349335043515351735283539355035613572358335943605361636183629364036513662367336843695370637173719373037413752376337743785379638073818382038313842385338643875388638973908391939213932394339543965397639873998400940244035404640574068407940904101411241144125413641474158416941804191420242134215422642374248425942704281429243034314431643274338434943604371438243934404441544174428443944504461447244834494450545164518452945404551456245734584459546064617461946304641465246634674468546964707471847204731474247534764477547864797480848194821483248434854486548764887489849094920492249334944495549664977498849995010502550365047505850695080509151025113511551265137514851595170518151925203521452165227523852495260527152825293530453155317532853395350536153725383539454055416541854295440545154625473548454955506551755195530554155525563557455855596560756185620563156425653566456755686569757085719572157325743575457655776578757985809582058225833584458555866587758885899591059215923593459455956596759785989600060116026603760486059607060816092610361146116612761386149616061716182619362046215621762286239625062616272628362946305631663186329634063516362637363846395640664176419643064416452646364746485649665076518652065316542655365646575658665976608661966216632664366546665667666876698670967206722673367446755676667776788679968106821682368346845685668676878688969006911692269246935694669576968697969907001701270277038704970607071708270937104711571177128713971507161717271837194720572167218722972407251726272737284729573067317731973307341735273637374738573967407741874207431744274537464747574867497750875197521753275437554756575767587759876097620762276337644765576667677768876997710772177237734774577567767777877897800781178227824783578467857786878797890790179127923792579367947795879697980799180028013802880398050806180728083809481058116811881298140815181628173818481958206821782198230824182528263827482858296830783188320833183428353836483758386839784088419842184328443845484658476848784988509852085228533854485558566857785888599861086218623863486458656866786788689870087118722872487358746875787688779879088018812882388258836884788588869888088918902891389248926893789488959897089818992900390149029904090519062907390849095910691179119913091419152916391749185919692079218922092319242925392649275928692979308931993219332934393549365937693879398940994209422943394449455946694779488949995109521952395349545955695679578958996009611962296249635964696579668967996909701971297239725973697479758976997809791980298139824982698379848985998709881989299039914992599279938994999609971998299935635번 생일https://www.acmicpc.net/problem/5635for문을 통해 글자들을 입력받고 연-월-일 부분을 정수형으로 치환한다.sort와 lambda를 이용해서 오름차순으로 정렬하면 가장 마지막 사람이 어리고 가장 처음 사람이 나이가 가장 많을 것이다.people=[]for i in range(int(input())):    person=list(input().split(' '))    person[1]=int(person[1])    person[2]=int(person[2])    person[3]=int(person[3])    people.append(person)people.sort(key=lambda x:[x[3],x[2],x[1]])print(people[-1][0])print(people[0][0])4Mickey 1 10 1991Jerry 18 9 1990Tom 15 8 1993Alice 30 12 1990[['Jerry', 18, 9, 1990], ['Alice', 30, 12, 1990], ['Mickey', 1, 10, 1991], ['Tom', 15, 8, 1993]]TomJerry11170번 0의 개수https://www.acmicpc.net/problem/11170a,b 두 숫자 사이의 모든 숫자들을 붙여서 하나의 글자로 만들어준다음 count함수를 사용하였다.for i in range(int(input())):    a,b=input().split()        word=''    for j in range(int(a),int(b)+1):        word=word+str(j)        print(word.count('0'))30 10233 10051991 40",
        "url": "/programming-baekjoon6"
    }
    ,
    
    "programming-baekjoon5": {
        "title": "백준 (5) 정렬 알고리즘 &lt;br&gt; (2750,11399,2751,1427, &lt;br&gt; 10989,1181,11650,2309)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)정렬에 관련된 8문제를 풀어보았다.중복되는 문제도 있습니다https://www.acmicpc.net/problemset?sort=ac_desc&amp;algo=158정렬은 병합 정렬, 분할 정복, 퀵 정렬, 힙 정렬, 계수 정렬 등 공부할 개념들이 많았다.2750번 수 정렬하기https://www.acmicpc.net/problem/2750sorted를 통해 입력받은 숫자들을 오름차순으로 정렬하는 문제였다.num=int(input())n_list=sorted([int(input()) for i in range(num)])for j in n_list:    print(j)5523411234511399번 ATMhttps://www.acmicpc.net/problem/11399total과 temp를 통해 temp에서는 n번째 사람이 걸리는 시간을 저장해주고 total에 전 사람까지 걸린 시간을 저장해주었다.people=int(input()) #사람수 입력time=sorted(list(map(int,input().split()))) #map으로 split된 값을 list로 저장 후 sorted로 오름차순 정렬53 1 4 3 2total=0 #n번째 사람까지 걸린 시간의 총합temp=0 #n번째 사람이 기다린 시간for i in time:    temp=temp+i #n번째 사람이 기다린 시간= 이전 사람이 기다린 시간+ 그 사람이 걸리는 시간    total=temp+total print(total)322751번 수 정렬하기 2https://www.acmicpc.net/problem/2751pypy3으로 제출하면 이렇게도 해결이 된다.num=int(input())n_list=sorted([int(input()) for i in range(num)])for j in n_list:    print(j)55432112345하지만 python3로는 시간초과가 발생한다.이 문제는 앞에 2750번과 다르게 N의 범위가 100만까지이다. 따라서 시간복잡도 문제라고 볼 수 있다.시간복잡도를 할 수 있는 방법은 병합 정렬, 퀵 정렬, 힙 정렬이 있다.병합 정렬병합 정렬은 데이터를 절반씩 나누어 끝까지 갔다가 다시 절반씩 합치면서 정렬하는 방법이다.이 때 분할 단계에서 깊이가 logN에 비례하지만, 깊이별로 수행되는 merge의 시간복잡도는 O(N)이다.  리스트 요소가 1개가 될때까지 나눈다.  분리한 왼쪽리스트, 오른쪽 리스트의 각각 첫번째 요소를 비교해 더 작은 값을 결과 리스트에 저장한다.  저장한 값은 리스트에서 지운다.  두 리스트 모두 요소가 하나도 안남을 때까지 반복한다.병합 정렬을 이용한 풀이병합 정렬을 이용할 때는 먼저 def로 병합 정렬 함수를 만들어준다.모든 리스트 요소가 1개가 될때까지 나눈다.따라서 중간값을 기준으로 나누어주면 된다.여기서도 input 대신 sys.stdin.readline()를 사용해야한다.import sysn=int(input())unsorted=[]result=[]# 분할def Divided(list):    #길이가 1일때 중단    if len(list)&lt;=1:        return list    #중간값을 기준으로 리스트 분할    mid = len(list)//2    less_part=list[:mid]    more_part=list[mid:]    less_part=Divided(less_part)    more_part=Divided(more_part)    return merge(less_part,more_part)#비교와 합병def merge(less,more):    merged_list=[]    l,h=0,0    #less와 more을 돌면서 대소관계 비교 후 작은 곳에 append    while l&lt;len(less) and h&lt;len(more):        if less[l]&lt;more[h]:            merged_list.append(less[l])            l=l+1        else:            merged_list.append(more[h])            h=h+1    merged_list+=less[l:]    merged_list+=more[h:]    return merged_listfor i in range(n):    num=int(input())    unsorted.append(num)result=Divided(unsorted)for i in result:    print(i)7643125712345671427번 소트인사이드https://www.acmicpc.net/problem/1427N을 문자열로 입력받아서 각 글자들을 list에 넣고 sort를 해줘도 오름차순으로 정렬이 된다.따라서 reverse를 통해 내림차순으로 만들어주고 join으로 다시 문자열로 만들어주었다.N=input()2143print(''.join(sorted([i for i in N],reverse=True)))432110989 수 정렬하기 3https://www.acmicpc.net/problem/10989메모리 초과가 발생하는 코드sys.stdin.readline()를 사용하여도, pypy3를 사용하여도 메모리 초과가 발생한다.아마 첫째 줄에 범위가 1&lt;=N&lt;=10000000 까지 넓고 수 또한 10000 이하의 자연수로 매우 크기 때문이 아닐까 생각된다.num=int(input())n_list=sorted([int(input()) for i in range(num)])for j in n_list:    print(j)1052314235171122334557계수 정렬(counting sort) 알고리즘따라서 검색을 해보니 계수 정렬 알고리즘이라는 방식이 있다고 한다.계수 정렬의 특징은  데이터의 크기 범위가 제한되어 정수 형태로 표현할 수 있을 때만 사용할 수 있다.매우 빠르다.모든 범위를 담을 수 있는 리스트를 선언해야한다.즉, 일단 범위가 되는 모든 자연수의 크기와 같은 리스트를 생성해야한다.숫자를 입력 받으면 해당하는 숫자가 나타내는 리스트 인덱스에 +1을 해주는 것이다.따라서 숫자가 입력 받았으면 1 이상이 나올 것이고 아니라면 0일 것이다.입력받은 숫자의 빈도를 물어본다면 인덱스와 값을 출력시키면 될 것이고입력받은 숫자를 정렬한다면 0이 아닌 리스트의 인덱스 값을 순서대로 출력해주면 된다.계수 정렬을 이용한 풀이따라서 nlist를 통해 자연수 범위만큼 0인 리스트를 선언해주었다.그 다음 입력받은 n값을 nlist의 인덱스에서 찾아서 +1 해주었다.그 다음 nlist의 값들을 찾으면서 0보다 큰 값들만 찾아주고 그 값들의 인덱스만 추출해주면 된다.이 문제도 input을 사용하면 시간 초과가 발생한다.#계수 정렬을 이용한 풀이import sysn=int(input())nlist=[0 for i in range(10001)]for i in range(n):    nlist[int(input())]+=1for i in range(len(nlist)):    if nlist[i]&gt;0:        for j in range(nlist[i]):            print(i)105231423517112233455711650번 좌표 정렬하기https://www.acmicpc.net/problem/11650 sort() 함수에서 key 파라미터에 (x,y)를 넣어주면 x를 우선 정렬해주고 y를 정렬해준다.주피터 노트북을 사용하면서 sys.stdin.readline()을 사용한 적이 없었는데 주피터 노트북에선 stdin이 잘 구현이 안된다고 한다.따라서 밑에 코드에는 input()을 사용하였지만 백준에 제출할 때는 sys.stdin.readline()으로 제출하였다.input()을 하게되면 시간 초과 결과가 나오고 pypy3로 제출하면 input()을 사용해도 괜찮은 것으로 보인다.import sysN=input()nlist=[]for i in range(int(N)):    x,y=map(int,input().split()) #실제 제출에서는 sys.stdin.readline()을 사용하였다    nlist.append((x,y))53 41 11 -12 23 3nlist.sort(key=lambda x: (x[0],x[1]))for j in nlist:    print(j[0],j[1])1 -11 12 23 33 42309번 일곱 난쟁이https://www.acmicpc.net/problem/2309이 문제는 input()을 사용하여도 시간초과는 나오지 않는다.n_list에서 두 가지씩 고르는 모든 경우의 수를 찾아야하기 때문에 이중 for문을 사용하였다.j는 i와 같이 않아야하기 때문에 i+1로 중복을 피했다.모든 값의 합이 100 이 되기 때문에 두 개씩 골라서 빼주었을 때 100 이 되면 그 두 값을 remove를 통해 제거하고 sorted로 내림차순 정렬한 뒤 출력해주면 된다.첫 for문이 끝나야하기 때문에 조건에 맞는 답을 골라주면 멈추면 된다.n_list=[]for i in range(1,10):    tall=int(input())    n_list.append(tall)2072319101525813result= sum(n_list)for i in range(9):    for j in range(i+1,9):        if result-(n_list[i]+n_list[j])==100:            a,b=n_list[i],n_list[j]            n_list.remove(a)            n_list.remove(b)            n_list=sorted(n_list)            for k in range(7):                print(n_list[k])            break    if len(n_list)==7:        break781013192023",
        "url": "/programming-baekjoon5"
    }
    ,
    
    "study-ml2": {
        "title": "머신러닝 정리 (2) &lt;br&gt; 지도학습 (2)",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학머신러닝 정리 (2) - 지도학습 (2)본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python지도학습 (2)  결정 트리  결정 트리의 앙상블  배깅, 엑스트라 트리, 에이다부스트          배깅      엑스트라 트리      선형 모델      에이다부스트        커널 서포트 벡터 머신  신경망 (딥러닝)  분류 예측의 불확실성 추정          결정 함수      예측 확률      다중 분류에서의 불확실성      결정 트리Decision Tree 결정 트리는 분류와 회귀 문제에서 사용되는 모델입니다.스무고개처럼 예/아니오로 나눌 수 있는 조건을 통해서 결정에 다다르게 됩니다.질문과 정답은 노드가 되고 특히 마지막 노드는 리프라고 합니다. 결정트리의 구조는 왼쪽 하단에 사진처럼 가장 위에는 Root node, 질문과 답을 연결하는 Edge, 내부의 Internal node, 마지막 노드는 Leaf node, 그리고 Depth로 구성됩니다.결정 트리 만들기 결정 트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예/아니오 질문 (TEST) 목록을 학습한다는 뜻입니다.보통 데이터들은 예/아니오 특성으로 구분되지 않고  연속적인 특성을 가진 2차원 데이터 셋에서 보통 ‘특성 i는 값 a보다 큰가?’의 형태와 같은 테스트를 가집니다.이 데이터들을 X[1]&lt;=0.6인 테스트로 나누어 봅니다.알고리즘은 가능한 모든 테스트에서 타깃 값에 대해 가장 많은 정보를 가진 것을 고르게 됩니다.따라서 X[1]&lt;=0.6인 테스트를 선택하게 됩니다.결정 트리에서 각 테스트는 하나의 축을 따라 데이터를 나눕니다.하나의 질문당 하나의 축을 만들어서 영역이 한 개의 타깃값을 가질 때까지 반복됩니다. 결정 트리의 멈춤 조건입니다.즉, 미리 정의한 조건들이 없다면 가지를 만들 수 있을 때까지 만드는 것을 알 수 있습니다.결정트리의 예측은 그 포인트가 어느 리프에 들어갈지 확인하는 것인데 분류는 타깃 값 중 다수인 것이 예측 결과가 되고 회귀의 경우 리프 노드의 훈련 데이터 평균값이 결과로 출력됩니다.결정 트리 복잡도 제어하기 결정 경계가 클래스 포인트에 멀리 떨어진 이상치에 민감하게 되어 모든 리프 노드가 순수 노드가 될 때까지 진행하면 모델이 복잡해지고 과대적합이 발생합니다.과대적합을 막기 위한 방법은 크게 사전가지치기, 사후 가지치기 두 가지입니다.사전 가지치기는 이름에서 알 수 있듯이 모델을 만들 때 깊이나 리프의 개수 또는 테스트의 최소 개수를 미리 제한하는 것입니다.미리 제한하기 때문에 정말로 중요한 포인트를 분류하지않을 수 있습니다.사후 가지치기 역시 이름에서 알 수 있듯이 트리가 만들어진 뒤 포인트가 적은 노드를 삭제 혹은 병합하게 되는데 에러감소 프루닝, 룰 포스트 프루닝 같은 방법들이 있습니다.참고에러감소 프루닝  모든 노드를 프루닝 대상으로 고려노드 제거 후 검증을 통해 제거 전, 후 정확도 비교제거 전보다 정확도가 낮아지기 전까지 반복룰 포스트 프루닝  의사결정 트리를 룰셋으로 변환 (룰은 루트부터 리프까지의 경로)이 룰셋 속성들에 정확도를 떨어뜨리는 속성을 제거프루닝 완료 후 정확도 순으로 정렬해 이 순서대로 적용결정 트리는 다음과 같이 만들 수 있고 정확도를 확인할 수 있습니다.Default값은 모든 리프가 순수 노드가 되는 모델을 만들기 때문에 훈련 세트의 정확도가 100%가 됩니다.하지만 트리가 무한정 깊어지고 복잡해지고 일반화가 잘 되지 않습니다.from sklearn.tree import DecisionTreeClassifiercancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split(    cancer.data, cancer.target, stratify=cancer.target, random_state=42)tree = DecisionTreeClassifier(random_state=0)tree.fit(X_train, y_train)print(\"훈련 세트 정확도: {:.3f}\".format(tree.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(tree.score(X_test, y_test)))훈련 세트 정확도: 1.000테스트 세트 정확도: 0.937과대적합 때문에 반드시 훈련 세트의 정확도가 테스트 정확도와 비례하지 않아서 max_depth와 같은 파라미터를 통해 과대적합을 줄이고 테스트 세트 정확도를 높일 수 있습니다.tree = DecisionTreeClassifier(max_depth=4, random_state=0)tree.fit(X_train, y_train)print(\"훈련 세트 정확도: {:.3f}\".format(tree.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(tree.score(X_test, y_test)))훈련 세트 정확도: 0.988테스트 세트 정확도: 0.951결정 트리 분석 결정 트리를 생성하고 시각화하기 위해서는 다음과 같은 모듈이 필요합니다.# 트리 모델 생성from sklearn.tree import DecisionTreeClassifier # 트리의 시각화_1from sklearn.tree import export graphviz # 트리의 시각화_2 (.dot 파일을 만들지 않아도 가능)from sklearn.tree import plot_tree 그래프를 시각화하는 코드는 다음과 같이 쓸 수 있습니다.# graphviz 이용from sklearn.tree import export_graphvizexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"악성\", \"양성\"],                feature_names=cancer.feature_names, impurity=False, filled=True)#plot_treefrom sklearn.tree import plot_treeplot_tree(tree, class_names=[\"악성\", \"양성\"], feature_names=cancer.feature_names,         impurity=False, filled=True, rounded=True, fontsize=4)filled=True를 넣어주면 다음과 같이 색상이 들어가는 트리 모델을 얻을 수 있습니다.트리의 특성 중요도tree.feature_importane를 통해 특성 중요도를 알 수 있습니다.특성 중요도는 0부터 1 사이에 존재하는데 0은 전혀 사용되지 않은 특성, 1은 완벽하게 타깃 클래스를 예측한 특성을 의미합니다. 특성 중요도가 낮다는 유용하지 않다가 아닌 모델이 만들어질 때 특성을 선택하지 않았거나 특성과 중복되는 정보가 있다는 것을 의미합니다.전체 합은 1이 되고 따라서 특성중요도는 ‘이 모델이 만들어지는데 어떤 특성의 비율이 높은가?’ 정도의 해석이라고 생각하면 될 것 같습니다.Worst_radius만 보고 ‘반지름이 크면 양성이다?’ 를 알 수 없는 것처럼 특성 중요도는 어떤 클래스를 지지하는지 알려주지 않습니다.결정 트리의 회귀도 분류와 비슷하게 적용됩니다.단, 결정 트리를 회귀 모델로 사용하게 되면 훈련 데이터 범위 밖의 정보가 없어서 그 부분에 대한 예측이 불가능하게 됩니다.다음 모델은 트리 복잡도에 제한을 두지않아서 훈련 데이터는 완벽하게 예측하지만 데이터 범위 밖으로 나가면 마지막 포인트로 예측값을 출력합니다.따라서 트리 모델은 가격의 등락과 같은 예측을 할 때는 좋은 예측 모델을 만들 수 있지만 시계열 데이터에서는 데이터가 가진 시간 범위 밖의 예측은 안되기 때문에 잘 맞지 않습니다.장단점과 매개변수장점  해석력이 높습니다.데이터의 스케일에 구애받지 않습니다. 정규화나 표준화 같은 전처리 불필요합니다.특성의 스케일이 다르거나 이진특성, 연속적인 특성이 혼합되어도 잘 작동합니다.단점  과대적합되는 경향이 있어 일반화 성능이 좋지 않습니다.  축 평행을 구분하여 일부 관계에서 모델링이 어려움이 있습니다. 훈련 데이터에 대한 약간의 변경은 전체 결정논리에 큰 변화를 야기하여 샘플에 민감합니다.            매개변수      설명                  min_samples_split      - 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용  - Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가              min_samples_leaf      - 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수- min_samples_split과 함께 과적합 제어 용도- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요              max_features      - 최적의 분할을 위해 고려할 최대 feature 개수- Default = None → 데이터 세트의 모든 피처를 사용- int형으로 지정 →피처 갯수 / float형으로 지정 →비중- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정              max_depth      - 트리의 최대 깊이- default = None→ 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요              max_leaf_nodes      리프노드의 최대 개수      여기서 max_depth, max_leaf_nodes,min_samples_leaf 중 하나만 지정해도 과대적합을 막는데 충분한 역할을 합니다.결정 트리의 앙상블Ensemble앙상블은 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법입니다.책에서는 결정 트리의 앙상블로 한정하고 가장 많이 쓰이는 랜덤포레스트나 부스팅 모델은 트리 기반 모델이지만 앙상블은 다른 분류 모델을 결합하여 사용할 수도 있습니다.  Voting – 서로 다른 알고리즘을 가진 분류기를 결합  Bagging – 각각의 분류기는 모두 같은 유형의 알고리즘 기반, 모델을 다양하게 만들기 위해 데이터를 재구성 (랜덤포레스트)  Boosting – 맞추기 어려운 데이터에 대해 좀 더 가중치를 두어 학습 (Adaboost, Gradient Boosting)  Stacking – 모델의 output 값을 새로운 독립변수로 사용앙상블의 조건입니다.랜덤 포레스트랜덤 포레스트는 조금씩 다른 결정 트리의 묶음입니다.      데이터의 일부에 과대적합되는 경향을 이용하여 서로 다른 방향으로 과대적합된 트리를 많이 만들어 그 결과를 평균냄으로써 예측 성능은 유지되면서 결과적으론 과대적합이 줄어드는 아이디어에 기초합니다.결정 트리를 많이 만들면서 각 트리는 타깃 예측을 잘 해야 하고 다른 트리와 구별되어야 합니다.따라서 무작위성을 주입하는데 트리를 만들 때 사용하는 데이터 포인트를 무작위로 선택하거나 분할 테스트에서 특성을 무작위로 선택하는 방법을 이용합니다.랜덤 포레스트 구축from sklearn.ensemble import RandomForestClassifier (or RandomForestRegressor)n_estimators로 생성할 트리의 개수를 정합니다.부트스트랩 샘플은 n_samples개의 데이터 포인트 중에서 n_samples 횟수만큼 무작위로 중복 가능하게 반복 추출하는 것을 의미합니다.따라서 데이터 셋이 원래 크기와 같지만 누락되거나 중복되는 데이터가 만들어집니다.각 노드에서 전체 특성을 대상으로 최선의 테스트를 찾는 것이 아닌 알고리즘이 각 노드에서 후보 특성을 무작위로 선택한 후 이 후보들 중에서 최선의 테스트를 찾습니다. (max_features) 부트스트랩 샘플링을 통해 트리가 조금씩 다른 데이터셋을 이용해 만들어지도록 합니다.각 노드에서 특성의 일부만 사용하기 때문에 트리의 각 분기는 각기 다른 특성 부분 집합을 사용됩니다.max_features=n_features는 특성 선택에 무작위성이 들어가지 않습니다. (부트스트랩 샘플링에는 무작위성 그대로 입니다.)max_feature=1 트리의 분기는 테스트할 특성을 고를 필요가 없게 되고 무작위로 선택한 특성의 임계값 찾기만 하면 됩니다.max_feature이 커지면 랜덤 포레스트 트리들은 매우 비슷하고 가장 두드러진 특성으로 데이터에 잘 맞춰질 것이고 작으면 트리들은 서로 많이 달라지고 각 트리는 데이터에 맞추기 위해 깊이가 깊어지게 됩니다.랜덤 포레스트 예측의 경우 알고리즘이 모델에 있는 모든 트리의 예측을 만듭니다.회귀의 경우 이 예측들을 평균하여 최종 예측을 만듭니다.분류의 경우 약한 투표 전략을 사용합니다.약한 투표 전략은 각 알고리즘이 가능성 있는 출력 레이블의 확률을 제공하고 예측한 확률을 평균으로 가장 높은 확률을 가진 클래스가 예측값이 됩니다.참고로 강한 투표 전략은 다수의 분류기가 결정한 예측값을 최대로 하는 것을 말합니다.랜덤 포레스트 분석랜덤 포레스트 훈련 모델from sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import make_moonsX, y = make_moons(n_samples=100, noise=0.25, random_state=3)X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)forest = RandomForestClassifier(n_estimators=5, random_state=2)forest.fit(X_train, y_train)부트스트랩 샘플링 때문에 한쪽 트리에 나타나는 훈련 포인트가 다른 트리에는 포함되지 않을 수 있어 각 트리는 불완전하지만 랜덤포레스트의 결과는 좋은 결정경계를 보여줍니다.단일 트리와 다르게 랜덤 포레스트에서 가장 특성 중요도가 높은 특성은 worst perimeter입니다.랜덤 포레스트에서 더 많은 특성이 0 이상의 중요도를 갖고 따라서 더 넓은 시각으로 데이터를 바라볼 수 있습니다.X_train, X_test, y_train, y_test = train_test_split(    cancer.data, cancer.target, random_state=0)forest = RandomForestClassifier(n_estimators=100, random_state=0)forest.fit(X_train, y_train)print(\"훈련 세트 정확도: {:.3f}\".format(forest.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(forest.score(X_test, y_test)))훈련 세트 정확도: 1.000테스트 세트 정확도: 0.972랜덤 포레스트에선 훈련 데이터 정확도가 100% 이지만 단일 트리에 비해서 테스트 정확도가 상승한 것을 확인 할 수 있습니다.장단점과 매개변수장점  매개변수 튜닝을 많이 하지 않습니다. 데이터의 스케일에 구애받지 않습니다. 단일 트리의 단점을 보완하고 장점을 그대로 가지고 있습니다.단점  랜덤 포레스트의 트리는 특성의 일부만 사용하므로 결정 트리보다 더 깊어지는 경향이 있습니다.다른 random_state를 지정하면 전혀 다른 모델이 만들어집니다.텍스트 데이터와 같은 차원이 높고 희소한 데이터에 잘 작동하지 않습니다.선형 모델에 비해 많은 메모리를 사용하며 훈련과 예측이 느림            매개변수      설명                  n_estimators      - 결정트리의 갯수를 지정- Default = 10 (0.22버전부터 100)- 무작정 트리 갯수를 늘리면 성능 좋아지는 것 대비 시간이 걸릴 수 있음              min_samples_split      - 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용- Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가              min_samples_leaf      - 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수- min_samples_split과 함께 과적합 제어 용도- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요              max_features      - 최적의 분할을 위해 고려할 최대 feature 개수- Default = ‘auto’ (결정트리에서는 default가 none이었음)- int형으로 지정 →피처 갯수 / float형으로 지정 →비중- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정 (RandomForestClassifier-sqrt(n_feature), RandomForestRegressor-n_feature)- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정              max_depth      - 트리의 최대 깊이- default = None→ 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요              max_leaf_nodes      리프노드의 최대 개수      N_estimatiors는 클수록 좋고 max_features와 max_depth와 같은 사전 가지치기 옵션은 단일 트리와 같이 주어집니다.그레이디언트 부스팅 회귀 트리이름은 회귀이지만 회귀와 분류 모두 사용됩니다. (GradientBoostingClassifier, GradientBoostingRegressor)그레이디언트 부스팅은 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듭니다.따라서 기본적으로 무작위성이 없습니다.대신 강력한 사전 가지치기가 사용되고 깊지 않은 트리를 사용합니다.각 트리는 데이터의 일부에 대해서만 예측을 잘 수행하여 트리가 많이 추가될수록 성능이 향상됩니다.이때 손실 함수를 정의하고 경사 하강법을 사용해서 다음 값을 보정합니다.random_state=0 만 입력했을 때from sklearn.ensemble import GradientBoostingClassifier​X_train, X_test, y_train, y_test = train_test_split(    cancer.data, cancer.target, random_state=0)​gbrt = GradientBoostingClassifier(random_state=0)gbrt.fit(X_train, y_train)​print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_test, y_test)))훈련 세트 정확도: 1.000테스트 세트 정확도: 0.965random_state=0, max_depth=1 을 입력했을 때gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)gbrt.fit(X_train, y_train)​print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_test, y_test)))훈련 세트 정확도: 0.991테스트 세트 정확도: 0.972random_state=0, learning_rate=0.01을 입력했을 때gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)gbrt.fit(X_train, y_train)print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train, y_train)))print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_test, y_test)))훈련 세트 정확도: 0.988테스트 세트 정확도: 0.965훈련 세트의 정확도가 100%로 과대적합이 된 모델은 max_depth나 learning_rate로 보완할 수 있습니다.Random_state는 고정시켜야 같은 모델이 나오는 것을 볼 수 있습니다.Learning_rate는 오차에 곱을 해서 예측값을 업데이트 해주는 값입니다.랜덤 포레스트에 비해 그레이디언트 부스팅은 특성들이 더 적습니다.안정성에서는 랜덤 포레스트가 더 좋지만 그레이디언트가 성능적으로 더 좋은 모습을 보여줄수 있습니다.참고XGBoost  XGBoost는 데이터 별 오류를 다음 round 학습에 반영 시킨다는 측면에서 기존 Gradient Boosting과 큰 차이는 없음Gradient Boosting과 달리 학습을 위한 목적식(loss function)에 Regularization term이 축가되어 모델이 과적합 되는 것을 방지해줌Regularization term을 통해 XGBoost는 복잡한 모델에 패널티를 부여함LighGBM  XGBoost와 다르게 lear-wise loss 사용 (loss를 더 줄일 수 있음)XGBoost 대비 2배 이상 빠른 속도 (동일 파라미터 기준)과대적합에 민감하여, 대량의 학습데이터를 필요로 함장단점과 매개변수장점  이진 특성이나 연속적인 특성에도 잘 작동합니다. 데이터의 스케일에 구애받지 않습니다.단점  매개변수의 조정이 필수입니다.휸련시간이 깁니다.  차원이 높고 희소한 데이터에 잘 작동하지 않습니다.N_estimators가 클수록 랜덤 포레스트는 좋았지만 그래이디언트 부스팅에서는 과대적합될 가능성이 높아집니다.N_estimator을 정하고 난 뒤에 learning_rate를 정하게 되는데 learning_rate를 낮추면 비슷한 복잡도의 모델을 만들기 위해 더 많은 트리를 추가해야합니다.            매개변수      설명                  n_estimators      - 트리의 개수를 지정- 커지면 모델이 복잡해지고 과대적합 가능성 높아짐              learning_rate      - 관례상 n_estimators를 맞추고 learning_rate를 찾음 - 이전 트리의 오차를 보정하는 정도              n_iter_no_change / validation_fraction      -조기 종료를 위한 매개변수 (default값: n_iter_no_change =None (조기 종료 x), validation_fraction=0.1) - validation_fraction 비율만큼 검증 데이터로 사용하여 n_iter_no_change 만큼 반복하여 향상되지 않으면 훈련 종료              max_depth / max_leaf_nodes      -각 트리의 복잡도를 낮춤 - max_depth는 보통 매우 작게 설정하며 트리의 깊이가 5보다 깊어지지 않게 함      배깅, 엑스트라 트리, 에이다부스트Baggingfrom sklearn.ensemble import BaggingClassifier배깅은 중복을 허용한 랜덤샘플링으로 만든 훈련 세트를 사용해 분류기를 각기 다르게 학습합니다.랜덤포레스트는 배깅의 일종이지만 설명변수도 무작위로 선택하는 것이 차이가 있습니다.predict_proba() 지원하면 메서드를 통해 확률값을 평균하여 예측을 수행합니다. (지원하지 않는다면 가장 빈도가 높은 클래스 레이블)oob_score=True로 지정하면 매개변수는 부트스트래핑에 포함되지 않은 샘플로 훈련된 모델을 평가할 수 있습니다. (OOB 오차, default=False)from sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import BaggingClassifierbagging = BaggingClassifier(LogisticRegression(), n_estimators=100, oob_score=True, n_jobs=-1, random_state=42)bagging.fit(Xc_train, yc_train)print(\"훈련 세트 정확도: {:.3f}\".format(bagging.score(Xc_train, yc_train)))print(\"테스트 세트 정확도: {:.3f}\".format(bagging.score(Xc_test, yc_test)))print(\"OOB 샘플의 정확도: {:.3f}\".format(bagging.oob_score_))훈련 세트 정확도: 0.953테스트 세트 정확도: 0.951OOB 샘플의 정확도: 0.946배깅은 랜덤포레스트와 달리 max_samples에 부트스트랩 샘플의 크기를 정할 수 있습니다. 또한 로지스틱 회귀가 들어갈 수도 있고 결정 트리가 들어갈 수도 있습니다.Extra Tree후보 특성을 무작위로 분할한 다음 최적의 분할을 찾습니다.엑스트라 트리도 랜덤 포레스트와 비슷하지만 splitter=‘random’을 사용합니다. 랜덤 포레스트는 splitter=‘best’가 고정입니다.Splitter=‘best’의 의미는 모든 변수의 정보 이득을 계산하고 그중 가장 설명력이 높은 변수를 선택하는 것입니다.또한 부트스트랩 샘플링을 적용하지 않습니다. 무작위성을 증가시키면 모델 편향은 늘어나지만 분산이 감소하는 모습을 보입니다.개별 트리는 매우 복잡하지만 결정 경계는 안정적입니다.계산 비용은 위 splitter에서의 feature의 차이 때문에 랜덤 포레스트보다 적지만  일반화 성능을 높이려면 많은 트리를 만들어야합니다.from sklearn.ensemble import ExtraTreesClassifierxtree = ExtraTreesClassifier(n_estimators=100, n_jobs=-1, random_state=0)xtree.fit(Xc_train, yc_train)print(\"훈련 세트 정확도: {:.3f}\".format(xtree.score(Xc_train, yc_train)))print(\"테스트 세트 정확도: {:.3f}\".format(xtree.score(Xc_test, yc_test))훈련 세트 정확도: 1.000테스트 세트 정확도: 0.972Adaptive Boosting이전의 모델이 잘못 분류한 샘플에 가중치를 높여서 다음 모델을 훈련합니다.훈련된 각 모델은 성능에 따라 가중치 부여합니다.예측을 만들 때는 모델이 예측한 레이블을 기준으로 모델의 가중치를 합산하여 가장 높은 값을 가진 레이블을 선택합니다.AdaBoostClassifier은 기본값으로 DecisionTreeClassifier(max_depth=1)를 갖습니다.AdaBoostRegressor은 기본값으로 DecisionTreeRegressor(max_depth=3)을 갖습니다. (base_estimator을 이용하여 다른 모델 지정 가능)에이다 부스팅의 원리와 수식예측정확도와 가중치의 곱의 합이 되어 높은 정확도를 만들게 됩니다.from sklearn.ensemble import AdaBoostClassifierada = AdaBoostClassifier(n_estimators=100, random_state=42)ada.fit(Xc_train, yc_train)print(\"훈련 세트 정확도: {:.3f}\".format(ada.score(Xc_train, yc_train)))print(\"테스트 세트 정확도: {:.3f}\".format(ada.score(Xc_test, yc_test)))훈련 세트 정확도: 1.000테스트 세트 정확도: 0.986커널 서포트 벡터 머신커널 서포트 벡터 머신은 보통 SVM이라고 한다.입력 데이터에서 단순한 초평면으로 정의되지 않는 더 복잡한 모델을 만들 수 있도록 확장한 것이다.분류와 회귀 모두 사용 가능하다. (SVC는 분류, SVR은 회귀)from sklearn.svm import LinearSVC #선형 모델LinearSVC(max_iter=)from sklearn.svm import SVCSVC(kernel='',C=,gamma=) # kernel, C, gamma 파라미터 존재선형 모델과 비선형 특성선형 모델은 직선으로만 데이터 포인트를 나눌 수 있어 밑에 같은 데이터는 잘 들어맞지 않는다.SVM 모델은 3차원에서 2차원으로 투영해본다면 더이상 선형 모델이 아니다.커널 기법커널 기법은 실제로 데이터를 확장하지 않고 확장된 특성에 대한 데이터 포인트들의 거리를 계산한다.$ (특성1)^2 * (특성2)^5 $하는 다항식 커널이 있고 가우시안 커널로 불리우는 RBF 커널이 있다.가우시안 커널은 차원이 무한한 특성 공간에 매핑하는 것이다.모든 차수의 모든 다항식을 고려하지만 특성의 중요도는 고차항이 될수록 줄어든다.SVM 이해하기두 클래스 사이에 경계한 데이터 포인트들을 서포트 벡터라고 한다.새로운 데이터 포인트에 대해 예측하려면 각 서포트 벡터와의 거리를 측정한다.서포트 벡터의 중요도는 훈련 과정에서 학습하는데 dual_coef_ 속성에 저장된다.가우시안 커널 공식 사진가우시안 커널에 의해 계산되며 $ X_1, X_2 $는 데이터 포인트이며 $ ||X_1 - X_2|| $는 유클리디안 거리이고 $Γ$ 은 가우시안 커널의 폭을 제어하는 매개변수이다.SVM 매개변수 튜닝$Γ$는 가우시안 커널 폭의 역수에 해당하는데 하나의 훈련 샘플이 미치는 영향의 범위를 결정한다.(1~0 사이의 범위이다.)작은 값은 넓은 영역을 뜻하고, 큰 값은 영향이 미치는 범위가 제한적이다.즉 커널의 반경이 클수록 훈련 샘플의 영향 범위도 커진다.작은 $Γ$ 값은 모델의 복잡도를 낮출 수 있다.C 매개 변수는 규제 매개변수이다. dual_coef_값을 제한합니다.작은 C는 매우 제약이 큰 모델을 만들고 각 데이터 포인트의 영향력이 작다.C를 증가시키면 이 포인트들이 영향을 크게 줘서 결정 경계를 휘게 만든다.SVM을 위한 데이터 전처리커널 SVM에서는 데이터셋의 특성 자릿수가 완전히 다르면 영향을 크게 미친다.따라서 특성 값을 평균이 0이고 단위 분산이 되도록 하거나, 0과 1 사이로 맞추는 방법을 많이 사용한다.(StandardScaler와 MinMaxScalar)장단점과 매개변수SVM은 저차원과 고차원의 데이터에 모두 잘 작동하지만 샘플이 많으면 잘 맞지 않는다.또한 전처리와 매개변수 설정에 신경을 많이 써야하는데 그래서 랜덤 포레스트나 그레이디언트 부스팅과 같은 전처리가 거의 필요 없는 트리 기반 모델이 선호된다.SVM은 분석도 어려워서 예측이 어떻게 결정되었는지 설명하기가 난해하다.하지만 모든 특성이 비슷한 단위이고 스케일이 비슷하다면 시도해볼 만하다.중요한 매개변수는 C이고 어떤 커널을 사용할지와 각 커널에 따른 매개변수이다.RBF는 $Γ$ 매개변수를 갖지만 다른 커널 종류도 많다.신경망 (딥러닝)다층 퍼셉트론은(MLP)는 간단하게 분류와 회귀에서 쓰일 수 있다.신경망 모델MLP는 여러 단계를 거처 결정을 만들어내는 선형 모델의 일반화된 모습이다.선형 회귀 모델의 예측 공식 사진$\\hat Y $는 x[0]에서 x[p]까지의 입력특성과 학습된 계수의 가중치의 합이다.퍼셉트론 사진왼쪽 노드는 입력 특성을 나타내며 연결선은 학습된 계수를 표현하고 오른쪽 노드는 입력의 가중치 합, 즉 출력을 나타낸다.MLP는 가중치 합을 만드는 과정이 여러 번 반복되며 먼저 중간 단계를 구성하는 은닉 유닛을 계산하고 이를 이용하여 최종 결과를 산출하기 위해 다시 가중치 합을 계산한다.다중 퍼셉트론 사진각 은닉 유닛의 가중치 합을 계산한 후 결과에 비선형 함수인 렐루나 하이퍼볼릭 탄젠트, 시그모이드 함수를 적용합니다.회귀 분석 사진w는 입력 x와 은닉층 h 사이의 가중치이고, v는 은닉층 h와 출력 $\\hat Y$ 사이의 가중치입니다.w와 v는 훈련 데이터에서 학습하고 x는 입력 특성이며 $ \\hat Y $는 계산된 출력, h는 중간 계산값 입니다.신경망 튜닝더 복잡도가 낮은 모델을 만들고 싶다면 hidden_layer_size를 통해 은닉 유닛의 개수를 줄인다.은닉 유닛을 추가하거나, 은닉층을 추가하거나 활성화함수를 바꾸면 더 매끄러운 결정 경계를 얻을 수도 있다.선형 분류와 리지 회귀 처럼 L2 페널티를 사용해서 가중치를 0에 가깝게 감소시킬 수도 있다.(default는 매우 낮다)신경망에서는 학습을 시작하기 전에 가중치를 무작위로 설정하며 이 무작위한 초기화가 모델의 학습에 영향을 준다.따라서 같은 매개변수를 사용하더라도 초깃값이 다르면 모델이 많이 달라질 수 있다.신경망도 입력 특성이 평균은 0 분산이 1이 되도록 변형하는 것이 좋다.은닉 유닛에서 작은 가중치를 가진 특성은 모델에 덜 중요하다고 추론할 수 있다.from sklearn.neural_network import MLPClassifier # MLP분류MLPClassifier(solver='',activation='',random_state=,hidden_layer_sizes=[,],max_itter=,alpha=) #solver에 최적화 알고리즘,activation에 활성화 함수 ,hidden_layer_size로 은닉 유닛의 개수 설정(default=100),max_itter은 반복 횟수,alpha는 L2 페널티장단점과 매개변수머신러닝 알고리즘을 뛰어넘는 성능을 보일 수 있지만 학습이 오래걸리고 데이터 전처리를 주의해서 해야한다.모든 특성이 같은 의미를 가지면 SVM, 다른 종류의 특성이라면 트리 기반 메딜이 더 잘 작동할 수 있다.신경망의 복잡도 추정가장 중요한 매개변수는 은닉층의 개수와 각 은닉층의 유닛 수이다.복잡도에 관해 연관된 측정치는 학습된 가중치 또는 계수의 수이다.특성이 100개 은닉 유닛 100개인 이진 분류라면 입력층과 첫 번째 은닉층 사이에는 편향을 포함하여 $ 100 * 100 + 100 = 10100 $개의 가중치가 있습니다.은닉층과 출력층 사이에 $ 100 * 1 + 1 = 101 $개의 가중치가 더 있어 가중치는 10201개 이다.이렇게 가중치는 은닉층을 추가할수록 훨씬 커지게 된다.매개변수를 조정하는 일반적인 방법은 충분히 과대적합되어 문제를 해결할만한 큰 모델을 만든 뒤 훈련 데이터가 충분히 학습될 수 있다고 생각되면 신경망 구조를 줄이거나 규제 강화를 위해 alpha 값을 증가시켜 일반화 성능을 향상시킨다.층의 개수, 층당 유닛 개수, 규제, 비선형성으로 모델 구성을 할 수 있으며, solver 매개변수를 통해서 학습시키는 방법을 지정할 수 있다.solver의 경우 기본값은 adam이고 데이터 스케일에 민감하다.lbfgs는 안정적이지만 규모가 크면 시간이 오래 걸린다sgd는 momentum과 nesterovs_momentom의 영향을 받는데 다른 여러 매개변수와 함께 튜닝하여 최선의 결과를 만들 수 있다.분류 예측의 불확실성 추정decision_function과 predict_proba로 추정 할 수 있다.결정 함수decision_function의 반환값의 크기는 (n_samples,)이며각 샘플이 하나의 실수 값을 반환한다.모델이 데이터 포인트가 양성 클래스인 클래스 1에 속한다고 믿는 정도이다.즉, 음수값은 다른 클래스에 속함을 의미한다.값의 범위는 데이터와 모델 파라미터에 따라 달라지게 된다.예측 확률predict_proba의 출력은 각 클래스에 대한 확률이고 이진 분류에서 이 값의 크기는 항상 (n_samples,2)이다.두 클래스의 확률 합은 1이므로 두 클래스 중 하나는 50% 이상의 확신을 가질 것이고 그 클래스가 예측값이 된다.데이터에 있는 불확실성이 얼마나 이 값에 잘 반영되는지는 모델과 매개변수 설정에 달렸다.그래서 과대적합된 모델 혹은 잘못된 예측도 예측의 확신이 강한 편이다.복잡도가 낮을 수록 예측에 불확실성이 더 많다.불확실성과 모델의 정확도가 동등하면 이 모델이 보정되었다고 한다.다중 분류에서의 불확실성다중 분류에서도 decision_funcion과 predict_proba를 사용할 수 있다.decision_function에서는 (n_samples, n_classes)가 결과값이 된다.글 클래스에 대한 확신 점수를 담고 그 수치가 크면 그 클래스일 가능성이 크다.데이터 포인트마다 점수들에서 가장 큰 값을 찾아 예측 결과를 재현할 수 있다.predict_proba는 (n_samples,n_classes)가 출력값이 된다.마찬가지로 각 데이터 포인트에서 클래스 확률의 합은 1이다.argmax 함수를 적용해서 예측 결과를 재현할 수 있지만 클래스가 문자열이거나 정수형을 사용하지만 연속적이지 않고 0부터 시작하지 않을 수 있다.따라서 predict 결과와 decision_function, predict_proba의 결과를 비교하기 위해서는 분류기의 classes_ 속성을 사용해 클래스의 실제 이름을 얻어야 한다.",
        "url": "/study-ML2"
    }
    ,
    
    "programming-baekjoon4": {
        "title": "백준 (4) &lt;br&gt; (1157, 1546, 2577, 2675, 2908, &lt;br&gt; 1018, 1436, 1259, 7568, 10250)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)백준 10문제를 풀어보았다.중복되는 문제도 있습니다1157번 단어공부https://www.acmicpc.net/problem/1157upper을 이용한 대문자 받기count로 dictionary형태로 word 개수 세기개수가 중복되는 단어들이 있으므로 max_list에서 따로 추출하기조건에 맞게 최댓값이 하나면 알파벳을, 아니면 물음표를 출력하기word=input().upper()count={}for i in word:    if i not in count:        count[i]=0    count[i]+=1max_list=[j for j,k in count.items() if max(count.values())==k]if len(max_list)==1:    print(max_list[0])else:    print('?')Mississipi?1546번 평균https://www.acmicpc.net/problem/1546map 함수를 통해서 점수 입력받기new_mean()이라는 함수는 문제에서 나온 $점수/최대점수*100$ 이다.map함수를 통해 new_mean함수를 score에 모두 적용해주고 sum을 통해 총합을 구한 뒤,count로 나눠주면 된다.count=int(input())score=list(map(int,input().split())) # 점수 입력받기340 80 60max_score=max(score) # 점수 최댓값 찾기def new_mean(x): #최댓값과의 비율로 새로운 점수 만드는 함수    return x/max_score*100print(sum(map(new_mean,score))/count) #평균을 구하는 식75.02577번 숫자의 개수https://www.acmicpc.net/problem/2577입력받는 숫자가 3개로 한정되어있으니까 for문을 통해서 세 숫자의 곱을 구했다.count함수의 인덱스 0-9까지를 0-9숫자가 나왔을 때 하나씩 늘려주는 방법을 택했다.mul=1for i in range(3): # 세 숫자의 곱 구하기    a=int(input())    mul=mul*aprint(mul)15026642717037300count=[0]*10 #0-9의 개수가 들어갈 listfor j in str(mul): #str(mul)로 해줘야 for문이 성립된다.    for k in range(10): #0-9까지의 숫자를 확인하는 for문        if int(j)==k:            count[k]=count[k]+1 #숫자가 등장했을때 1 늘려주기for l in count:    print(l)31020002002675번 문자열 반복https://www.acmicpc.net/problem/2675각 글자마다 R번 반복해서 출력해주는 문제이다.case를 통해서 몇 번 반복할지를 결정해준다.word_list에 각 단어마다 R번씩 반복하여 append해준다.join으로 리스트에 있는 단어들을 문장으로 만들어준다.case=int(input()) # 시도할 횟수for i in range(case):     R,P=input().split() # R,P입력받기    word_list=[] #반복한 뒤 append해줄 list    for j in P: #P에 있는 글자 순서대로 반복해주기        for k in range(int(R)): #R을 str로 입력받아서 int로 변경해줘야함            word_list.append(j) #반복된 글자를 append    print(''.join(word_list)) #list안에 글자들 붙여서 출력해주기23 ABCAAABBBCCC5 /HTP/////HHHHHTTTTTPPPPP2908번 상수https://www.acmicpc.net/problem/2908list(A)로 숫자들을 리스트화 해준다.[::-1]로 거꾸로 뒤집어준다. A로 다시 저장하기 싫으면 reverse() 함수를 사용하면 된다.join을 통해 숫자로 바꿔주고 int 형태로 바꿔준 뒤, max를 통해 최댓값을 찾는다.A,B=input().split() # A,B 숫자 거꾸로 만들기A=list(A)[::-1] B=list(B)[::-1]# A,B 다시 숫자로 만든 뒤 대소비교A=int(''.join(A))B=int(''.join(B))print(max(A,B))734 8934371018번 체스판 다시 칠하기https://www.acmicpc.net/problem/1018N,M 크기를  받고 보드 만들기nXm형태의 위치를 파악하기 쉽게 리스트 형태로 받았다.n, m=map(int,input().split())if 8&lt;=n&lt;=50 and 8&lt;=m&lt;=50:    board = [input() for i in range(n)]10 13BBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBWWWWWWWWWWBWBWWWWWWWWWWBWB위치가 짝수일 때와 홀수일 때로 나눠서  W, B가 아닐 때마다 점수를 추가해준 다음 가장 최소가 되는 값만 찾아내면 된다.따라서 n * m의 보드에서 가능한 경우의 수는 n-7 * m-7이다. ex)10 13을 입력받을 경우 18가지.8 * 8로 잘라주기 위해서 k와 l을 (i,i+8), (j,j+8)로 한정짓는다.k+l이 홀수일 경우와 짝수일 경우, W로 시작할 경우와 B로 시작할 경우를 나눠서 모든 경우의 수를 반복문으로 확인해준다.마지막으로 total_score에 들어있는 값들 중 최솟값을 구해준다.total_score=[]# 보드에서 경우의 수 나누어주기for i in range(n-7):    for j in range(m-7):        count_w=0 #w가 아닐때        count_b=0 #b가 아닐때        #8*8 크기로 잘라주기        for k in range(i,i+8):             for l in range (j,j+8):                #각 경우의 수마다 비교해서 점수 추가하기                if (k+l)%2==0:                    if board[k][l]!='W':                                                    count_w=count_w+1                    if board[k][l]!='B':                        count_b=count_b+1                else:                    if board[k][l]!='W':                        count_b=count_b+1                    if board[k][l]!='B':                                                    count_w=count_w+1        # 점수들 한 list에 모아주기        total_score.append(count_w)        total_score.append(count_b)print(min(total_score)) #최솟값 출력121436번 영화감독 숌https://www.acmicpc.net/problem/1436666이 적어도 3개이상 연속으로 들어가는 수를 만든다처음에 문제를 풀 때 중간에 666이 3개 이상 들어가는 경우를 제외해서 틀렸다.list666에 가장 작은 숫자인 666부터 ‘666’이 문자열로 들어가있는 숫자들을 확인해서 추가하였다.입력받은 숫자가 list666의 길이보다 크면 계속 추가해주었고 list666[num-1]을 통하여 값을 출력해준다.num=int(input())list666=[]i=666while len(list666)&lt;num:    if '666' in str(i):        list666.append(i)    i=i+1print(list666[num-1]) 326661259번 팰린드롬수https://www.acmicpc.net/problem/1259앞에서 읽어도 뒤에서 읽어도 같은 숫자 찾기0을 입력하면 반복문이 끝나게 while과 if를 이용하였다.입력받은 숫자는 위치를 찾기 편하게 문자형으로 입력받았다.입력받은 숫자의 길이/2 만큼의 반복문을 돌리면 반대쪽은 (숫자의 길이-i-1)로 대응된다.한가지 숫자라도 값이 다르면 False 값을 가지고 ‘no’를 출력하면 ‘yes’를 출력하는 것을 만들 때보다 길이가 짧아질 수 있다.while True:    word=input() # 숫자를 무한으로 입력받기 위해 while문 사용    quest=True # 한가지 입력값을 처리하고나서 True, False값을 True로 초기화    if word=='0': # 0을 입력하면 반복문 종료        break    else:        word_len=len(word)        for i in range(int((word_len)/2)): #단어 길이의 반만 확인하면 반대쪽 숫자와 대응된다.                if word[i]!=word[word_len-1-i]: #반대쪽 숫자와 대응하기 위해서 word_len-1-i 사용                    quest=False # 하나의 경우라도 False가 나오면 반복문 종료                    continue        if quest==False: # False가 나오면 바로 'no' 출력            print('no')        else: print('yes')121yes1231no12421yes07568번 덩치https://www.acmicpc.net/problem/7568처음에 문제를 풀 때 너무 복잡하게 생각해서 무게 따로, 키 따로 점수 매기고 sort해서 index로 출력하려고 했는데 예제 문제는 옳게 나오지만 다른 경우에서 틀렸었다.또 and 말고 &amp; 로 써서 한 번 더 틀렸는데 이건 bitwise 연산자라서 답이 다르게 나왔다.# r값 입력 받고 (무게, 키) 형태로 리스트 만들기count=int(input())human=[]for i in range(count):    weight,tall=input().split()    human.append((int(weight),int(tall)))print(human)555 18558 18388 18660 17546 155[(55, 185), (58, 183), (88, 186), (60, 175), (46, 155)]for j in human:    rank=1 # 등수는 1등 부터니까 1    for k in human:        if j[0]&lt;k[0] and j[1]&lt;k[1]: # 둘 다 k가 우세할 경우에만 rank에 1 추가-&gt;순위 하락            rank+=1    print(rank,end=' ')2 2 1 2 5 for m in human:    rank=1    for n in human:        if m[0]&lt;n[0] &amp; m[1]&lt;n[1]: # &amp;는 비트 연산자기 때문에 결과가 다르게 나온다.            rank+=1    print(rank,end=' ')3 1 1 1 1 10250번 ACM호텔https://www.acmicpc.net/problem/102501호가 우선시 된다면 H명씩 순서대로 채운다고 생각하면 편하다.따라서 층수는 N/H의 나머지가 되고 호수는 N/H의 몫+1이 된다.하지만 N이 H의 배수일 때, 층수가 최고층이 되기때문에 H를 대신 입력해준다.또한 호수도 N/H의 몫과 같아지기 때문에 +1 한 것을 다시 -1 해준다.또한 호수가 10보다 작을 때 앞에 0을 적어줘야한다.for i in range(int(input())):    H,W,N=map(int,input().split())    floor=N%H #나머지가 층수가 된다.    order=(N//H)+1 #몫을 정수로 받은 뒤 +1해주면 호수가 된다.    if floor==0: #N이 H의 배수일 때만 따로 구분해서 값을 준다.        floor=H        order=order-1    if order&lt;10: # 호수 앞에 0을 붙여준다.        print(str(floor)+'0'+str(order))    else:        print(str(floor)+str(order))23 2 33014 1 8402 ```",
        "url": "/programming-baekjoon4"
    }
    ,
    
    "programming-baekjoon3": {
        "title": "백준 (3) 문자열 알고리즘 &lt;br&gt;(11720, 8958, 1152, 10809, &lt;br&gt; 1157, 9012, 11718)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)문자열에 관련된 7문항을 풀어보았다.https://www.acmicpc.net/problemset?sort=ac_desc&amp;algo=15811720번 숫자의 합https://www.acmicpc.net/problem/11720공백없는 숫자들의 합구하기방법 1. for문을 이용해서 풀기n=int(input())m=input()554321range를 이용한 풀이 방법result=0for i in range(n):    result=int(m[i])+resultprint(result)15m을 읽어가면서 더해주는 방법result=0for i in m:    result=int(i)+resultprint(result)15방법 1. sum과 map을 이용해서 풀기print(sum(map(int,input())))54321158958 번 OX퀴즈https://www.acmicpc.net/problem/8958for문을 이용하여 입력받을 ox의 개수를 입력받고 for문 중첩을 이용하여 ox의 길이를 파악하고 if문으로 ox 여부를 확인하였다.ox의 연속성에 따른 점수변화를 num_score로 두고 total_score을 num_score의 합으로 설정하였다.num=int(input())for i in range(num):    ox=input()    total_score=0    num_score=0    for i in range(len(ox)):        if (ox[i]=='O') is True:            num_score=num_score+1        else:            num_score=0        total_score=total_score+num_score    print(total_score)5OOXXOXXOOO10OOXXOOXXOO9OXOXOXOXOXOXOX7OOOOOOOOOO55OOOOXOOOOXOOOOX301152번 단어의 개수https://www.acmicpc.net/problem/1152단어의 개수=공백의 위치마다 구분해줘서 입력받았다.sentence=list(map(str,input().split()))print(len(sentence))The Curious Case of Benjamin Button610809번 알파벳찾기https://www.acmicpc.net/problem/10809for문과 알파벳 list 선언s=list(map(str,input()))alpha=list('abcdefghijklmnopqrstuvwxyz')array=[-1 for i in range(len(alpha))]for i in range(len(s)):    if array[alpha.index(s[i])]==-1:        array[alpha.index(s[i])]=ifor j in array:    print(j,end=' ')baekjoon1 0 -1 -1 2 -1 -1 -1 -1 4 3 -1 -1 7 5 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 아스키코드 이용알파벳의 아스키코드는 (97,123)이다.s=input()alpha=list(range(97,123))for i in alpha:    print(s.find(chr(i)),end=' ')baekjoon1 0 -1 -1 2 -1 -1 -1 -1 4 3 -1 -1 7 5 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 1157번 단어 공부https://www.acmicpc.net/problem/1157upper을 이용한 대문자 받기count로 dictionary형태로 word 개수 세기개수가 중복되는 단어들이 있으므로 max_list에서 따로 추출하기조건에 맞게 최댓값이 하나면 알파벳을, 아니면 물음표를 출력하기word=input().upper()count={}for i in word:    if i not in count:        count[i]=0    count[i]+=1max_list=[j for j,k in count.items() if max(count.values())==k]if len(max_list)==1:    print(max_list[0])else:    print('?')Mississipi?9012번 괄호https://www.acmicpc.net/problem/9012 while문을 통해 ‘()’가 vps에 존재한다면 계속 replace를 통해 제거해주었다.만약 VPS 문장이었다면 문자열에 아무것도 남지않고 VPS 문장이 아니라면 어떤 문자라도 남았을 것이다.따라서 결과물의 길이로 YES와 NO를 구분지어줬다.num=int(input())for i in range(num):    vps=input()    while '()' in vps:        vps=vps.replace('()','')    if len(vps)==0:        print('YES')    else:        print('NO')6(())())NO(((()())()NO(()())((()))YES((()()(()))(((())))()NO()()()()(()()())()YES(()((())()(NO11718번 그대로 출력하기https://www.acmicpc.net/problem/11718while True로 반복문을 만들어주고 try~except문으로 오류가 발생했을때는 멈출수 있게 해준다.while True:    try:        print(input())    except:        breakOnline JudgeOnline Judge",
        "url": "/programming-baekjoon3"
    }
    ,
    
    "study-ml1": {
        "title": "머신러닝 정리 (1) &lt;br&gt; 지도학습 (1)",
            "author": "keonju",
            "category": "",
            "content": "머신러닝 공부 관련 글    머신러닝 정리 (1)-지도학습 (1)    머신러닝 정리 (2)-지도학습 (2)    머신러닝 정리 (3)-비지도학습 (1)    머신러닝 정리 (4)-비지도학습 (2)    머신러닝 정리 (5)-데이터 표현과 특성 공학머신러닝 정리 (1) - 지도학습 (1)본 문서는 [파이썬 라이브러리를 활용한 머신러닝] 책을 공부하면서 요약한 내용입니다.또 데이터 청년 캠퍼스 수업과 학교 수업에서 배운 내용들도 함께 정리했습니다.글의 순서는 [파이썬 라이브러리를 활용한 머신러닝]에 따라 진행됩니다.코드는 밑에 링크에 공개되어있기 때문에 올리지않습니다.소스 코드: https://github.com/rickiepark/introduction_to_ml_with_python지도학습 (1)  분류와 회귀  일반화, 과대적합, 과소적합          모델복잡도와 데이터셋 크기의 관계        지도 학습 알고리즘          예제에 사용할 데이터셋      k-nn 모델      선형 모델      Naive Bayes 분류기      분류와 회귀분류란? 미리 정의된 가능성 있는 여러 클래스 레이블 중 하나를 예측하는 것이다.이진 분류: 예, 아니오로 구분할 수 있다. ex) 이 이메일은 스팸인가요?다중 분류: 셋 이상의 클래스로 분류된다. ex) 붓꽃 데이터회귀란? 부동소수점수(수학으로 말하면 실수)를 예측하는 것이다.어떤 사람의 여러 조건들을 통해 연간 소득을 예측하는 것과 같은 문제이다.출력 값에 연속성이 있다면 회귀 문제 없다면 분류 문제이다.일반화, 과대적합, 과소적합모델이 처음 보는 데이터에 대해 정확히 예측할 수 있다면 훈련 세트(train_set)에서 테스트 세트(test_set)으로 일반화되었다고 한다.훈련 세트에 대해서 정확히 예측하고 테스트 세트에서도 정확히 예측하길 바란다.하지만 모델이 복잡하다면 훈련 세트에서만 정확한 모델이 될 수 있다.예를 들어 “내 주변 20대가 모두 아이폰을 쓰기 때문에 다른 20대도 모두 아이폰을 살 것이다.”라는 예측을 한다면 훈련 세트가 내 주변 20대가 될 것이고 테스트 세트가 다른 모든 20대가 될 것이다.이처럼 알고리즘이 새로운 데이터를 잘 처리하는지 측정하는 방법은 테스트 세트로 평가를 해야한다.이 때, 너무 복잡한 모델을 만들어 훈련 세트에 집중되어 테스트 세트에 일반화가 부족하다면 과대적합(overfitting)이라 한다.반대로 너무 간단한 모델이라 훈련 세트에도 잘 맞지 않다면 과소적합(underfitting)이라 한다.모델복잡도와 데이터셋 크기의 관계모델의 복잡도는 훈련 데이터 셋에 담긴 입력 데이터의 다양성과 관련이 있다.데이터셋에 데이터 포인트가 다양하면 과대적합 없이 복잡한 모델을 만들 수 있다.따라서 중복이거나 비슷한 데이터를 모으는 것은 도움이 되지않는다.위의 예를 생각해보면 내 주변 20대라는 특징말고 대학 동기, 친구라는 데이터를 얻더라도 모두 20대라는 범주안에 들어갈테니 불필요한 데이터라 할 수 있다.따라서 좋은 데이터를 많이 얻는 것이 좋다.지도 학습 알고리즘각 모델의 장단점과 어떤 데이터와 어울리는지, 매개변수와 옵션의 의미를 알아보도록 하자.scikit-learn 문서를 참고하면 더 자세한 정보를 얻을 수 있다.예제에 사용할 데이터셋forge 데이터셋은 인위적으로 만든 이진 분류 데이터셋이다.k-NN 모델k-NN 모델은 단순히 데이터셋을 분류하는 것이다.k개의 레이블 중에서 어느 쪽에 더 가까운 것인지 투표를 하여 결정한다고 생각하면 이해하기 편하다.k=1일 때는 가장 가까운 것이 빨간색이었다면, k=3일 때는 파란색 두 개와 빨간색 한 개가 가까울 수도 있다.그렇게 된다면 k=3일 때는 파란색으로 분류가 된다.KNeighborClassifier(n_neighbors)을 통해 분류 모델을 만들 수 있다.이를 통해 확인할 수 있는 것은 k의 값이 커질수록 보다 단순한 모델이 만들어질 수 있다는 것이다.하지만 반드시 k가 커진다고 좋은 모델은 아니다. 정확도가 낮아질 수 있기 때문이다.KNeighborsRegressor(n_neighbors)을 통해 회귀 모델 또한 만들 수 있다.회귀 모델에서도 k를 너무 적게 쓴다면 모든 데이터를 지나가고 불안정한 모델이 만들어진다.장단점과 매개변수가장 중요한 매개변수는 거리를 재는 방법과 k값이다.보통 거리를 재는 방법은 유클리디안 거리 방식을 사용한다.장점은 이해하기가 쉬운 모델이고 조정을 많이 하지않아도 좋은 성능을 발휘할 수 있다는 것이다.단점은 훈련 세트가 크면 예측이 느려지고 전처리 과정이 중요하다는 것이다.데이터가 특성이 많거나 대부분이 0인 데이터셋에서는 잘 작동하지 않는다.선형 모델선형 모델은 $y=w[0]*x[0]+b$와 같은 모델을 갖는 예측 함수이다.y는 예측값, w와 b는 모델이 학습할 파라미터, x는 데이터의 특성이다. 위 식은 특성이 하나인 데이터 셋의 선형 함수이다.선형 회귀(최소제곱법)선형 회귀는 예측과 훈련 세트에 있는 타깃 사이의 평균제곱오차(MSE)를 최소화하는 파라미터를 찾는 것이다.평균제곱오차는 예측값과 타깃값의 차이를 제곱하여 더한 후에 샘플의 개수로 나눈 것이다.매개변수가 없는 것이 장점이지만 복잡도를 제어할 방법도 없다.LinearRegression()리지 회귀리지 회귀도 예측 함수를 사용하지만 가중치의 절댓값을 가능한 작게 만드는 목적을 갖는다.이런 제약을 규제라고 하며 L2 규제라고 한다.리지는 덜 자유로운 모델이라 과대적합이 적다. 따라서 일반화에 도움이 된다.Ridge(alpha)라소리지의 대안으로 라소가 있다. L1 규제라고도 하며 완전히 제외하는 특성이 생긴다.일부 계수가 0이 되고 모델을 이해하기 쉬워지며 중요한 특성을 찾기 쉽ㄴ다.max_itter을 조절하여 과소적합을 줄인다.Lasso(alpha, max_itter())분류용 선형 모델이진 분류의 경우 선형 회귀와 비슷하지만 가중치 합을 그냥 사용하는 대신 예측값을 임계치 0과 비교한다.0보다 작으면 -1, 크면 1이라고 예측한다. 결정 경계를 선형 함수로 잡는다.LogisticRegression과 LinearSVC가 잘 알려져있는데 규제의 강도를 결정하는 매개변수 C를 주의해야한다.C가 높으면 훈련 세트에 최대로 맞추려 노력하고 낮추면 계수 벡터(w)가 0에 가까워지도록 만든다.로지스틱 회귀분석을 제외하면 다중 클래스를 대부분 지원하지 않는다.장단점과 매개변수회귀 모델에서는 alpha, LinearSVC와 LogisticRegression에서는 C가 중요하다.alpha가 클수록, C가 작을수록 모델이 단순해진다. 로그 스케일로 최적치를 정한다.L1, L2를 결정하는 것도 정해야한다. 중요한 특성이 적으면 L1, 그렇지 않으면 L2를 이용한다.선형 모델은 학습 속도와 예측 속도가 빠르다. solver=’sag’ 옵션을 이용하면 더 빨리 처리할 수 있다.아니면 SGDClassifier과 SGDRegressor을 이용할 수도 있다.또한 선형 모델은 예측이 어떻게 만들어지는지 비교적 이해하기 쉽다. 하지만 계수의 값이 명확하지가 않다.Naive Bayes 분류기나이브 베이즈 분류기는 선형 분류기보다 훈련 속도가 빠르지만 일반화 성능이 뒤쳐진다.개별적으로 파라미터를 학습하고 특성에서 클래스별 통계를 단순하게 취합한다.GaussianNB, BernoulliNB, MultinomialNB를 scikit-learn에서 구현되어있다.GaussianNB는 연속적인, BernoulliNB는 이진 데이터를, MultinomialNB는 카운트 데이터를 적용한다.BernoulliNB는 클래스의 특성중 0이 아닌 것이 몇 개인지 센다.MultinomialNB는 클래스별 특성의 평균을, GaussianNB는 클래스별로 각 특성의 표준편차와 평균을 저장한다.MultinomialNB와 GaussianNB는 선형 모델과 예측 공식이 갖지만 coef_는 기울기 w가 아니라 의미는 다르다.장단점과 매개변수MultinomialNB와 BernoulliNB는 모델 복잡도를 조절하는 alpha변수가 하나이다.alpha가 주어지면 알고리즘이 모든 특성에 양의 값을 가진 데이터 포인트를 alpha 개수만큼 추가한다.alpha가 크면 더 완만하고 덜 복잡한 모델이 나오지만 성능 변동은 비교적 크지 않다.GaussianNB는 고차원 데이터 셋을 사용한다. 다른 모델은 데이터를 카운트하는 데 사용된다.",
        "url": "/study-ML1"
    }
    ,
    
    "programming-kaggle1": {
        "title": "캐글 (1) &lt;br&gt; Simple Matplotlib &amp; Visualization Tips 공부하기",
            "author": "keonju",
            "category": "",
            "content": "kaggle 관련 글    캐글 (1) Simple Matplotlib &amp; Visualization Tips 공부하기    캐글 (2) 타이타닉 튜토리얼 1,2 공부하기    캐글 (3) 타이타닉 생존자 EDA 부터 분류까지Simple Matplotlib &amp; Visualization Tips 공부하기https://www.kaggle.com/subinium/simple-matplotlib-visualization-tips/notebook해당과정을 필사하였으며 영어로 되어있는 부분은 다시 정리해서 적었다.  Table of Contents          Settingdpifigsizetitle        Alignments          subplots, tight_layoutsubplot2gridadd_axesadd_gridspec        Colormap          divergingqualitativesequentialscientific        Text &amp; Annotate &amp; Patch          parametertext examplepatches example        Details &amp; Example          font weight, color, size, etcHorizontal and Vertical (barplot)Border(edge) color and thicknessMain Color &amp; Sub ColorTransparencySpan        MEME          xkcd style      import numpy as npimport matplotlib as mplimport matplotlib.pyplot as plt#matplotlib.pyplot 모듈의 각각의 함수를 사용해서 간편하게 그래프를 만들고 변화를 줄 수 있습니다.import matplotlib.gridspec as gridspec# gridspec 에는 Figure 내에서 격자 모양의 패턴으로여러 Axes 레이아웃하는 데 도움이되는 클래스가 포함되어 있습니다.import seaborn as sns# Seaborn은 Matplotlib에 기반하여 제작된 파이썬 데이터 시각화 모듈print(f\"Matplotlib Version :{mpl.__version__}\")print(f\"Seaborn Version :{sns.__version__}\")import pandas as pdnetflix_titles=pd.read_csv(\"netflix_titles.csv\")Matplotlib Version :3.3.2Seaborn Version :0.11.0Setting해상도 설정 matplotlib의 기본 해상도는 떨어지는 편이라고 한다.또한 그래프의 모양에 따라서 느낌이 달라지기 때문에 크기를 많이 변경해봐야한다.plt.title() 그래프의 제목 그리기ax.set_title() 개별 서브 플롯에 제목을 추가하는 데 사용 fig.suptitle() 모든 서브 플롯에 공통 인 메인 타이틀을 추가plt.rcParams['figure.dpi'] = 200 # or dpi=200Alignmentsmatplotlib 레이아웃과 설계의 조합이다.두 개의 그래프가 한 개의 그래프보다 시각적으로 의미적으로 모두 좋다.두 개를 비교하기 위해 가장 쉬운 방법은 직사각형으로 배치하는 것이다.subplot을 통하여 초기 크기로 시작할 수 있다.subplots() 하나의 그림에 여러 플롯을 그리기 subplot2grid() 일반 그리드 내부의 특정 위치에 서브플롯을 생성 add_axes() 축 추가하기 gridspec() Figure 내에 서브플롯을 배치 add_subplot() Figure 내에 서브플롯을 배치inset_axes() 하위에 축 추가하기make_axes_locatable() 축 배치하기fig, axes = plt.subplots(2, 3, figsize=(8, 5))plt.show()# tight_layout을 통해 그래프 사이에 여유공간이 생김fig,axes =plt.subplots(2, 3, figsize=(8, 5))plt.tight_layout()plt.show()# subplot이 항상 같을 필요가 없음. 이 때 subplot2grid를 사용fig=plt.figure(figsize=(8,5)) #시작 크기 지정ax=[None for _ in range(6)]ax[0]=plt.subplot2grid((3,4),(0,0),colspan=4)ax[1]=plt.subplot2grid((3,4),(1,0),colspan=1)ax[2]=plt.subplot2grid((3,4),(1,1),colspan=1)ax[3]=plt.subplot2grid((3,4),(1,2),colspan=1)ax[4]=plt.subplot2grid((3,4),(1,3),colspan=1,rowspan=2)ax[5]=plt.subplot2grid((3,4),(2,0),colspan=3)for ix in range(6):     ax[ix].set_title('ax[{}]'.format(ix)) # subplot에 제목 생성    #ticks는 축에 표시되는 숫자    ax[ix].set_xticks([])    ax[ix].set_yticks([])    fig.tight_layout()plt.show()fig = plt.figure(figsize=(8, 5))ax = [None for _ in range(4)]#add_axes를 이해하기 힘들어서 값들을 눈에 띄게 변경해보았다.[왼쪽, 아래쪽, 너비, 높이]ax[0]=fig.add_axes([0.1,0.2,0.3,0.4])ax[1]=fig.add_axes([0.3,0.4,0.5,0.6])ax[2]=fig.add_axes([0.5,0.6,0.7,0.8]) ax[3]=fig.add_axes([0.6,0.7,0.8,0.9])for ix in range(4):    ax[ix].set_title('ax[{}]'.format(ix))    ax[ix].set_xticks([])    ax[ix].set_yticks([])plt.show()# gridspec을 이용하여 만들기fig=plt.figure(figsize=(8, 5))gs=fig.add_gridspec(3, 3) #(3x3 크기)ax=[None for _ in range(5)]ax[0]=fig.add_subplot(gs[0, :]) ax[0].set_title('gs[0, :]')ax[1]=fig.add_subplot(gs[1, :-1])ax[1].set_title('gs[1, :-1]')ax[2]=fig.add_subplot(gs[1:, -1])ax[2].set_title('gs[1:, -1]')ax[3]=fig.add_subplot(gs[-1, 0])ax[3].set_title('gs[-1, 0]')ax[4]=fig.add_subplot(gs[-1, -2])ax[4].set_title('gs[-1, -2]')for ix in range(5):    ax[ix].set_xticks([])    ax[ix].set_yticks([])plt.tight_layout()plt.show()fig,ax=plt.subplots()axin1=ax.inset_axes([0.8, 0.1, 0.15, 0.15])plt.show()from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatablefig,ax=plt.subplots(1, 1)ax_divider=make_axes_locatable(ax)ax=ax_divider.append_axes(\"right\", size=\"7%\", pad=\"2%\")plt.show()Colormaphttps://medium.com/nightingale/how-to-choose-the-colors-for-your-data-visualizations-50b2557fa335그래프에서 색상은 중요한 요소이다diverging 중앙을 기준으로 색상 변경qualitative 범주별로 색상 변경sequential 순차적으로 색상 변경scientific 다양한 색상으로 순차적 변경def cmap_plot(cmap_list, ctype):    cmaps=cmap_list    n=len(cmaps)    fig=plt.figure(figsize=(8.25, n*.20), dpi=200)    ax=plt.subplot(1, 1, 1, frameon=False, xlim=[0,10], xticks=[], yticks=[])    fig.subplots_adjust(top=0.99, bottom=0.01, left=0.18, right=0.99)    y,dy,pad = 0, 0.3, 0.08    ticks,labels = [], []    for cmap in cmaps[::-1]:        Z=np.linspace(0,1,512).reshape(1,512)        plt.imshow(Z, extent=[0,10,y,y+dy], cmap=plt.get_cmap(cmap))        ticks.append(y+dy/2)        labels.append(cmap)        y = y + dy + pad    ax.set_ylim(-pad,y)    ax.set_yticks(ticks)    ax.set_yticklabels(labels)    ax.tick_params(axis='y', which='both', length=0, labelsize=5)    plt.title(f'{ctype} Colormap', fontweight='bold', fontsize=8)    plt.show()#diverging#양쪽 끝으로 갈수록 색이 어두워진다.diverge_cmap=('PRGn', 'PiYG', 'RdYlGn', 'BrBG', 'RdGy', 'PuOr', 'RdBu', 'RdYlBu',  'Spectral', 'coolwarm_r', 'bwr_r', 'seismic_r')cmap_plot(diverge_cmap, 'Diverging')#Qualitative Colormap#최대 10가지 색상을 구성하고 점점 더 작은 범주, 다른 범주와 그룹화#유사한 색상은 피하고 채도나 밝기보다 색을 분명하게 바꾸는 것이 좋다.qualitative_cmap=('tab10', 'tab20', 'tab20b', 'tab20c',         'Pastel1', 'Pastel2', 'Paired',         'Set1', 'Set2', 'Set3', 'Accent', 'Dark2' )cmap_plot(qualitative_cmap, 'Qualitative')#Sequential Colormap#밀도 표현에 효과적이고 지도 그래프에도 효과적이다.#밝기의 변화에 따라서 값을 비교할 수 있다.sequential_cmap=('Greys', 'Reds', 'Oranges',          'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',         'Purples', 'YlGnBu', 'Blues', 'PuBu', 'GnBu', 'PuBuGn', 'BuGn',         'Greens', 'YlGn','bone', 'gray', 'pink', 'afmhot', 'hot', 'gist_heat', 'copper',          'Wistia', 'autumn_r', 'summer_r', 'spring_r', 'cool', 'winter_r')            cmap_plot(sequential_cmap, 'Sequential')netflix_date = netflix_titles[['date_added']].dropna()netflix_date['year'] = netflix_date['date_added'].apply(lambda x : x.split(', ')[-1])netflix_date['month'] = netflix_date['date_added'].apply(lambda x : x.lstrip().split(' ')[0])month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'][::-1]df = netflix_date.groupby('year')['month'].value_counts().unstack().fillna(0)[month_order].T#응용하면 이런 그래프도 만들어진다.plt.figure(figsize=(10,7),dpi=200)plt.pcolor(df,cmap='gist_heat_r',edgecolors='white',linewidths=2)plt.xticks(np.arange(0.5,len(df.columns),1),df.columns,fontsize=7,fontfamily='serif')plt.yticks(np.arange(0.5,len(df.index),1),df.index,fontsize=7,fontfamily='serif')plt.title('Netflix Contents Update',fontsize=12,fontfamily='serif',fontweight='bold',position=(0.23,1.0+0.02))cbar=plt.colorbar()cbar.ax.tick_params(labelsize=8) cbar.ax.minorticks_on()plt.show()#Scientific Colormapscientific_cmap=('viridis','plasma','inferno','magma')cmap_plot(scientific_cmap,'Scientific')Text &amp; Annotate &amp; Patch그래프에 세부 사항을 넣을 수 있다.ax.text와 ax.annotate는 유사하지만 다르다.ax.text 그래프의 비율 좌표를 표현ax.annotate 그래프의 좌표를 표현va, ha 현재 좌표가 텍스트의 중심, 왼쪽, 오른쪽을 결정color 색상과 RGB값 선택 bbox 텍스트를 포장하는 상자 표현facecolor와 edgecolor가 분리되어있음pad로 html에서 처럼 padding 가능boxstyle로 직사각형의 모서리 조정 가능fig,ax=plt.subplots(figsize=(5, 5), dpi=100)# Gray Boxax.text(0.1, 0.9, 'Test', color='gray', va=\"center\", ha=\"center\")# Red Boxax.text(0.3, 0.7, 'Test', color='red', va=\"center\", ha=\"center\",        bbox=dict(facecolor='none', edgecolor='red'))# Blue Boxax.text(0.5, 0.5, 'Test', color='blue', va=\"center\", ha=\"center\",        bbox=dict(facecolor='none', edgecolor='blue', pad=10.0))# Green Boxax.text(0.7, 0.3, 'Test', color='green', va=\"center\", ha=\"center\",        bbox=dict(facecolor='none', edgecolor='green', boxstyle='round'))# Blackax.text(0.9, 0.1, 'Test', color='black', va=\"center\", ha=\"center\",        bbox=dict(facecolor='none', edgecolor='black', boxstyle='round, pad=0.5'))ax.set_xticks([])ax.set_yticks([])plt.show()효과적인 표현을 위한 그림들import matplotlib.path as mpathimport matplotlib.lines as mlinesimport matplotlib.patches as mpatchesfrom matplotlib.collections import PatchCollectiondef label(xy, text):    y = xy[1] - 0.15      plt.text(xy[0], y, text, ha=\"center\", family='sans-serif', size=14)fig, ax = plt.subplots()grid = np.mgrid[0.2:0.8:3j, 0.2:0.8:3j].reshape(2, -1).Tpatches = []# 원circle = mpatches.Circle(grid[0], 0.1, ec=\"none\")patches.append(circle)label(grid[0], \"Circle\")# 직사각형rect = mpatches.Rectangle(grid[1] - [0.025, 0.05], 0.05, 0.1, ec=\"none\")patches.append(rect)label(grid[1], \"Rectangle\")# wedgewedge = mpatches.Wedge(grid[2], 0.1, 30, 270, ec=\"none\")patches.append(wedge)label(grid[2], \"Wedge\")# Polygonpolygon = mpatches.RegularPolygon(grid[3], 5, 0.1)patches.append(polygon)label(grid[3], \"Polygon\")# ellipseellipse = mpatches.Ellipse(grid[4], 0.2, 0.1)patches.append(ellipse)label(grid[4], \"Ellipse\")# arrowarrow = mpatches.Arrow(grid[5, 0] - 0.05, grid[5, 1] - 0.05, 0.1, 0.1,                       width=0.1)patches.append(arrow)label(grid[5], \"Arrow\")# path patchPath = mpath.Pathpath_data = [    (Path.MOVETO, [0.018, -0.11]),    (Path.CURVE4, [-0.031, -0.051]),    (Path.CURVE4, [-0.115, 0.073]),    (Path.CURVE4, [-0.03, 0.073]),    (Path.LINETO, [-0.011, 0.039]),    (Path.CURVE4, [0.043, 0.121]),    (Path.CURVE4, [0.075, -0.005]),    (Path.CURVE4, [0.035, -0.027]),    (Path.CLOSEPOLY, [0.018, -0.11])]codes, verts = zip(*path_data)path = mpath.Path(verts + grid[6], codes)patch = mpatches.PathPatch(path)patches.append(patch)label(grid[6], \"PathPatch\")# fancy boxfancybox = mpatches.FancyBboxPatch(    grid[7] - [0.025, 0.05], 0.05, 0.1,    boxstyle=mpatches.BoxStyle(\"Round\", pad=0.02))patches.append(fancybox)label(grid[7], \"FancyBboxPatch\")# linex, y = np.array([[-0.06, 0.0, 0.1], [0.05, -0.05, 0.05]])line = mlines.Line2D(x + grid[8, 0], y + grid[8, 1], lw=5., alpha=0.3)label(grid[8], \"Line2D\")colors = np.linspace(0, 1, len(patches))collection = PatchCollection(patches, cmap=plt.cm.hsv, alpha=0.3)collection.set_array(np.array(colors))ax.add_collection(collection)ax.add_line(line)plt.axis('equal')plt.axis('off')plt.tight_layout()plt.show()Details &amp; Examples그림에 대한 다양한 설정Horizontal and Vertical (barplot)Border(edge) color and thicknessMain Color &amp; Sub ColorTransparencySpanFont Weight, Color, Family, Size …글꼴 굵기, 크기를 설정할 수 있다.serifs와 sans serifs도 차이가 있다.fontsize, color, fontweight, fontfamily같은 키워들을 이용할 수 있다.Horizontal keyboard &amp; Vertical (barplot)x축의 수가 많으면 가독성이 낮다. 예를들면 countplot은 x축과 겹치는 경우가 발생한다.이럴 때는 수직으로 배치하면 가독성이 좋다.Border(Edge) Color &amp; Thickness (Width)별도의 테두리를 이용하여 가독성을 높일 수 있다.R에서 자주 이용한다.경계선은 밝기와 투명도를 조절하여 구별하는 것이 좋다.Main Color &amp; Sub Color화려한 색상보다 정보의 전달을 편하게 하는 것에 목적을 둔다. 그래프의 종류에 따라 색상도 변경하는 것이 좋다.특정 부품을 강조 표시할 때 목록으로 전달하는 것이 좋다.from matplotlib.ticker import FuncFormatterdef age_band(num):    for i in range(1, 100):        if num &lt; 10*i :             return f'under {i*10}'titanic_train=pd.read_csv(\"train.csv\")titanic_train['age_band']=titanic_train['Age'].apply(age_band)titanic_age = titanic_train[['age_band', 'Survived']].groupby('age_band')['Survived'].value_counts().sort_index().unstack().fillna(0)titanic_age['Survival rate']=titanic_age[1] / (titanic_age[0] + titanic_age[1]) * 100fig, ax=plt.subplots(1, 2, figsize=(18, 7), dpi=300)# ax1ax[0].bar(titanic_age['Survival rate'].index, titanic_age['Survival rate'], color='gray')ax[0].set_title('Age Band &amp; Survival Rate(Before)')# ax2color_map=['gray' for _ in range(9)]color_map[0] = color_map[8] = '#3caea3'ax[1].bar(titanic_age['Survival rate'].index, titanic_age['Survival rate'], alpha=0.7, color=color_map, width=0.6, edgecolor='black', linewidth=1.2)ax[1].set_title('Age Band &amp; Survival Rate(After)', fontsize=15, fontweight='bold', position=(0.25, 1.0+0.05))for i in titanic_age['Survival rate'].index:    ax[1].annotate(f\"{titanic_age['Survival rate'][i]:.02f}%\",                    xy=(i, titanic_age['Survival rate'][i] + 2),                   va = 'center', ha='center',fontweight='bold', color='#383838')ax[1].yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:}%')) plt.suptitle('* Focus on survival rates for young and old', x=0.65, y=0.94, color='gray')plt.subplots_adjust(left=0.5, right=0.8)plt.tight_layout()plt.show()Main Color &amp; Sub Color산점도와 같이 많은 점들이 겹치면 투명도가 중요하다. 투명도를 이용해서 둘 이상의 그림을 함께 배치할 수 있다.import seaborn as snsexam_data=pd.read_csv(\"StudentsPerformance.csv\")fig, ax=plt.subplots(1, 2, figsize = (15, 7), dpi=150)ax[0].scatter(x='math score', y='reading score',data=exam_data, color='gray')ax[0].set_title('Before')ax[1].scatter(x='math score', y='reading score',data=exam_data[exam_data['gender']=='male'], color='skyblue', alpha=0.5, label='Male', s=70)ax[1].scatter(x='math score', y='reading score',data=exam_data[exam_data['gender']=='female'], color='salmon', alpha=0.5, label='Female', s=70)ax[1].set_title('After', fontsize=15, fontweight='bold')ax[1].legend()plt.gca().spines['top'].set_visible(False)plt.gca().spines['right'].set_visible(False)plt.show()Spanaxvspan: 수직axhspan: 수평sns.set_style('whitegrid') # plot with gridmovie=netflix_titles[netflix_titles['type'] == 'Movie']    rating_order=['G', 'TV-Y', 'TV-G', 'PG', 'TV-Y7', 'TV-Y7-FV', 'TV-PG', 'PG-13', 'TV-14', 'R', 'NC-17', 'TV-MA']movie_rating=movie['rating'].value_counts()[rating_order] fig, ax=plt.subplots(1, 1, figsize=(14, 7), dpi=200)ax.bar(movie_rating.index, movie_rating,  color=\"#d0d0d0\", width=0.6, edgecolor='black')ax.set_title(f'Distribution of Movie Rating (Before)', fontweight='bold')plt.show()def rating_barplot(data, title, height, h_lim=None):    fig, ax=plt.subplots(1,1, figsize=(14, 7), dpi=200)    if h_lim:        ax.set_ylim(0, h_lim)    ax.bar(data.index, data,  color=\"#e0e0e0\", width=0.52, edgecolor='black')    color=['green',  'blue',  'orange',  'red']    span_range=[[0, 2], [3,  6], [7, 8], [9, 11]]    for idx, sub_title in enumerate(['Little Kids', 'Older Kids', 'Teens', 'Mature']):        ax.annotate(sub_title,xy=(sum(span_range[idx])/2 ,height),xytext=(0,0), textcoords='offset points',                    va=\"center\", ha=\"center\",color=\"w\", fontsize=16, fontweight='bold',                    bbox=dict(boxstyle='round4', pad=0.4, color=color[idx], alpha=0.6))        ax.axvspan(span_range[idx][0]-0.4,span_range[idx][1]+0.4,  color=color[idx], alpha=0.07)    ax.set_title(f'Distribution of {title} Rating (After)', fontsize=15, fontweight='bold', position=(0.20, 1.0+0.03))    plt.show()rating_barplot(movie_rating,'Movie', 1200, 1400)MEME : xkcd themeimport matplotlibmatplotlib.font_manager._rebuild()with plt.xkcd():    fig=plt.figure()    ax=fig.add_axes((0.1, 0.2, 0.8, 0.7))    ax.spines['right'].set_color('none')    ax.spines['top'].set_color('none')    ax.set_xticks([])    ax.set_yticks([])    ax.set_ylim([-30, 10])    data=np.ones(100)    data[70:]-=np.arange(30)    ax.annotate('THE DAY I REALIZED\\nI COULD COOK BACON\\nWHENEVER I WANTED',        xy=(70, 1), arrowprops=dict(arrowstyle='-&gt;'), xytext=(15, -10))    ax.plot(data)    ax.set_xlabel('time')    ax.set_ylabel('my overall health')    fig.text(0.5, 0.05,'\"Stove Ownership\" from xkcd by Randall Munroe',ha='center')with plt.xkcd():    fig=plt.figure()    ax=fig.add_axes((0.1, 0.2, 0.8, 0.7))    ax.bar([0, 1], [0, 100], 0.25)    ax.spines['right'].set_color('none')    ax.spines['top'].set_color('none')    ax.xaxis.set_ticks_position('bottom')    ax.set_xticks([0, 1])    ax.set_xticklabels(['CONFIRMED BY\\nEXPERIMENT', 'REFUTED BY\\nEXPERIMENT'])    ax.set_xlim([-0.5, 1.5])    ax.set_yticks([])    ax.set_ylim([0, 110])    ax.set_title(\"CLAIMS OF SUPERNATURAL POWERS\")    fig.text(0.5, -0.05,'\"The Data So Far\" from xkcd by Randall Munroe',ha='center')plt.show()",
        "url": "/programming-kaggle1"
    }
    ,
    
    "programming-baekjoon2": {
        "title": "백준 (2) &lt;br&gt; (1018, 1085, 1181, 1259, &lt;br&gt; 1436, 1654, 1874, 1920)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)solved.ac의 class 2단계 8문항을 풀어보았다.파이썬 알고리즘 과제는 알고리즘 공부를 많이 하지 않은 나로써는 조금 어려웠다.https://solved.ac/search?query=in_class:21018번 체스판 다시 칠하기https://www.acmicpc.net/problem/1018N,M 크기를  받고 보드 만들기nXm형태의 위치를 파악하기 쉽게 리스트 형태로 받았다.n, m=map(int,input().split())if 8&lt;=n&lt;=50 and 8&lt;=m&lt;=50:    board = [input() for i in range(n)]10 13BBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBBBBBBBBBWBWBWBBBBBBBBBWBWBWWWWWWWWWWBWBWWWWWWWWWWBWB위치가 짝수일 때와 홀수일 때로 나눠서  W, B가 아닐 때마다 점수를 추가해준 다음 가장 최소가 되는 값만 찾아내면 된다.따라서 n * m의 보드에서 가능한 경우의 수는 n-7 * m-7이다. ex)10 13을 입력받을 경우 18가지.8 * 8로 잘라주기 위해서 k와 l을 (i,i+8), (j,j+8)로 한정짓는다.k+l이 홀수일 경우와 짝수일 경우, W로 시작할 경우와 B로 시작할 경우를 나눠서 모든 경우의 수를 반복문으로 확인해준다.마지막으로 total_score에 들어있는 값들 중 최솟값을 구해준다.total_score=[]# 보드에서 경우의 수 나누어주기for i in range(n-7):    for j in range(m-7):        count_w=0 #w가 아닐때        count_b=0 #b가 아닐때        #8*8 크기로 잘라주기        for k in range(i,i+8):             for l in range (j,j+8):                #각 경우의 수마다 비교해서 점수 추가하기                if (k+l)%2==0:                    if board[k][l]!='W':                                                    count_w=count_w+1                    if board[k][l]!='B':                        count_b=count_b+1                else:                    if board[k][l]!='W':                        count_b=count_b+1                    if board[k][l]!='B':                                                    count_w=count_w+1        # 점수들 한 list에 모아주기        total_score.append(count_w)        total_score.append(count_b)print(min(total_score)) #최솟값 출력121085번 직사각형에서 탈출https://www.acmicpc.net/problem/1085x,y,w,h 입력받기x,y,w,h =map(int,input().split())6 2 10 30과 가까운 경계선은 x, y로 w,h와 가까운 경계선은 w-x, h-y로 표현해주는 대신 음수가 나올수 있기때문에 abs()를 이용하여 절댓값으로 표현.print(min(x,y,abs(w-x),abs(h-y)))11181번 단어 정렬https://www.acmicpc.net/problem/1181단어 리스트 입력받기n=int(input())word_list=[input() for i in range(n)]13butiwonthesitatenomorenomoreitcannotwaitimyours길이가 짧은 것부터 같으면 사전 순으로 정렬하기 (단, 중복 제외)set함수로 중복을 먼저 제거하였다.단어의 길이를 먼저, 그다음에 단어를 하나의 tuple로 만들어 리스트를 다시 만들어주었다.sort를 이용하면 앞에 숫자가 들어갔기 떄문에 길이, 알파벳 순으로 정렬된다.# 중복 단어 제거word_list=list(set(word_list))len_word_list=[]# (단어 길이, 단어) 형태의 tuple 만들기for i in word_list:    len_word_list.append((len(i),i))# 정렬하기len_word_list.sort()print(len_word_list)# 출력하기for j,k in len_word_list:    print(k)[(1, 'i'), (2, 'im'), (2, 'it'), (2, 'no'), (3, 'but'), (4, 'more'), (4, 'wait'), (4, 'wont'), (5, 'yours'), (6, 'cannot'), (8, 'hesitate')]iimitnobutmorewaitwontyourscannothesitate1259번 팰린드롬수https://www.acmicpc.net/problem/1259앞에서 읽어도 뒤에서 읽어도 같은 숫자 찾기0을 입력하면 반복문이 끝나게 while과 if를 이용하였다.입력받은 숫자는 위치를 찾기 편하게 문자형으로 입력받았다.입력받은 숫자의 길이/2 만큼의 반복문을 돌리면 반대쪽은 (숫자의 길이-i-1)로 대응된다.한가지 숫자라도 값이 다르면 False 값을 가지고 ‘no’를 출력하면 ‘yes’를 출력하는 것을 만들 때보다 길이가 짧아질 수 있다.while True:    word=input() # 숫자를 무한으로 입력받기 위해 while문 사용    quest=True # 한가지 입력값을 처리하고나서 True, False값을 True로 초기화    if word=='0': # 0을 입력하면 반복문 종료        break    else:        word_len=len(word)        for i in range(int((word_len)/2)): #단어 길이의 반만 확인하면 반대쪽 숫자와 대응된다.                if word[i]!=word[word_len-1-i]: #반대쪽 숫자와 대응하기 위해서 word_len-1-i 사용                    quest=False # 하나의 경우라도 False가 나오면 반복문 종료                    continue        if quest==False: # False가 나오면 바로 'no' 출력            print('no')        else: print('yes')121yes1231no12421yes01436번 영화감독 숌https://www.acmicpc.net/problem/1436666이 적어도 3개이상 연속으로 들어가는 수를 만든다처음에 문제를 풀 때 중간에 666이 3개 이상 들어가는 경우를 제외해서 틀렸다.list666에 가장 작은 숫자인 666부터 ‘666’이 문자열로 들어가있는 숫자들을 확인해서 추가하였다.입력받은 숫자가 list666의 길이보다 크면 계속 추가해주었고 list666[num-1]을 통하여 값을 출력해준다.num=int(input())list666=[]i=666while len(list666)&lt;num:    if '666' in str(i):        list666.append(i)    i=i+1print(list666[num-1])326661654번 랜선 자르기https://www.acmicpc.net/problem/1654숫자 입력받기k,n=map(int,input().split())lan=[int(input()) for i in range(k)]4 11802743457539시간초과된 코드for문을 이용하니까 연산자가 너무 많아서 시간이 초과되었던 것 같다.div_list=[]for i in range(min(lan)):    div=int(min(lan))-i    count=[]    for w in lan:        count.append(w//div)    n_result=0    for j in range(k):        n_result=n_result+count[j]    if n_result==n:        div_list.append(div)print(max(div_list))200이진탐색을 이용하여보자.이진탐색이란 마치 병뚜껑 숫자맞추기를 할 때 50을 먼저 외치고 다음에 25나 75를 외치는 것처럼 가운데에 위치한 값들을 기반으로 탐색하는 것이다.정렬된 데이터일 때 사용 가능하다.따라서 입력받은 값들을 기준으로 max값과 1을 양 끝 값으로 놓는다.시작과 끝이 같을 때 까지 while문을 돌련준다.중간값을 구하고 이 값으로 입력받은 값들을 나누어준다.이때 잘라진 갯수가 n보다 크면 시작값에 중간값+1을, 작으면 끝값을 중간값-1을 해준다.while문이 다 돌고 나면 그 중 작은 값이 정답이다.start , end= 1,max(lan) #시작값과 끝값 구하기while start&lt;=end: #루프를 돌기위한 조건문    mid=(start+end)//2 #중간값 설정    cutting=0 #잘라진 선의 갯수 선언    for i in lan:        cutting+=i//mid #잘라진 선의 갯수 구하는 for문    if cutting&gt;=n: #n과 잘라진 선의 크기 비교를 통한 중간값 찾기        start=mid+1     else:        end=mid-1print(min(start,end))2001874번 스택 수열https://www.acmicpc.net/problem/1874스택과 푸쉬, 팝 이해하기push를 세 번 하면 [1,2,3] 스택이 쌓이게 되고 여기서 pop을 하면 3이 출력된다.n을 통해 입력할 숫자의 갯수를 입력받고 num을 통해 숫자를 입력받는다.count는 입력받을 숫자가 stack에 입력되도록 해준다. 0으로 두면 0부터 시작이다.따라서 1로 한다.result를 통해 +와 -를 입력받고 stack에는 count에 생긴 숫자들을 쌓아둔다.while문을 통해 stack을 완성하고 if문을 통해 해당 숫자가 나오면 -를 입력한 뒤pop해서 숫자를 제거한다.n=int(input())count=1 #count=1로 해줘야 0부터 숫자를 세지않는다.result=[] # +와 -를 저장하기 위한 리스트stack=[] # 쌓인 숫자를 저장하기 위한 리스트temp=True # 불가능한 경우에 False처리하기 위한 tempfor i in range(n):    num=int(input())     while count&lt;=num: #num과 같거나 작아질때 까지 stack에 숫자를 입력받는다.        stack.append(count)        result.append('+') #입력받은 숫자만큼 결과에 +를 입력해준다.        count=count+1    if stack[-1]==num: #스택의 마지막 숫자가 num과 같을 경우 해당 숫자를 pop하고 -를 입력해준다.        stack.pop()        result.append('-')    else:        temp=False # if문이 적용되지 않는 경우에는 False를 전달해준다.        if temp==False:    print('NO')else:    for j in result:        print(j)843687521++++--++-++-----1920번 스택 수열https://www.acmicpc.net/problem/1920시간초과list를 사용했을 때는 시간초과가 나왔고 set을 이용했을 때는 정상적으로 나왔다.리스트의 in연산자를 통한 포함 여부의 시간 복잡도는 O(N)이다.이분 탐색의 시간 복잡도는 O(logN) 이다.Set과 Dictionary의 in연산을 통한 포함 여부 확인의 시간 복잡도는 O(1)이다.따라서 N만 set으로 받아줘도 시간이 매우 단축된다.n=int(input())N=set(map(int,input().split()))m=int(input())M=list(map(int,input().split()))54 1 5 2 351 3 7 9 5for i in range(m):    if M[i] in N:        print(1)    else:        print(0)11001",
        "url": "/programming-baekjoon2"
    }
    ,
    
    "programming-baekjoon1": {
        "title": "백준 (1) &lt;br&gt; (2557, 8958, 1000, 1001, 1008, &lt;br&gt; 2935, 2753, 2884, 5063,4101)",
            "author": "keonju",
            "category": "",
            "content": "백준 관련 글    백준 (1) (2557, 8958, 1000, 1001, 1008, 2935, 2753, 2884, 5063, 4101)    백준 (2) (1018, 1085, 1181, 1259, 1436, 1654, 1874, 1920)    백준 (3) 문자열 알고리즘(11720, 8958, 1152, 10809, 1157, 9012, 11718)    백준 (4) (1157, 1546, 2577, 2675, 2908, 1018, 1436, 1259, 7568, 10250)    백준 (5) 정렬 알고리즘(2750,11399,2751,1427, 10989,1181,11650)    백준 (6) (3085, 2563, 4673, 5635, 11170)    백준 (7) 스택 알고리즘(10828,10773,1874,10799, 4949,1406,2493)    백준 (8) 큐 알고리즘(10845,1158,1966,2164,11866,18258)백준 10문제를 풀어보았다.2557. Hello Worldhttps://www.acmicpc.net/problem/2557Hello World!를 출력해야하는데 Hello World를 출력하여서 한번 틀렸다. 문제를 잘 읽어야한다.print(\"Hello World!\")Hello World!8958. OX퀴즈https://www.acmicpc.net/problem/8958for문을 이용하여 입력받을 ox의 갯수를 입력받고 for문 중첩을 이용하여 ox의 길이를 파악하고 if문으로 ox 여부를 확인하였다.ox의 연속성에 따른 점수변화를 num_score로 두고 total_score을 num_score의 합으로 설정하였다.num=int(input())for i in range(num):    ox=input()    total_score=0    num_score=0    for i in range(len(ox)):        if (ox[i]=='O') is True:            num_score=num_score+1        else:            num_score=0        total_score=total_score+num_score    print(total_score)5OOXXOXXOOO10OOXXOOXXOO9OXOXOXOXOXOXOX7OOOOOOOOOO55OOOOXOOOOXOOOOX301000. A+Bhttps://www.acmicpc.net/problem/1000map을 이용하여 a와 b를 사이 공백으로 분류시켜주는 것이 필요한 문제이다.a,b=map(int,input().split())if a &gt;0 and b&lt;10:    print(a+b)1 231001. A-Bhttps://www.acmicpc.net/problem/1001위 문제와 마찬가지로 map을 이용하여 a와 b를 사이 공백으로 분류시켜주는 것이 필요한 문제이다.a,b=map(int,input().split())if a &gt;0 and b&lt;10:    print(a-b)3 211008. A/Bhttps://www.acmicpc.net/problem/1008위 문제와 마찬가지로 map을 이용하여 a와 b를 사이 공백으로 분류시켜주는 것이 필요한 문제이다.무한 소수일 경우a,b=map(int,input().split())if a &gt;0 and b&lt;10:    print(a/b)1 30.3333333333333333유한 소수일 경우a,b=map(int,input().split())if a &gt;0 and b&lt;10:    print(a/b)4 50.82935. 소음https://www.acmicpc.net/problem/2935a와 b는 정수형으로 입력받고 + 와 * 는 문자형으로 입력받았다.a와 b가 10의 제곱 형태이므로 반복문을 통하여 10 ** i, 10 ** j로 제곱 형태를 판별하였고 + 와 * 는 if문으로 구분지어서 계산을 해주었다.파이썬에서는 제곱을 ** 형태로 표현하는 것을 상기해야한다.*를 이용한 경우a=int(input())cal=input()b=int(input())for i in range(99):    for j in range(99):        if (a==10**i) is True and (b==10**j) is True:            if cal=='+':                print(a+b)            if cal=='*':                print(a*b)1000*100100000+를 이용한 경우a=int(input())cal=input()b=int(input())for i in range(99):    for j in range(99):        if (a==10**i) is True and (b==10**j) is True:            if cal=='+':                print(a+b)            if cal=='*':                print(a*b)10000+10100102753. 윤년https://www.acmicpc.net/problem/2753입력받은 연도를 1 이상 4000 이하로 제한하고 연도를 4로 나눈 나머지가 0, 100으로 나눈 나머지가 0이 아닌 경우로 하나, 400으로 나눈 나머지가 0인 경우 하나로 나누어서 1을 출력해주고 나머지는 0을 출력하는 형태로 만들었다.나머지는 %로 구한다는 것을 상기해주었다.윤년인 경우year=int(input())if year&gt;=1 and year&lt;=4000:    if year%4==0 and year%100!=0:        print(1)    elif year%400==0:        print(1)    else:        print(0)20001윤년이 아닌 경우year=int(input())if year&gt;=1 and year&lt;=4000:    if year%4==0 and year%100!=0:        print(1)    elif year%400==0:        print(1)    else:        print(0)199902884. 알람 시계https://www.acmicpc.net/problem/2884위에 사칙연산 문제와 같이 map을 이용해서 시와 분을 분리하여 입력받는다.우선 시를 0 이상 23 이하, 분을 0 이상 59 이하로 한정해었다.첫번째 경우 분이 45 이상일 경우 분에서 45를 빼주더라도 시간은 변하지 않는다. 따라서 시간, 분-45 를 출력해주면 된다.두번째 경우 분이 45 미만일 경우 시가 하나 작아진다. 0 시 45 분 이전에는 날짜가 바뀌므로 23 에서 시를 빼주고 나머지 경우는 시에서 1 빼준다.분이 45 미만일 경우 60 - (45 - 분) 해주면 바뀐 분이 나온다. 따라서 분 + 15 로 표현해 주었다.시와 분 모두 바뀔 때h,m=map(int,input().split())if 0&lt;=h&lt;=23 and 0&lt;=m&lt;=59 :    if m-45&gt;=0:        print(h,m-45)    else:        if h-1&lt;0:            print(23-h,m+15)        else:            print(h-1,m+15)10 109 25날짜 까지 바뀔 때h,m=map(int,input().split())if 0&lt;=h&lt;=23 and 0&lt;=m&lt;=59 :    if m-45&gt;=0:        print(h,m-45)    else:        if h-1&lt;0:            print(23-h,m+15)        else:            print(h-1,m+15)0 3023 455063. TGNhttps://www.acmicpc.net/problem/5063test_case를 입력받아서 for문의 횟수를 한정시킨다.r, e, c를 map을 이용하여 입력받았으며 범위를 한정시켜주었다. 이때 (-10) ** 6으로 잘못 작성하여서 코드가 실행되지 않았다.광고 비용이 광고 수익과 일반 수익의 차보다 작을 때 광고를 하고 같으면 소용이 없고 크면 광고를 하지 않아야하므로 if문으로 구분시켰다.test_case=int(input())for i in range(test_case):    r,e,c=map(int,input().split())    if -(10**6)&lt;=r&lt;=(10**6) and -(10**6)&lt;=e&lt;=(10**6) and 0&lt;=c&lt;=(10**6):        if e-r&gt;c:            print('advertise')        elif e-r==c:            print('does not matter')        else:            print('do not advertise')30 100 70advertise100 130 30does not matter-100 -70 40do not advertise4101. 크냐?https://www.acmicpc.net/problem/4101while 반복문을 사용하여 계속 두 숫자를 입력받았으며 map을 통하여 공백을 기준으로 숫자를 나누었다.먼저 두 숫자가 0이면 해당 while문이 정지를 하게 만들어주고, 그 뒤에 두 숫자의 범위가 True면 두 숫자의 대소비교를 진행하였다.while True:    a,b=map(int,input().split())    if a==0 and b==0:            break    if 0&lt;a&lt;=10**6 and 0&lt;b&lt;=10**6:        if a&gt;b:            print(\"Yes\")        else:            print(\"No\")1 19No4 4No23 14Yes0 0",
        "url": "/programming-baekjoon1"
    }
    ,
    
    "programming-webcrawling1": {
        "title": "웹크롤링 (1) &lt;br&gt; urllib사용을 통한 크롤링",
            "author": "keonju",
            "category": "",
            "content": "웹크롤링 관련 글    웹크롤링 (1)-urllib사용을 통한 크롤링urllib사용을 통한 크롤링  url을 입력하여 작동하는 라이브러리로 통신을 통해 데이터를 주고받는 기능을 한다.  데이터를 받아오거나 다운로드할 수 있다1.urlretrieve  url로 표시된 네트워크 정보를 파일로 저장할 수 있는 기능 (이미지 , html)  (filename, headers) 튜플로 반환  ex file, header = req.urlretrieve(url, path)import urllib.request as requrl 에 접근할 url주소를 담고, path에 저장할 경로와 파일명을 적으면 된다.파일명만 적을 경우 현재 위치로 저장이 된다.url = \"https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMTA4MjBfMTQx%2FMDAxNjI5NDIxNjQ5NzM5.D1F-l6COowiUicFVRlpfQeSJRtkR4f9lkbVZgwJm6r4g.lBjYtG_wiubtJdiCYg8reMDwyC3wkFhPy5Ou0VXWRIQg.JPEG.hyun_0930%2F1629420539963.jpg&amp;type=sc960_832\"path = \"test1.jpg\"url'https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMTA4MjBfMTQx%2FMDAxNjI5NDIxNjQ5NzM5.D1F-l6COowiUicFVRlpfQeSJRtkR4f9lkbVZgwJm6r4g.lBjYtG_wiubtJdiCYg8reMDwyC3wkFhPy5Ou0VXWRIQg.JPEG.hyun_0930%2F1629420539963.jpg&amp;type=sc960_832'path'test1.jpg'현재위치 조회import osos.getcwd()file, header = req.urlretrieve(url,path)print(file)test1.jpgprint(header)accept-ranges: bytescache-control: max-age=2592000content-length: 37532content-type: image/jpegexpires: Sun, 26 Sep 2021 23:53:24 GMTlast-modified: Fri, 27 Aug 2021 23:53:24 GMTp3p: CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"date: Fri, 27 Aug 2021 23:53:24 GMTage: 665797server: Testa/5.1.1strict-transport-security: max-age=31536000connection: closehtml저장url2 = \"https://www.naver.com/\"path2 = \"naver.html\"file2, header2 = req.urlretrieve(url2, path2)print(\"----------------------------------------------------\")print(f\"file name: {file}\")print(\"----------------------------------------------------\")print(\"Header Info :\")print(header)print(\"----------------------------------------------------\")print(f\"file name: {file2}\")print(\"----------------------------------------------------\")print(\"Header Info :\")print(header2)----------------------------------------------------file name: test1.jpg----------------------------------------------------Header Info :accept-ranges: bytescache-control: max-age=2592000content-length: 37532content-type: image/jpegexpires: Sun, 26 Sep 2021 23:53:24 GMTlast-modified: Fri, 27 Aug 2021 23:53:24 GMTp3p: CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"date: Fri, 27 Aug 2021 23:53:24 GMTage: 665797server: Testa/5.1.1strict-transport-security: max-age=31536000connection: close----------------------------------------------------file name: naver.html----------------------------------------------------Header Info :Server: NWSDate: Sat, 04 Sep 2021 16:50:00 GMTContent-Type: text/html; charset=UTF-8Transfer-Encoding: chunkedConnection: closeSet-Cookie: PM_CK_loc=4d397054570413bd11c4b9094901203f9fef0e3df1bd78a55cef2ac0fd1d9e5e; Expires=Sun, 05 Sep 2021 16:50:00 GMT; Path=/; HttpOnlyCache-Control: no-cache, no-store, must-revalidatePragma: no-cacheP3P: CP=\"CAO DSP CURa ADMa TAIa PSAa OUR LAW STP PHY ONL UNI PUR FIN COM NAV INT DEM STA PRE\"X-Frame-Options: DENYX-XSS-Protection: 1; mode=blockStrict-Transport-Security: max-age=63072000; includeSubdomainsReferrer-Policy: unsafe-url2.urlerror  크롤링에서 발생할 수 있는 에러처리  에러처리를 통해서 어떤 에러가 발생하였는지 파악하고 코드를 수정  URLError: 요청한 곳의 서버가 없거나 네트워크 연결이 없는 상황  HTTPError: HTTP응답에 있는 status에 따라서 상태를 반환, status코드에 따라서 에러 유형이 다름  주의사항: URLError가 HTTPError도 잡기 때문에 HTTPError처리를 먼저 해줘야함1번 예제와 다르게 list에 넣고 for문을 통한 이미지 다운로드를 실시from urllib.error import URLError, HTTPErrorurl_list = ['https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMTA4MjBfMTQx%2FMDAxNjI5NDIxNjQ5NzM5.D1F-l6COowiUicFVRlpfQeSJRtkR4f9lkbVZgwJm6r4g.lBjYtG_wiubtJdiCYg8reMDwyC3wkFhPy5Ou0VXWRIQg.JPEG.hyun_0930%2F1629420539963.jpg&amp;type=sc960_832',            'https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMDA5MTNfNjMg%2FMDAxNjAwMDAxNjM1NzQ2.TuGLdOsJ8vLFnN589WEiiA5j5XrsWRA7lJUJicpozJwg.694y_QRQKQwqd7QR41nweA3T4vYnAGT4OqVuxWvJdrYg.JPEG.ecoanimal%2F51d63faf6312a3bc4873ee24d98cdfed.jpg&amp;type=a340']name_list = ['nuguli1.jpg', 'nuguli2.jpg']for i,url in enumerate(url_list):    # 예외 처리    try:        # 웹 수신 정보 읽기        response = req.urlopen(url)                # 수신 내용        contents = response.read()        print('----------------------------------------------------------------------------------------------------------------')        # 상태 정보 중간 출력        print(f'file_name : {name_list[i]}')        print('&lt;Header Info&gt;')        print(f'{response.info()}')        print(f'Status Code : {response.getcode()}')        print()        print('----------------------------------------------------------------------------------------------------------------')        # 파일 쓰기        with open(name_list[i], 'wb') as c:            c.write(contents)            except HTTPError as e: # HTTP 에러        print(\"다운로드 실패.\")        print('HTTPError Code : ', e.code)    except URLError as e: # URL 에러        print(\"Download failed.\")        print('URL Error Reason : ', e.reason)        # 성공    else:        print()        print(f'{name_list[i]}이미지 다운 완료.')----------------------------------------------------------------------------------------------------------------file_name : nuguli1.jpg&lt;Header Info&gt;accept-ranges: bytescache-control: max-age=2592000content-length: 37532content-type: image/jpegexpires: Sun, 26 Sep 2021 23:53:24 GMTlast-modified: Fri, 27 Aug 2021 23:53:24 GMTp3p: CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"date: Fri, 27 Aug 2021 23:53:24 GMTage: 665797server: Testa/5.1.1strict-transport-security: max-age=31536000connection: closeStatus Code : 200----------------------------------------------------------------------------------------------------------------nuguli1.jpg이미지 다운 완료.----------------------------------------------------------------------------------------------------------------file_name : nuguli2.jpg&lt;Header Info&gt;accept-ranges: bytescache-control: max-age=2592000content-length: 50098content-type: image/jpegexpires: Fri, 24 Sep 2021 14:11:17 GMTlast-modified: Wed, 25 Aug 2021 14:11:17 GMTp3p: CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"date: Wed, 25 Aug 2021 14:11:17 GMTage: 873524server: Testa/5.1.1strict-transport-security: max-age=31536000connection: closeStatus Code : 200----------------------------------------------------------------------------------------------------------------nuguli2.jpg이미지 다운 완료.3.urlopen/ urlparseimport urllib.request as reqfrom urllib.parse import urlparseurl=\"https://www.seoultech.ac.kr/index.jsp\"ele=req.urlopen(url)print('type : {}'.format(type(ele)))print()print(\"geturl : {}\".format(ele.geturl()))print()print(\"status : {}\".format(ele.status))print()print(\"headers : {}\".format(ele.getheaders()))print()print()print('parse : {}'.format(urlparse('https://www.smu.ac.kr/ko/index.do?param=test').query))print()type : &lt;class 'http.client.HTTPResponse'&gt;geturl : https://www.seoultech.ac.kr/index.jspstatus : 200headers : [('Date', 'Sat, 04 Sep 2021 16:50:01 GMT'), ('Content-Type', 'text/html; charset=UTF-8'), ('Set-Cookie', 'JSESSIONID=5BabR1bj8eGHwllZbNNO9LXiYD2V1HlqI1KZJiR7EG01ZnEBpYTlBwaFVwCi61YT.web1_servlet_www;Path=/;HttpOnly'), ('X-Cache', 'MISS from cf4.seoultech.ac.kr'), ('X-Cache-Lookup', 'HIT from cf4.seoultech.ac.kr:3128'), ('Transfer-Encoding', 'chunked'), ('Via', ''), ('Connection', 'close')]parse : param=testprint(ele.info())Date: Sat, 04 Sep 2021 16:50:01 GMTContent-Type: text/html; charset=UTF-8Set-Cookie: JSESSIONID=5BabR1bj8eGHwllZbNNO9LXiYD2V1HlqI1KZJiR7EG01ZnEBpYTlBwaFVwCi61YT.web1_servlet_www;Path=/;HttpOnlyX-Cache: MISS from cf4.seoultech.ac.krX-Cache-Lookup: HIT from cf4.seoultech.ac.kr:3128Transfer-Encoding: chunkedVia: Connection: closeheaders에 데이터 추가하기# pip install fake_useragentfrom fake_useragent import UserAgentua = UserAgent()### fake_useragentua = UserAgent()print(ua.random)print(ua.ie)print(ua.msie)print(ua['Internet Explorer'])print(ua.opera)print(ua.chrome)print(ua.google)print(ua['google chrome'])print(ua.firefox)print(ua.ff)print(ua.safari)Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.3319.102 Safari/537.36Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/4.0; GTB7.4; InfoPath.1; SV1; .NET CLR 2.8.52393; WOW64; en-US)Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.2; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0)Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0; FunWebProducts)Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/5.0 Opera 11.11Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1468.0 Safari/537.36Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:23.0) Gecko/20131011 Firefox/23.0Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20130401 Firefox/31.0Mozilla/5.0 (Windows; U; Windows NT 6.0; de-DE) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.3 Safari/533.19.4야후 파이낸스 데이터 받아오기import jsonimport urllib.request as reqfrom fake_useragent import UserAgent  url 주소를 찾는 것에서 시간이 조금 걸림이미지같은 경우 주소가 연동되지만 변동되는 데이터는 적용되지않음크롬 개발자도구에서 Network항목에서 RequestURL 찾기import jsonimport urllib.request as reqfrom fake_useragent import UserAgent# Fake Header 정보(가상으로 User-Agent 생성)ua = UserAgent()# 헤더 선언headers = {    'User-Agent': ua.ie,    'referer': 'https://finance.yahoo.com/'}# 다음 주식 요청 URLurl = \"https://query1.finance.yahoo.com/v7/finance/quote?formatted=true&amp;crumb=5b5ru0zoR.q&amp;lang=en-US&amp;region=US&amp;symbols=ADA-USD%2CBTC-USD%2CDOGE-USD%2CETH-USD%2CZM&amp;fields=symbol%2CshortName%2ClongName%2CregularMarketPrice%2CregularMarketChange%2CregularMarketChangePercent&amp;corsDomain=finance.yahoo.com\"res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')# 응답 데이터 str -&gt; json 변환 및 data 값 저장_json_data = json.loads(res)print( _json_data, '\\n'){'quoteResponse': {'result': [{'fullExchangeName': 'CCC', 'exchangeTimezoneName': 'Europe/London', 'symbol': 'ADA-USD', 'regularMarketChange': {'raw': -0.12517643, 'fmt': '-0.13'}, 'gmtOffSetMilliseconds': 3600000, 'firstTradeDateMilliseconds': 1506812400000, 'exchangeDataDelayedBy': 0, 'language': 'en-US', 'regularMarketTime': {'raw': 1630774030, 'fmt': '5:47PM BST'}, 'regularMarketChangePercent': {'raw': -4.1827593, 'fmt': '-4.18%'}, 'exchangeTimezoneShortName': 'BST', 'quoteType': 'CRYPTOCURRENCY', 'marketState': 'REGULAR', 'regularMarketPrice': {'raw': 2.8674967, 'fmt': '2.87'}, 'market': 'ccc_market', 'quoteSourceName': 'CoinMarketCap', 'tradeable': False, 'exchange': 'CCC', 'sourceInterval': 15, 'shortName': 'Cardano USD', 'region': 'US', 'regularMarketPreviousClose': {'raw': 2.9606647, 'fmt': '2.96'}, 'triggerable': True}, {'fullExchangeName': 'CCC', 'exchangeTimezoneName': 'Europe/London', 'symbol': 'BTC-USD', 'regularMarketChange': {'raw': -611.96094, 'fmt': '-611.96'}, 'gmtOffSetMilliseconds': 3600000, 'firstTradeDateMilliseconds': 1410908400000, 'exchangeDataDelayedBy': 0, 'language': 'en-US', 'regularMarketTime': {'raw': 1630774082, 'fmt': '5:48PM BST'}, 'regularMarketChangePercent': {'raw': -1.2106228, 'fmt': '-1.21%'}, 'exchangeTimezoneShortName': 'BST', 'quoteType': 'CRYPTOCURRENCY', 'marketState': 'REGULAR', 'regularMarketPrice': {'raw': 49937.133, 'fmt': '49,937.13'}, 'market': 'ccc_market', 'quoteSourceName': 'CoinMarketCap', 'tradeable': False, 'exchange': 'CCC', 'sourceInterval': 15, 'shortName': 'Bitcoin USD', 'region': 'US', 'regularMarketPreviousClose': {'raw': 49922.355, 'fmt': '49,922.36'}, 'triggerable': True}, {'fullExchangeName': 'CCC', 'exchangeTimezoneName': 'Europe/London', 'symbol': 'DOGE-USD', 'regularMarketChange': {'raw': 0.0015876293, 'fmt': '0.00'}, 'gmtOffSetMilliseconds': 3600000, 'firstTradeDateMilliseconds': 1410908400000, 'exchangeDataDelayedBy': 0, 'language': 'en-US', 'regularMarketTime': {'raw': 1630774083, 'fmt': '5:48PM BST'}, 'regularMarketChangePercent': {'raw': 0.52755743, 'fmt': '0.53%'}, 'exchangeTimezoneShortName': 'BST', 'quoteType': 'CRYPTOCURRENCY', 'marketState': 'REGULAR', 'regularMarketPrice': {'raw': 0.30252412, 'fmt': '0.30'}, 'market': 'ccc_market', 'quoteSourceName': 'CoinMarketCap', 'tradeable': False, 'exchange': 'CCC', 'sourceInterval': 15, 'shortName': 'Dogecoin USD', 'region': 'US', 'regularMarketPreviousClose': {'raw': 0.29575953, 'fmt': '0.30'}, 'triggerable': True}, {'fullExchangeName': 'CCC', 'exchangeTimezoneName': 'Europe/London', 'symbol': 'ETH-USD', 'regularMarketChange': {'raw': -63.86255, 'fmt': '-63.86'}, 'gmtOffSetMilliseconds': 3600000, 'firstTradeDateMilliseconds': 1438902000000, 'exchangeDataDelayedBy': 0, 'language': 'en-US', 'regularMarketTime': {'raw': 1630774082, 'fmt': '5:48PM BST'}, 'regularMarketChangePercent': {'raw': -1.608492, 'fmt': '-1.61%'}, 'exchangeTimezoneShortName': 'BST', 'quoteType': 'CRYPTOCURRENCY', 'marketState': 'REGULAR', 'regularMarketPrice': {'raw': 3906.476, 'fmt': '3,906.48'}, 'market': 'ccc_market', 'quoteSourceName': 'CoinMarketCap', 'tradeable': False, 'exchange': 'CCC', 'sourceInterval': 15, 'shortName': 'Ethereum USD', 'region': 'US', 'regularMarketPreviousClose': {'raw': 3933.8274, 'fmt': '3,933.83'}, 'triggerable': True}, {'fullExchangeName': 'NasdaqGS', 'symbol': 'ZM', 'gmtOffSetMilliseconds': -14400000, 'language': 'en-US', 'regularMarketTime': {'raw': 1630699203, 'fmt': '4:00PM EDT'}, 'regularMarketChangePercent': {'raw': 1.084419, 'fmt': '1.08%'}, 'quoteType': 'EQUITY', 'tradeable': False, 'regularMarketPreviousClose': {'raw': 295.09, 'fmt': '295.09'}, 'exchangeTimezoneName': 'America/New_York', 'regularMarketChange': {'raw': 3.2000122, 'fmt': '3.20'}, 'firstTradeDateMilliseconds': 1555594200000, 'exchangeDataDelayedBy': 0, 'exchangeTimezoneShortName': 'EDT', 'marketState': 'CLOSED', 'regularMarketPrice': {'raw': 298.29, 'fmt': '298.29'}, 'market': 'us_market', 'quoteSourceName': 'Delayed Quote', 'priceHint': 2, 'exchange': 'NMS', 'sourceInterval': 15, 'shortName': 'Zoom Video Communications, Inc.', 'region': 'US', 'triggerable': True, 'longName': 'Zoom Video Communications, Inc.'}], 'error': None}} data_list=_json_data['quoteResponse']['result']from pprint import pprintpprint(data_list)[{'exchange': 'CCC',  'exchangeDataDelayedBy': 0,  'exchangeTimezoneName': 'Europe/London',  'exchangeTimezoneShortName': 'BST',  'firstTradeDateMilliseconds': 1506812400000,  'fullExchangeName': 'CCC',  'gmtOffSetMilliseconds': 3600000,  'language': 'en-US',  'market': 'ccc_market',  'marketState': 'REGULAR',  'quoteSourceName': 'CoinMarketCap',  'quoteType': 'CRYPTOCURRENCY',  'region': 'US',  'regularMarketChange': {'fmt': '-0.13', 'raw': -0.12517643},  'regularMarketChangePercent': {'fmt': '-4.18%', 'raw': -4.1827593},  'regularMarketPreviousClose': {'fmt': '2.96', 'raw': 2.9606647},  'regularMarketPrice': {'fmt': '2.87', 'raw': 2.8674967},  'regularMarketTime': {'fmt': '5:47PM BST', 'raw': 1630774030},  'shortName': 'Cardano USD',  'sourceInterval': 15,  'symbol': 'ADA-USD',  'tradeable': False,  'triggerable': True}, {'exchange': 'CCC',  'exchangeDataDelayedBy': 0,  'exchangeTimezoneName': 'Europe/London',  'exchangeTimezoneShortName': 'BST',  'firstTradeDateMilliseconds': 1410908400000,  'fullExchangeName': 'CCC',  'gmtOffSetMilliseconds': 3600000,  'language': 'en-US',  'market': 'ccc_market',  'marketState': 'REGULAR',  'quoteSourceName': 'CoinMarketCap',  'quoteType': 'CRYPTOCURRENCY',  'region': 'US',  'regularMarketChange': {'fmt': '-611.96', 'raw': -611.96094},  'regularMarketChangePercent': {'fmt': '-1.21%', 'raw': -1.2106228},  'regularMarketPreviousClose': {'fmt': '49,922.36', 'raw': 49922.355},  'regularMarketPrice': {'fmt': '49,937.13', 'raw': 49937.133},  'regularMarketTime': {'fmt': '5:48PM BST', 'raw': 1630774082},  'shortName': 'Bitcoin USD',  'sourceInterval': 15,  'symbol': 'BTC-USD',  'tradeable': False,  'triggerable': True}, {'exchange': 'CCC',  'exchangeDataDelayedBy': 0,  'exchangeTimezoneName': 'Europe/London',  'exchangeTimezoneShortName': 'BST',  'firstTradeDateMilliseconds': 1410908400000,  'fullExchangeName': 'CCC',  'gmtOffSetMilliseconds': 3600000,  'language': 'en-US',  'market': 'ccc_market',  'marketState': 'REGULAR',  'quoteSourceName': 'CoinMarketCap',  'quoteType': 'CRYPTOCURRENCY',  'region': 'US',  'regularMarketChange': {'fmt': '0.00', 'raw': 0.0015876293},  'regularMarketChangePercent': {'fmt': '0.53%', 'raw': 0.52755743},  'regularMarketPreviousClose': {'fmt': '0.30', 'raw': 0.29575953},  'regularMarketPrice': {'fmt': '0.30', 'raw': 0.30252412},  'regularMarketTime': {'fmt': '5:48PM BST', 'raw': 1630774083},  'shortName': 'Dogecoin USD',  'sourceInterval': 15,  'symbol': 'DOGE-USD',  'tradeable': False,  'triggerable': True}, {'exchange': 'CCC',  'exchangeDataDelayedBy': 0,  'exchangeTimezoneName': 'Europe/London',  'exchangeTimezoneShortName': 'BST',  'firstTradeDateMilliseconds': 1438902000000,  'fullExchangeName': 'CCC',  'gmtOffSetMilliseconds': 3600000,  'language': 'en-US',  'market': 'ccc_market',  'marketState': 'REGULAR',  'quoteSourceName': 'CoinMarketCap',  'quoteType': 'CRYPTOCURRENCY',  'region': 'US',  'regularMarketChange': {'fmt': '-63.86', 'raw': -63.86255},  'regularMarketChangePercent': {'fmt': '-1.61%', 'raw': -1.608492},  'regularMarketPreviousClose': {'fmt': '3,933.83', 'raw': 3933.8274},  'regularMarketPrice': {'fmt': '3,906.48', 'raw': 3906.476},  'regularMarketTime': {'fmt': '5:48PM BST', 'raw': 1630774082},  'shortName': 'Ethereum USD',  'sourceInterval': 15,  'symbol': 'ETH-USD',  'tradeable': False,  'triggerable': True}, {'exchange': 'NMS',  'exchangeDataDelayedBy': 0,  'exchangeTimezoneName': 'America/New_York',  'exchangeTimezoneShortName': 'EDT',  'firstTradeDateMilliseconds': 1555594200000,  'fullExchangeName': 'NasdaqGS',  'gmtOffSetMilliseconds': -14400000,  'language': 'en-US',  'longName': 'Zoom Video Communications, Inc.',  'market': 'us_market',  'marketState': 'CLOSED',  'priceHint': 2,  'quoteSourceName': 'Delayed Quote',  'quoteType': 'EQUITY',  'region': 'US',  'regularMarketChange': {'fmt': '3.20', 'raw': 3.2000122},  'regularMarketChangePercent': {'fmt': '1.08%', 'raw': 1.084419},  'regularMarketPreviousClose': {'fmt': '295.09', 'raw': 295.09},  'regularMarketPrice': {'fmt': '298.29', 'raw': 298.29},  'regularMarketTime': {'fmt': '4:00PM EDT', 'raw': 1630699203},  'shortName': 'Zoom Video Communications, Inc.',  'sourceInterval': 15,  'symbol': 'ZM',  'tradeable': False,  'triggerable': True}]result_list = []for data in data_list:    _set={}    _set['symbol'] = data['symbol']    _set['Last_price'] = data['regularMarketPrice']['fmt']    _set['Change'] = data['regularMarketChange']['fmt']    _set['%Change'] = data['regularMarketChangePercent']['fmt']    result_list.append(_set)import pandas as pddf = pd.DataFrame(result_list)df                  symbol      Last_price      Change      %Change                  0      ADA-USD      2.87      -0.13      -4.18%              1      BTC-USD      49,937.13      -611.96      -1.21%              2      DOGE-USD      0.30      0.00      0.53%              3      ETH-USD      3,906.48      -63.86      -1.61%              4      ZM      298.29      3.20      1.08%      ",
        "url": "/programming-webcrawling1"
    }
    ,
    
    "project-first": {
        "title": "project 연습글",
            "author": "keonju",
            "category": "",
            "content": "project 게시글 목록    project 연습글프로젝트 게시판 첫 글 연습",
        "url": "/project-first"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>
            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://keonju2.github.io/">주건나's Blog</a> &copy; 2021</section>
                <!--
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                -->
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    <a href="https://facebook.com/monkeykeonju" target="_blank" rel="noopener">Facebook</a>
                    
                    <!--
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                     -->
                </nav>

            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
    <div id="subscribe" class="subscribe-overlay">
        <a class="subscribe-overlay-close" href="#"></a>
        <div class="subscribe-overlay-content">
            
            <h1 class="subscribe-overlay-title">Search 주건나's Blog</h1>
            <p class="subscribe-overlay-description">
            검색어를 입력해주세요 </p>
            <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
        </div>
    </div>
    


    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-6FJ2289869', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
